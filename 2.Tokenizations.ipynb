{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35aac40f-f400-4be5-aed9-af16a8cab0bf",
   "metadata": {},
   "source": [
    "## Words and Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd1ee470-0163-41d0-b77a-1b55bf7d8635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple rule based tokenization\n",
    "import re\n",
    "def simple_tokenize(text):\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(r\"[a-z]+(?:'[a-z]+)?\",text)\n",
    "    #[a-z]+ matches the main word part (e.g. \"don\").\n",
    "    #(?:'[a-z]+) optionally matches 't part (apostrophe + letters).\n",
    "    # don't , we're, it's\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a8aad6f-ce90-4252-a67f-675dae97b0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'how', 'are', 'you', \"i'm\", 'fine', 'i', 'hope', 'this', 'letter', 'finds', 'you', 'in', 'pink', 'of', 'health']\n"
     ]
    }
   ],
   "source": [
    "text = \"hello how are you, I'm fine, I hope this letter finds you in pink of health\"\n",
    "tokens = simple_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b52d320e-6a27-4f3e-b52d-19bd6932facd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BPE is the binary pair encoder, were we divide words into subwords or small set of letters which often appear together.\n",
    "from collections import Counter, defaultdict \n",
    "\n",
    "def bpe_train(corpus_tokens, vocab_size = 1000):\n",
    "    \n",
    "    words = [' '.join(list(w)) + '</w>' for w in corpus_tokens]\n",
    "    word_counts = Counter(words)\n",
    "    def get_stats(word_counts):\n",
    "        pairs = Counter()\n",
    "        for w,c in word_counts.items():\n",
    "            symbols = w.split()\n",
    "            for i in range(len(symbols)-1):\n",
    "                pairs[(symbols[i], symbols[i+1])] += c\n",
    "        return pairs\n",
    "\n",
    "    merges = []\n",
    "    while True:\n",
    "        stats = get_stats(word_counts)\n",
    "        if not stats: break\n",
    "        (a,b) ,freq = stats.most_common(1)[0]\n",
    "        if len(merges) + 256 >= vocab_size: break\n",
    "        merges.append((a,b))\n",
    "        new_counts = Counter()\n",
    "        bigram = \" \".join((a,b))\n",
    "        for w,c in word_counts.items():\n",
    "            new_w = w.replace(bigram, a+b)\n",
    "            new_counts[new_w] += c\n",
    "        word_counts = new_counts\n",
    "\n",
    "    return merges\n",
    "\n",
    "def bpe_encode(word, merges):\n",
    "    w = list(word) + ['</w>']\n",
    "    merges_set = {tuple(m) for m in merges}\n",
    "    while True:\n",
    "        pairs = [(i,(w[i], w[i+1])) for i in range(len(w)-1)]\n",
    "        idx= next((i for i,p in pairs if p in merges_set), None)\n",
    "        if idx is None: break \n",
    "        w = w[:idx] + [w[idx] + w[idx+1]] + w[idx+2:]\n",
    "    return [t for t in w if t != '</w>']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97757faf-6059-4e77-a7bf-b3633437c67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample text: \n",
      " Speech and Language Processing\n",
      "An Introduction to Natural Language Processing,\n",
      "Computational Linguistics, and Speech Recognition\n",
      "with Language Models\n",
      "Third Edition draft\n",
      "Daniel Jurafsky\n",
      "Stanford University\n",
      "James H. Martin\n",
      "University of Colorado at Boulder\n",
      "Copyright Â©2025. All rights reserved.\n",
      "Draft of August 24, 2025. Comments and typos welcome!Summary of Contents\n",
      "I Large Language Models 1\n",
      "1 Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path,'rb') as f:\n",
    "        reader = PdfReader(f)\n",
    "        for page_num, page in enumerate(reader.pages):\n",
    "            text += page.extract_text() or \"\"\n",
    "    return text \n",
    "\n",
    "pdf_path = \"ed3book_aug25 no changes.pdf\"\n",
    "corpus_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "print(\"sample text: \\n\", corpus_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "508ea4b1-620b-4f1e-abaf-b331d3aacd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = simple_tokenize(corpus_text)\n",
    "tokens_merges = bpe_train(tokens, vocab_size = 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f030156-0c46-4814-b88f-36bf4e95b228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['la', 'ngu', 'ag', 'e']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_encode('language', tokens_merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8159c474-c8db-413a-811b-374ce02b922f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('t', 'h'),\n",
       " ('i', 'n'),\n",
       " ('t', 'i'),\n",
       " ('th', 'e</w>'),\n",
       " ('e', 'n'),\n",
       " ('a', 'n'),\n",
       " ('r', 'e'),\n",
       " ('c', 'o'),\n",
       " ('t', 'e'),\n",
       " ('o', 'n</w>')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_merges[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e83f45-2993-4687-81dd-ea67e2d1a83c",
   "metadata": {},
   "source": [
    "### Edit Distance (Levenshtein) \n",
    "This is a dynamic programming concept where we create a table dynamically and find the shortest path or efficient edit distance between two words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b089b13-b181-41d4-9070-1659d8b3bccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_distance(a,b):\n",
    "    n,m = len(a), len(b)\n",
    "    dp = [[0]*(m+1) for _ in range(n+1)]\n",
    "    for i in range(n+1): dp[i][0] = i\n",
    "    for j in range(m+1): dp[0][j] = j\n",
    "    for i in range(1,n+1):\n",
    "        for j in range(1,m+1):\n",
    "            cost = 0 if a[i-1]==b[j-1] else 1\n",
    "            dp[i][j] = min(dp[i-1][j]+1, dp[i][j-1]+1, dp[i-1][j-1]+cost)\n",
    "    return dp[n][m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9444f56e-b4cc-4682-aa20-9aab1b11145f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_distance('model','modal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5c169f-b468-4e07-9b98-d03d7d50af93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cuda-env GPU)",
   "language": "python",
   "name": "cuda-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
