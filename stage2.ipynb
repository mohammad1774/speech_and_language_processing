{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a4a2c4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
      "Collecting pdfminer.six==20250506 (from pdfplumber)\n",
      "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting Pillow>=9.1 (from pdfplumber)\n",
      "  Using cached pillow-12.0.0-cp311-cp311-win_amd64.whl.metadata (9.0 kB)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-5.0.0-py3-none-win_amd64.whl.metadata (67 kB)\n",
      "Collecting charset-normalizer>=2.0.0 (from pdfminer.six==20250506->pdfplumber)\n",
      "  Using cached charset_normalizer-3.4.4-cp311-cp311-win_amd64.whl.metadata (38 kB)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six==20250506->pdfplumber)\n",
      "  Downloading cryptography-46.0.3-cp311-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting cffi>=2.0.0 (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber)\n",
      "  Downloading cffi-2.0.0-cp311-cp311-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting pycparser (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber)\n",
      "  Using cached pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\n",
      "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
      "Downloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 5.6/5.6 MB 68.7 MB/s  0:00:00\n",
      "Using cached charset_normalizer-3.4.4-cp311-cp311-win_amd64.whl (106 kB)\n",
      "Downloading cryptography-46.0.3-cp311-abi3-win_amd64.whl (3.5 MB)\n",
      "   ---------------------------------------- 0.0/3.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.5/3.5 MB 104.9 MB/s  0:00:00\n",
      "Downloading cffi-2.0.0-cp311-cp311-win_amd64.whl (182 kB)\n",
      "Using cached pillow-12.0.0-cp311-cp311-win_amd64.whl (7.0 MB)\n",
      "Downloading pypdfium2-5.0.0-py3-none-win_amd64.whl (3.1 MB)\n",
      "   ---------------------------------------- 0.0/3.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.1/3.1 MB 60.3 MB/s  0:00:00\n",
      "Using cached pycparser-2.23-py3-none-any.whl (118 kB)\n",
      "Installing collected packages: pypdfium2, pycparser, Pillow, charset-normalizer, cffi, cryptography, pdfminer.six, pdfplumber\n",
      "\n",
      "   ----- ---------------------------------- 1/8 [pycparser]\n",
      "   ---------- ----------------------------- 2/8 [Pillow]\n",
      "   ---------- ----------------------------- 2/8 [Pillow]\n",
      "   -------------------- ------------------- 4/8 [cffi]\n",
      "   ------------------------- -------------- 5/8 [cryptography]\n",
      "   ------------------------------ --------- 6/8 [pdfminer.six]\n",
      "   ----------------------------------- ---- 7/8 [pdfplumber]\n",
      "   ---------------------------------------- 8/8 [pdfplumber]\n",
      "\n",
      "Successfully installed Pillow-12.0.0 cffi-2.0.0 charset-normalizer-3.4.4 cryptography-46.0.3 pdfminer.six-20250506 pdfplumber-0.11.7 pycparser-2.23 pypdfium2-5.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea038d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample text: \n",
      " Speech and Language Processing\n",
      "An Introduction to Natural Language Processing,\n",
      "Computational Linguistics, and Speech Recognition\n",
      "with Language Models\n",
      "Third Edition draft\n",
      "Daniel Jurafsky\n",
      "Stanford University\n",
      "James H. Martin\n",
      "University of Colorado at Boulder\n",
      "Copyright Â©2025. All rights reserved.\n",
      "Draft of August 24, 2025. Comments and typos welcome!Summary of Contents\n",
      "I LargeLanguageModels 1\n",
      "1 Introduction................................................... 3\n",
      "2 WordsandTokens.........................\n"
     ]
    }
   ],
   "source": [
    "#creating the text corpus from the textbook ed3\n",
    "from PyPDF2 import PdfReader\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path,'rb') as f:\n",
    "        reader = PdfReader(f)\n",
    "        for page_num, page in enumerate(reader.pages):\n",
    "            text += page.extract_text() or \"\"\n",
    "    return text \n",
    "\n",
    "pdf_path = \"C:\\Users\\moham\\Desktop\\deeplearning\\ed3book_aug25 no changes.pdf\"\n",
    "corpus_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "print(\"sample text: \\n\", corpus_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85e238dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284753\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict \n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = re.findall(r\"\\b[a-zA-Z]+\\b\", text.lower())\n",
    "    return tokens\n",
    "\n",
    "tokenized_corpus = tokenize(corpus_text)\n",
    "print(len(tokenized_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b88993c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17830\n"
     ]
    }
   ],
   "source": [
    "#building vocab\n",
    "vocab = sorted(set(tokenized_corpus))\n",
    "word2idx = {w:i for i,w in enumerate(vocab)}\n",
    "idx2word = {i:w for w,i in word2idx.items()}\n",
    "\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "746f90ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example probabilities\n",
      "         a -> 0.0285\n",
      "        aa -> 0.0001\n",
      "      aaaa -> 0.0000\n",
      "      aaab -> 0.0000\n",
      "aaacshicbzbls -> 0.0000\n",
      "      aaai -> 0.0001\n",
      "       aab -> 0.0000\n",
      "       aac -> 0.0000\n",
      "       aae -> 0.0000\n",
      "  aaffects -> 0.0000\n"
     ]
    }
   ],
   "source": [
    "#Unigram Probabilities , counting occurance of each word\n",
    "counts = Counter(tokenized_corpus)\n",
    "total = sum(counts.values())\n",
    "unigram_probs = {w: counts[w]/total for w in vocab}\n",
    "\n",
    "print(\"example probabilities\")\n",
    "for w,p in list(unigram_probs.items())[:10]:\n",
    "    print(f\"{w:>10} -> {p:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f43ec512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.160633952934649e-05"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_probs['hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44d98a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0039683515186846145"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_probs['language']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9f332a",
   "metadata": {},
   "source": [
    "## N- Gram Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "765e2bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "for i in range(len(tokenized_corpus)-1):\n",
    "    w1, w2 = tokenized_corpus[i], tokenized_corpus[i+1]\n",
    "    bigram_counts[w1][w2] += 1\n",
    "\n",
    "\n",
    "bigram_probs = defaultdict(dict)\n",
    "for w1 in bigram_counts:\n",
    "    total = sum(bigram_counts[w1].values())\n",
    "    for w2 in bigram_counts[w1]:\n",
    "        bigram_probs[w1][w2] = bigram_counts[w1][w2] / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "849e3fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'language': 0.3758169934640523,\n",
       " 'input': 0.0032679738562091504,\n",
       " 'systems': 0.0032679738562091504,\n",
       " 'number': 0.02287581699346405,\n",
       " 'set': 0.029411764705882353,\n",
       " 'corpus': 0.026143790849673203,\n",
       " 'industrialized': 0.0032679738562091504,\n",
       " 'problem': 0.006535947712418301,\n",
       " 'lan': 0.00980392156862745,\n",
       " 'enough': 0.0196078431372549,\n",
       " 'objects': 0.0032679738562091504,\n",
       " 'n': 0.00980392156862745,\n",
       " 'leading': 0.0032679738562091504,\n",
       " 'as': 0.00980392156862745,\n",
       " 'neural': 0.0032679738562091504,\n",
       " 'datasets': 0.0196078431372549,\n",
       " 'overshooting': 0.0032679738562091504,\n",
       " 'majority': 0.0032679738562091504,\n",
       " 'difais': 0.0032679738562091504,\n",
       " 'numbers': 0.0196078431372549,\n",
       " 'is': 0.0032679738562091504,\n",
       " 'weights': 0.0032679738562091504,\n",
       " 'sets': 0.0032679738562091504,\n",
       " 'because': 0.0032679738562091504,\n",
       " 'values': 0.013071895424836602,\n",
       " 'alan': 0.0032679738562091504,\n",
       " 'network': 0.0032679738562091504,\n",
       " 'perhaps': 0.0032679738562091504,\n",
       " 'more': 0.0032679738562091504,\n",
       " 'they': 0.0032679738562091504,\n",
       " 'part': 0.0032679738562091504,\n",
       " 'amount': 0.0032679738562091504,\n",
       " 'pretraining': 0.0032679738562091504,\n",
       " 'web': 0.006535947712418301,\n",
       " 'amounts': 0.029411764705882353,\n",
       " 'pretrained': 0.00980392156862745,\n",
       " 'spans': 0.006535947712418301,\n",
       " 'positive': 0.0032679738562091504,\n",
       " 'to': 0.0032679738562091504,\n",
       " 'like': 0.0032679738562091504,\n",
       " 'models': 0.00980392156862745,\n",
       " 'the': 0.0032679738562091504,\n",
       " 'for': 0.006535947712418301,\n",
       " 'batch': 0.0032679738562091504,\n",
       " 'multilingual': 0.006535947712418301,\n",
       " 'magnitudes': 0.0032679738562091504,\n",
       " 'instruction': 0.006535947712418301,\n",
       " 'feedforward': 0.0032679738562091504,\n",
       " 'kresults': 0.0032679738562091504,\n",
       " 'collection': 0.0032679738562091504,\n",
       " 'and': 0.00980392156862745,\n",
       " 'parallel': 0.00980392156862745,\n",
       " 'or': 0.0032679738562091504,\n",
       " 'phrase': 0.0032679738562091504,\n",
       " 'bilingual': 0.006535947712418301,\n",
       " 'ones': 0.006535947712418301,\n",
       " 'role': 0.0032679738562091504,\n",
       " 'peaks': 0.0032679738562091504,\n",
       " 'peak': 0.0032679738562091504,\n",
       " 'vocabularies': 0.0032679738562091504,\n",
       " 'librispeech': 0.0032679738562091504,\n",
       " 'scale': 0.032679738562091505,\n",
       " 'concern': 0.0032679738562091504,\n",
       " 'hybrid': 0.0032679738562091504,\n",
       " 'system': 0.0032679738562091504,\n",
       " 'diverse': 0.0032679738562091504,\n",
       " 'model': 0.0032679738562091504,\n",
       " 'speech': 0.006535947712418301,\n",
       " 'ontology': 0.0032679738562091504,\n",
       " 'vocabulary': 0.0196078431372549,\n",
       " 'vocabular': 0.0032679738562091504,\n",
       " 'tagsets': 0.0032679738562091504,\n",
       " 'dependency': 0.0032679738562091504,\n",
       " 'supply': 0.0032679738562091504,\n",
       " 'kitchen': 0.0032679738562091504,\n",
       " 'training': 0.006535947712418301,\n",
       " 'database': 0.0032679738562091504,\n",
       " 'text': 0.00980392156862745,\n",
       " 'semantic': 0.0032679738562091504,\n",
       " 'logical': 0.0032679738562091504,\n",
       " 'cor': 0.0032679738562091504,\n",
       " 'seed': 0.0032679738562091504,\n",
       " 'modern': 0.0032679738562091504,\n",
       " 'in': 0.0032679738562091504,\n",
       " 'background': 0.006535947712418301,\n",
       " 'are': 0.0032679738562091504,\n",
       " 'source': 0.0032679738562091504,\n",
       " 'gains': 0.0032679738562091504,\n",
       " 'chains': 0.0032679738562091504,\n",
       " 'question': 0.0032679738562091504,\n",
       " 'answer': 0.0032679738562091504,\n",
       " 'percentage': 0.0032679738562091504,\n",
       " 'window': 0.0032679738562091504,\n",
       " 'intracompany': 0.0032679738562091504,\n",
       " 'texts': 0.0032679738562091504,\n",
       " 'plain': 0.0032679738562091504,\n",
       " 'span': 0.0032679738562091504,\n",
       " 'legal': 0.0032679738562091504,\n",
       " 'webtext': 0.0032679738562091504,\n",
       " 'vocabu': 0.0032679738562091504,\n",
       " 'annotated': 0.0032679738562091504,\n",
       " 'margin': 0.0032679738562091504,\n",
       " 'corpora': 0.006535947712418301}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_probs['large']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d093d207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ngram_counts(corpus, n=3):\n",
    "    counts = defaultdict(lambda: defaultdict(int))\n",
    "    for i in range(len(corpus)-n):\n",
    "        ngram = tuple(corpus[i:i+n])\n",
    "        w2 = corpus[i+n]\n",
    "        counts[ngram][w2] += 1\n",
    "\n",
    "    ngram_probs = defaultdict(dict)\n",
    "    for ngram in counts:\n",
    "        total = sum(counts[ngram].values())\n",
    "        for w2 in counts[ngram]:\n",
    "            ngram_probs[ngram][w2] = counts[ngram][w2] / total\n",
    "    \n",
    "    return ngram_probs \n",
    "\n",
    "ngram_proba = build_ngram_counts(tokenized_corpus,n=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9834094c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae861fca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d70cdd3",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "(context-target) pairs , with context being fixed and getting target windows from -n to +n window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e12e8976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1139006\n"
     ]
    }
   ],
   "source": [
    "def gen_context_target_pairs(corpus, window_size=2):\n",
    "    pairs=[]\n",
    "    n = len(corpus)\n",
    "    for i, target in enumerate(corpus):\n",
    "        start = max(0,i-window_size)\n",
    "        end = min(n, i+window_size+1)\n",
    "        context_words = [corpus[j] for j in range(start,end) if j != i]\n",
    "        for cw in context_words:\n",
    "            pairs.append((cw, target))\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "cw_target_pairs = gen_context_target_pairs(tokenized_corpus)\n",
    "print(len(cw_target_pairs))\n",
    "\n",
    "#now when we want to do training for the context target pair, we will simply make it dictionary \n",
    "#with (cw,target) as the key and their value is [1xD] vector or any vector which is initialiased\n",
    "#randomly then , we will keep updating the value as \n",
    "#but for this approach at end we will get a pairs set of size vocab_size*vocab_size for only 2 pair context\n",
    "#and this value will keep increasing in permutation order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf134ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
