{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7564c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2) (2000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "N = 2000\n",
    "X = np.random.randn(N,2)\n",
    "y = (X[:,0] * X[:,1] > 0).astype(int)\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "380391ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y, num_classes):\n",
    "    Y = np.zeros((y.size, num_classes))\n",
    "    Y[np.arange(y.size),y] = 1\n",
    "    return Y\n",
    "\n",
    "def softmax(z):\n",
    "    z -= np.max(z, axis=1, keepdims=True)\n",
    "    exp = np.exp(z)\n",
    "    return exp/ np.sum(exp, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06246b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_dims, out_dims):\n",
    "        self.W = np.random.randn(in_dims, out_dims) * np.sqrt(2. / in_dims)\n",
    "        self.b = np.zeros((1, out_dims))\n",
    "    def forward(self,x):\n",
    "        self.x = x\n",
    "        return x @ self.W + self.b\n",
    "    def backward(self, grad_out, lr):\n",
    "        dW = self.x.T @ grad_out / self.x.shape[0]\n",
    "        db = np.mean(grad_out, axis=0, keepdims=True)\n",
    "        grad_x = grad_out @ self.W.T\n",
    "        # update weights\n",
    "        self.W -= lr * dW\n",
    "        self.b -= lr * db \n",
    "        return grad_x \n",
    "\n",
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        self.mask = x > 0\n",
    "        return x * self.mask \n",
    "    def backward(self, grad_out, lr):\n",
    "        return grad_out * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8f1d0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxCrossEntropy:\n",
    "    def forward(self, logits, y_true):\n",
    "        self.probs = softmax(logits)\n",
    "        self.y_true = y_true \n",
    "        N = logits.shape[0]\n",
    "        # compute loss\n",
    "        log_likelihood = -np.log(self.probs[np.arange(N),y_true] + 1e-12) #basically we will take the value from probs , at the index at y_true where it is 1\n",
    "        return np.sum(log_likelihood) / N\n",
    "    def backward(self):\n",
    "        N = self.y_true.shape[0]\n",
    "        grad = self.probs.copy()\n",
    "        grad[np.arange(N), self.y_true] -= 1\n",
    "        return grad / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe517cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, in_dims, hidden_dims, out_dims, lr=0.1):\n",
    "        self.lr = lr\n",
    "        self.l1 = Linear(in_dims, hidden_dims)\n",
    "        self.relu = ReLU()\n",
    "        self.l2 = Linear(hidden_dims, out_dims)\n",
    "        self.loss_fn = SoftmaxCrossEntropy()\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.l1.forward(x)\n",
    "        out = self.relu.forward(out)\n",
    "        out = self.l2.forward(out)\n",
    "        return out \n",
    "    \n",
    "    def backward(self,grad):\n",
    "        grad = self.l2.backward(grad, self.lr)\n",
    "        grad = self.relu.backward(grad, self.lr)\n",
    "        grad = self.l1.backward(grad, self.lr)\n",
    "        return grad\n",
    "    \n",
    "    def predict(self, x):\n",
    "        logits = self.forward(x)\n",
    "        probs = softmax(logits)\n",
    "        return np.argmax(probs, axis=1)\n",
    "    \n",
    "    def train_batch(self, x, y):\n",
    "        logits = self.forward(x)\n",
    "        loss = self.loss_fn.forward(logits, y)\n",
    "        grad = self.loss_fn.backward()\n",
    "        self.backward(grad)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "925dda0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020: Loss=0.9945, Acc=0.608\n",
      "Epoch 040: Loss=0.9941, Acc=0.608\n",
      "Epoch 060: Loss=0.9936, Acc=0.608\n",
      "Epoch 080: Loss=0.9931, Acc=0.609\n",
      "Epoch 100: Loss=0.9927, Acc=0.609\n",
      "Epoch 120: Loss=0.9922, Acc=0.609\n",
      "Epoch 140: Loss=0.9917, Acc=0.609\n",
      "Epoch 160: Loss=0.9913, Acc=0.609\n",
      "Epoch 180: Loss=0.9908, Acc=0.609\n",
      "Epoch 200: Loss=0.9904, Acc=0.609\n",
      "Epoch 220: Loss=0.9899, Acc=0.609\n",
      "Epoch 240: Loss=0.9894, Acc=0.609\n",
      "Epoch 260: Loss=0.9890, Acc=0.609\n",
      "Epoch 280: Loss=0.9885, Acc=0.609\n",
      "Epoch 300: Loss=0.9881, Acc=0.609\n",
      "Epoch 320: Loss=0.9876, Acc=0.609\n",
      "Epoch 340: Loss=0.9872, Acc=0.609\n",
      "Epoch 360: Loss=0.9867, Acc=0.609\n",
      "Epoch 380: Loss=0.9862, Acc=0.609\n",
      "Epoch 400: Loss=0.9858, Acc=0.610\n",
      "Epoch 420: Loss=0.9853, Acc=0.610\n",
      "Epoch 440: Loss=0.9849, Acc=0.610\n",
      "Epoch 460: Loss=0.9844, Acc=0.610\n",
      "Epoch 480: Loss=0.9840, Acc=0.609\n",
      "Epoch 500: Loss=0.9835, Acc=0.609\n",
      "Epoch 520: Loss=0.9830, Acc=0.610\n",
      "Epoch 540: Loss=0.9826, Acc=0.610\n",
      "Epoch 560: Loss=0.9821, Acc=0.610\n",
      "Epoch 580: Loss=0.9816, Acc=0.610\n",
      "Epoch 600: Loss=0.9812, Acc=0.610\n",
      "Epoch 620: Loss=0.9807, Acc=0.609\n",
      "Epoch 640: Loss=0.9802, Acc=0.609\n",
      "Epoch 660: Loss=0.9798, Acc=0.610\n",
      "Epoch 680: Loss=0.9793, Acc=0.610\n",
      "Epoch 700: Loss=0.9788, Acc=0.610\n",
      "Epoch 720: Loss=0.9784, Acc=0.610\n",
      "Epoch 740: Loss=0.9779, Acc=0.610\n",
      "Epoch 760: Loss=0.9774, Acc=0.610\n",
      "Epoch 780: Loss=0.9770, Acc=0.610\n",
      "Epoch 800: Loss=0.9765, Acc=0.610\n",
      "Epoch 820: Loss=0.9760, Acc=0.611\n",
      "Epoch 840: Loss=0.9756, Acc=0.611\n",
      "Epoch 860: Loss=0.9751, Acc=0.611\n",
      "Epoch 880: Loss=0.9746, Acc=0.611\n",
      "Epoch 900: Loss=0.9741, Acc=0.610\n",
      "Epoch 920: Loss=0.9737, Acc=0.611\n",
      "Epoch 940: Loss=0.9732, Acc=0.611\n",
      "Epoch 960: Loss=0.9727, Acc=0.610\n",
      "Epoch 980: Loss=0.9722, Acc=0.610\n",
      "Epoch 1000: Loss=0.9717, Acc=0.610\n",
      "Epoch 1020: Loss=0.9712, Acc=0.610\n",
      "Epoch 1040: Loss=0.9707, Acc=0.610\n",
      "Epoch 1060: Loss=0.9702, Acc=0.611\n",
      "Epoch 1080: Loss=0.9697, Acc=0.610\n",
      "Epoch 1100: Loss=0.9692, Acc=0.610\n",
      "Epoch 1120: Loss=0.9687, Acc=0.611\n",
      "Epoch 1140: Loss=0.9682, Acc=0.611\n",
      "Epoch 1160: Loss=0.9677, Acc=0.611\n",
      "Epoch 1180: Loss=0.9672, Acc=0.611\n",
      "Epoch 1200: Loss=0.9667, Acc=0.611\n",
      "Epoch 1220: Loss=0.9661, Acc=0.611\n",
      "Epoch 1240: Loss=0.9656, Acc=0.611\n",
      "Epoch 1260: Loss=0.9651, Acc=0.611\n",
      "Epoch 1280: Loss=0.9645, Acc=0.611\n",
      "Epoch 1300: Loss=0.9640, Acc=0.611\n",
      "Epoch 1320: Loss=0.9635, Acc=0.611\n",
      "Epoch 1340: Loss=0.9629, Acc=0.611\n",
      "Epoch 1360: Loss=0.9624, Acc=0.611\n",
      "Epoch 1380: Loss=0.9618, Acc=0.611\n",
      "Epoch 1400: Loss=0.9613, Acc=0.611\n",
      "Epoch 1420: Loss=0.9607, Acc=0.611\n",
      "Epoch 1440: Loss=0.9601, Acc=0.611\n",
      "Epoch 1460: Loss=0.9596, Acc=0.611\n",
      "Epoch 1480: Loss=0.9590, Acc=0.611\n",
      "Epoch 1500: Loss=0.9584, Acc=0.611\n",
      "Epoch 1520: Loss=0.9578, Acc=0.611\n",
      "Epoch 1540: Loss=0.9572, Acc=0.610\n",
      "Epoch 1560: Loss=0.9566, Acc=0.610\n",
      "Epoch 1580: Loss=0.9560, Acc=0.610\n",
      "Epoch 1600: Loss=0.9554, Acc=0.610\n",
      "Epoch 1620: Loss=0.9548, Acc=0.610\n",
      "Epoch 1640: Loss=0.9542, Acc=0.610\n",
      "Epoch 1660: Loss=0.9536, Acc=0.611\n",
      "Epoch 1680: Loss=0.9530, Acc=0.611\n",
      "Epoch 1700: Loss=0.9524, Acc=0.611\n",
      "Epoch 1720: Loss=0.9518, Acc=0.611\n",
      "Epoch 1740: Loss=0.9513, Acc=0.611\n",
      "Epoch 1760: Loss=0.9507, Acc=0.611\n",
      "Epoch 1780: Loss=0.9501, Acc=0.610\n",
      "Epoch 1800: Loss=0.9495, Acc=0.610\n",
      "Epoch 1820: Loss=0.9489, Acc=0.610\n",
      "Epoch 1840: Loss=0.9483, Acc=0.610\n",
      "Epoch 1860: Loss=0.9477, Acc=0.609\n",
      "Epoch 1880: Loss=0.9471, Acc=0.609\n",
      "Epoch 1900: Loss=0.9466, Acc=0.609\n",
      "Epoch 1920: Loss=0.9460, Acc=0.609\n",
      "Epoch 1940: Loss=0.9454, Acc=0.609\n",
      "Epoch 1960: Loss=0.9448, Acc=0.609\n",
      "Epoch 1980: Loss=0.9442, Acc=0.608\n",
      "Epoch 2000: Loss=0.9437, Acc=0.608\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "in_dim = 2\n",
    "hidden_dim = 8\n",
    "out_dim = 2\n",
    "lr = 0.1\n",
    "epochs = 2000\n",
    "\n",
    "model = MLP(in_dim, hidden_dim, out_dim, lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = model.train_batch(X, y)\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        preds = model.predict(X)\n",
    "        acc = np.mean(preds == y)\n",
    "        print(f\"Epoch {epoch+1:03d}: Loss={loss:.4f}, Acc={acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1a45440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_grad_check(layer, x, grad_out, eps=1e-5):\n",
    "    analytic = layer.x.T @ grad_out / x.shape[0]\n",
    "    num_grad = np.zeros_like(layer.W)\n",
    "    for i in range(layer.W.shape[0]):\n",
    "        for j in range(layer.W.shape[1]):\n",
    "            old = layer.W[i, j]\n",
    "            layer.W[i, j] = old + eps\n",
    "            loss1 = np.sum(layer.forward(x))\n",
    "            layer.W[i, j] = old - eps\n",
    "            loss2 = np.sum(layer.forward(x))\n",
    "            layer.W[i, j] = old\n",
    "            num_grad[i, j] = (loss1 - loss2) / (2 * eps)\n",
    "    print(\"Diff:\", np.linalg.norm(analytic - num_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3076ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
