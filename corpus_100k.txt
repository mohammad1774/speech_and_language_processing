speech language processing introduction natural language processing computational linguistic speech recognition language model edition draft daniel jurafsky stanford university james martin university colorado boulder copyright right reserve draft august comment typo content large language model introduction word token gram language model logistic regression embedding neural network large language model transformer mask language model post training instruction tuning alignment test time pute retrieval base model machine translation rnn lstms phonetic speech feature extraction automatic speech recognition text speech annotate linguistic structure sequence labeling part speech name entity context free grammar constituency parse dependency parsing information extraction relation event time semantic role labeling lexicon sentiment affect connotation coreference resolution entity link discourse coherence conversation structure bibliography subject index large language model introduction word token word morpheme part word unicode subword tokenization byte pair encoding rule base tokenization corpora regular expression simple unix tool word tokenization minimum edit distance summary historical note exercise gram language model gram evaluate language model training test set evaluate language model perplexity sample sentence language model generalizing overﬁtte training set smoothing interpolation backoff advance perplexity relation entropy summary historical note exercise logistic regression machine learning classiﬁcation sigmoid function classiﬁcation logistic regression multinomial logistic regression learning logistic regression cross entropy loss function gradient descent learning multinomial logistic regression evaluation precision recall measure test set cross validation statistical signiﬁcance testing avoid harm classiﬁcation interpreting model advance regularization advance derive gradient equation summary historical note exercise embedding lexical semantic vector semantic intuition simple count base embedding cosine measure similarity visualize embedding semantic property embedding bias embedding evaluate vector model summary historical note exercise neural network unit xor problem feedforward neural network feedforward network nlp classiﬁcation embedding input neural net classiﬁer training neural net summary historical note large language model architecture language model conditional generation text intuition prompt generation sampling training large language model evaluate large language model ethical safety issue language model summary historical note transformer attention transformer block parallelizing computation single matrix input embedding token position language modeling head sampling training deal scale interpret transformer summary historical note mask language model bidirectional transformer encoder training bidirectional encoder contextual embedding fine tune classiﬁcation fine tune sequence labelling name entity recognition summary historical note post training instruction tuning alignment test time compute instruction tuning learning preference llm alignment preference base learning test time compute summary historical note retrieval base model information retrieval information retrieval dense vector answer question rag question answer dataset evaluating question answer summary historical note exercise machine translation language divergence typology machine translation encoder decoder detail encoder decoder model decode beam search translating low resource situation evaluation bias ethical issue summary historical note exercise rnn lstms recurrent neural network rnn language model rnns nlp task stack bidirectional rnn architecture lstm summary common rnn nlp architecture encoder decoder model rnns attention summary historical note phonetic speech feature extraction speech sound phonetic transcription articulatory phonetic prosody acoustic phonetic signal feature extraction speech recognition log mel spectrum mfcc mel frequency cepstral coefﬁcient summary historical note exercise automatic speech recognition automatic speech recognition task convolutional neural network encoder decoder architecture asr self supervise model hubert ctc asr evaluation word error rate summary historical note exercise text speech tt overview codec learn discrete audio token generate audio stage tts evaluation speech task speak language model summary historical note exercise annotate linguistic structure sequence labeling part speech name entity english word class speech tagging name entity name entity tagging hmm speech tagging conditional random field crf evaluation name entity recognition detail summary historical note exercise context free grammar constituency parse constituency context free grammar treebank grammar equivalence normal form ambiguity cky parse dynamic programming approach span base neural constituency parse evaluating parser head head finding summary historical note exercise dependency parse dependency relation transition base dependency parsing graph base dependency parsing evaluation summary historical note exercise information extraction relation event time relation extraction relation extraction algorithm extract event represent time represent aspect temporally annotate dataset timebank automatic temporal analysis template filling summary historical note exercise semantic role label semantic role diathesis alternation semantic role problem thematic role proposition bank framenet semantic role labeling selectional restriction primitive decomposition predicate summary historical note exercise lexicon sentiment affect connotation deﬁne emotion available sentiment affect lexicon create affect lexicon human labeling semi supervised induction affect lexicon supervised learning word sentiment lexicon sentiment recognition lexicon affect recognition lexicon base method entity centric affect connotation frame summary historical note exercise coreference resolution entity link coreference phenomena linguistic background coreference task dataset mention detection architecture coreference algorithm classiﬁer hand build feature neural mention rank algorithm entity link evaluation coreference resolution winograd schema problem gender bias coreference summary historical note exercise discourse coherence coherence relation discourse structure parse centering entity base coherence representation learning model local coherence global coherence summary historical note exercise conversation structure property human conversation dialog act corpora bibliography subject index olume large language model ﬁrst book introduce fundamental suite algorithmic linguistic tool modern neural large language model begin tokenization preprocessing include unicode proceed duce basic language modeling idea simple gram language model introduce algorithm component large language model logistic regression embedding feedforward network ready introduce principle large language modeling encoder decoder ing fundamental transformer architecture mask language model architecture like rnns lstms information retrieval base algorithm like rag machine translation encoder decoder model ﬁnally speak language modeling include asr derni ere choose trouve faisant ouvrage est savoir celle faut mettre premi ere thing ﬁgure write book ﬁrst pascal ords token chapter token user need help certain eliza mean get help user learn mother eliza tell family user mother take care eliza family take care user father eliza father user like father dialogue eliza early natural language processing system eliza carry limited conversation user imitate response rogerian psychotherapist weizenbaum eliza surprisingly simple program use pattern match word recognize phrase like need change word suitable output like mean get eliza mimicry human conversation crude modern standard remarkably successful people interact eliza come believe understand result work lead researcher ﬁrst think impact chatbot user weizenbaum course modern chatbot use simple pattern base mimicry eliza pioneer pattern base approach word instantiate eliza relevant today context tokenization task separate tokenization tokenizing word word part run text tokenization ﬁrst step modern nlp include pattern base approach date eliza understand tokenization ﬁrst need ask word uma word new york nature word similar language language like vietnamese cantonese short word like turkish long word need think represent word term character introduce unicode modern system ing character text encoding introduce morpheme meaningful subpart word like morpheme word long standard way tokenize text use input character guide understand possible subpart word introduce dard byte pair encoding bpe algorithm automatically break input text bpe token algorithm use simple statistic letter sequence induce vocabulary subword tokens tokenization system depend regular expression processing step regular expression language formallyregular expression specify manipulate text string important tool modern nlp tem introduce regular expression example use finally introduce metric call edit distance measure similar word string base number edit insertion deletion tution take change string edit distance play role nlp need compare word string example crucial word error rate metric automatic speech ords word word following sentence picnic pool lie grass look star sentence word count punctuation word count punctuation treat period comma word depend task punctuation critical ﬁnde boundary thing mas period colon identify aspect meaning question mark exclamation mark quotation mark large language model generally count tuation separate word speak language introduce complication regard deﬁning word utterance speak conversation utterance technical utterance linguistic term spoken correlate sentence mainly business datum process utterance kind disﬂuencie break word disﬂuency call fragment word like uhandumare call ﬁller orﬁlle pause fragment ﬁlle pause consider word depend application build speech transcription system want eventually strip disﬂuencie disﬂuencie disﬂuencie like orumare actually helpful speech recognition predict upcoming word signal speaker restart clause idea speech recognition treat regular word different people use different disﬂuencie cue speaker identiﬁcation fact clark fox tree show uhandumhave different meaning english think important think word need distinguish way talk word useful book word type word type number distinct word corpus set word vocabulary isv number type vocabulary sizejvj word instance total word instance number nof run ignore punctuation picnic sentence type instance picnic pool lie grass look star decision example consider capitalize string like uncapitalize like word type answer depend task andtheymight lump type task care formatting task capitalization useful feature retain version particular nlp model capitalization capitalization far talk orthographic word word base english writing system possible way deﬁne word example orthographically word grammatically function word subject pronoun iand verb short early tradition occasionally word instance refer word token try reserve word token instead mean output subword tokenization ords tokens corpus type jvjinstance shakespeare thousand thousand brown corpus thousand million switchboard telephone conversation thousand million coca million million google gram million trillion figure rough number wordform type instance english language corpora large google grams corpus contain million type count include type appear time true number large distinction hard start think guage example write system language like chinese japanese thai simply orthographic word use space mark potential word boundary chinese example word compose character call hanzi chinese character generally represent single hanzi unit meaning call morpheme introduce pronounceable single syllable word character long average chinese orthographic word decide count word chinese complex example consider following sentence ıng zˇong yao ming reach ﬁnal chen point treat word deﬁnition word call chinese treebank deﬁnition chinese name family follow personal name treat single word yaoming进入 reaches总决赛 ﬁnal sentence treat word peke university standard name separate unit adjective appear distinct word yao明 ming进入 reaches总 overall决赛 ﬁnal finally possible chinese simply ignore word altogether use character basic element treat sentence series character work pretty chinese character reasonable semantic level application yao明 ming进 enter入 enter总 overall决 decision赛 game method work japanese thai individual character small unit issue deﬁning word make hard use word basis tokenize text nlp language problem word word english speak number word language generally refer word type fig show rough number type instance compute english corpora notice large corpora look word type ﬁnd suggest clear answer word answer keep grow datum fact ords relationship number type jvjand number instance nis call herdan law herdan heap law heap herdan law heap law discoverer linguistic information retrieval respectively show kandbare positive constant jvj value bdepend corpus size genre number high report roughly vocabulary size text go little fast square root length word variant law capture fact distinguish roughly class word function word grammatical word like function word english aandof tend grow indeﬁnitely language tend ﬁxed number content word noun adjective verb tend content word meaning people place event noun especially ular noun like name technical term tend grow indeﬁnitely model sensitive difference function word content word value bfor initial corpus word appear second bafterword content word appear fig show example tria show value bfor heap law compute gutenberg corpus book entropy figure growth number distinct word compute gutenberg corpus text position text corpus choose random case similar behaviour observe system zipf heap law section compare law observe zipf law frequency occurrence element system heap law temporal appearance claim heap zipf law trivially related derive heap law zipf know true general turn true speciﬁc hypothesis random sampling follow suppose existence strict power law behaviour frequency rank distribution construct sequence element randomly sample zipf distribution procedure recover heap law functional form order need consider correct expression include normalisation factor expression derive follow approximate integral zrmax let distinguish case obtain neglect term maxin equation write max figure ocabulary size function text length compute gutenberg corpus publicly available book figure tria fact word grow end lead problem computational model matter big vocabulary vocabulary capture possible word occur mean computational model constantly unknown word word see huge problem machine learning model problem ﬁrst language graphic word deﬁne post hoc challenging second ber word grow bound language model nlp model tend use word unit processing instead use small unit call subword recombine model new word model see think deﬁning subword ﬁrst need talk unit small word morpheme andcharacter ords token morpheme part word word part level character obvious word catsis pose character true subtle level word component coherent meaning nent call morpheme study morpheme call morphology morphology morpheme minimal meaning bear unit language example morpheme word foxconsist morpheme morpheme fox word catsconsist morpheme catand morpheme indicate plural sentence english segment morpheme hyphen doc work care ful wash ing glass mention chinese conveniently writing system set character mainly describe morpheme sentence mandarin chinese morpheme character glossed follow translation plum干 dry菜 vegetable用 use清 clear水 water泡 soak软 soft remove出 out后 drip干 dry chop碎 fragment soak preserve vegetable water soft remove drain chop generally distinguish broad class morpheme root central root morpheme word supply main meaning afﬁxe add afﬁx ditional meaning kind english example word work work root afﬁx similarly glass glass root afﬁx afﬁxe fall class correctly continuum pole end inﬂectional morpheme grammatical morpheme thatinﬂectional morpheme tend play syntactic role mark agreement example english inﬂectional morpheme mark plural noun tional morpheme mark past tense verbs inﬂectional morpheme tend productive obligatory meaning tend predictable derivational morpheme idiosyncratic application morpheme usually apply speciﬁc subclass word result word ferent grammatical class root meaning hard predict exactly example word care noun combine derivational afﬁx produce adjective careful derivational afﬁx result adverb carefully class morpheme clitic clitic morpheme act clitic syntactically like word reduce form attach phonologically orthographically word example english morpheme vein word clitic grammatical meaning word form appear sentence english possessive morpheme sin phrase teacher book clitic french deﬁnite article word clitic preposition arabic like conjunction like study language vary morphology word break part call morphological typology morphology lan morphological typology guage differ dimension dimension particularly relevant computational word orpheme part word ﬁrst dimension number morpheme word language like vietnamese cantonese word average morpheme language end scale isolate language example isolating word follow cantonese sentence morpheme syllable building say big house country alternatively language like koryak chukotko kamchatkan language ken northern kamchatka peninsula russia single word morpheme correspond sentence english arkadiev kurebito language end scale synthetic synthetic guage end scale polysynthetic language polysynthetic mej midnight big sew lot yurt cover middle night koryak chukotko kamchatkan russia kurebito fig show early computation morpheme word language linguistic typologist joseph greenberg wordpolysynthetic figure early estimate morpheme word joseph greenberg second dimension degree morpheme easily segmentable range agglutinative language like turkish morpheme agglutinative atively clean boundary fusion language like russian single afﬁx fusion conﬂate multiple morpheme like word stolom instr decl fuse distinct morphological category instrumental singular ﬁrst declension english read sthe article example fusion sufﬁx mean person singular mean present tense way divide meaning different part loosely talk property analytic polysynthetic fusional agglutinative property language fact language use different morphological system accurate talk general tendency nonetheless fact morpheme hard deﬁne language complex morpheme easy break piece make difﬁcult use morpheme standard tokenization cross chapter ords token unicode option consider tokenization level individual acter represent character language write system theunicode standard method represent text write character unicode script language world include dead language like sumerian cuneiform invent language like klingon let start brief historical note english speciﬁc subset unicode technically call basic latin unicode commonly refer ascii start latin character write english like one sentence represent code call ascii american standard ascii code information interchange ascii represent character single byte byte represent different character ascii high order bit ascii byte set actually rest control code obsolete machine call teletype ascii character representation hex decimal hex dec hex dec hex dec hex dec figure select ascii code english letter code show hexadecimal decimal ascii course insufﬁcient lot character world writing system script use latin character ascii example spanish phrase mean sir reply sancho non ascii character respondi lot language base latin character devanagari devanagari script language include hindi marathi nepali sindhi skrit devanagari example hindi text universal declaration human right chinese chinese character unicode include ping non overlapping variant chinese japanese korean namese collectively refer cjkv character different script port unicode script world add unicode script ern language chinese arabic hindi cherokee ethiopic khmer turkish spanish script ancient language cuneiform ugaritic egyptian hieroglyph pahlavi mathematical symbol emoji currency symbol code point work unicode assign unique call code point code nicode character code point abstract representation character code point represent number traditionally write hexadecimal number decimal have million code point mean lot room new character traditional represent code point preﬁx mean following unicode hex representation code point code point character note unicode design backwards compatible ascii mean ﬁrst code point include code fora identical ascii sample code point come description alatin small letter blatin small letter clatin small letter ulatin small letter grave ulatin small letter acute ulatin small letter circumflex ulatin small letter diaeresis jurafsky grinning face jurafsky mahjong tile character note code point speciﬁy glyph visual representation glyph character glyph store font code point abstract representation indeﬁnite number visual representation example different font like time roman courier different font style like boldface italic represent code encode code point unique abstract unicode representation character stick text ﬁle instead need represent character text string write encoding character different possible encoding method encode encoding method call far frequent example entire web encode let talk encoding unicode representation word hello sist follow sequence code point imagine simple encoding method write code point ﬁle million character bit byte need use byte bit capture bit need represent million character byte inconvenient use multiple byte byte representation word hello encode ing set chapter ords token use encoding technically call make ﬁle time long ascii make ﬁle big zero zero cause problem turn have byte completely zero mess thing backwards compatibility ascii base system historically byte end string marker instead common encoding standard unicode tion format represent character efﬁciently few byte erage write character few byte byte variable length encoding length encoding character ﬁrst code point set ascii character encode single byte encoding hello conveniently mean ﬁle encode ascii valid coding variable length encoding mean code point encode sequence byte byte confuse ascii byte indicate ﬁrst bit byte byte byte encoding code point encode bit value byte byte byte byte xxxxxxxx yyxxxxxx zzzzyyyy yyxxxxxx zzzzyyyy yyxxxxxx figure mapping unicode code point variable length encoding give code point range bit value column pack byte figure adapt unicode core spec chapter table fig show mapping occur example rule explain character code point bit sequence blue indicate sequence yyyyy red sequence xxxxxx encode byte bit sequence result rule ﬁrst character ascii map byte ing character european middle eastern african script map byte chinese japanese korean character map byte rarer cjkv character emoji symbol map byte number advantage relatively efﬁcient few byte commonly encounter character use zero byte literally represent null character backwards compatible ascii self synchronize mean ﬁle corrupt possible ﬁnd start prior character move byte leave right unicode python start python python string store ternally unicode string sequence unicode code point string function regular expression apply natively code point example function like len string return length character code point length byte read write ﬁle code point need encode decode method like ﬁle encode ubword tokenization byte pairencode encoding old encoding method like ascii thing text ﬁle encoding encoding method speciﬁed python open ﬁle reading writing subword tokenization byte pair encoding tokenization ﬁrst stage natural language processing process tokenization mente run input text tokens token see candidate token word morpheme character problem unit word morpheme approximately right level nlp processing tend consistent meaning challenging deﬁne formally character clear deﬁne small unit choose token section introduce practice nlp use data drive approach deﬁne token generally result unit size pheme word occasionally use unit small character tokenize input reason convert input deterministic ﬁxed set unit mean different algorithm system agree simple question example long text unit ornew york token standardizing essential cability nlp experiment algorithm introduce book like perplexity metric language model assume text ﬁxed tokenization tokenization algorithm include small token morpheme letter eliminate problem unknown word chapter nlp algorithm learn fact language corpus training corpus use fact decision separate testcorpus language training corpus contain word low new new low word low appear test corpus system know deal unknown word problem modern tokenizer automatically duce set token include token small word call subword subword word arbitrary substring meaning bear unit like morpheme modern tokenization scheme token word token frequently occur morpheme subword like unseen word represent sequence know subword unit example happen word low pear segment successfully lowanderwhich see bad case unusual word acronym like grpo tokenize sequence individual letter necessary tokenization algorithm widely modern language model pair encoding bpe sennrich unigram language modeling ulm kudo section introduce byte pair encode orbpe bpe algorithm sennrich gage fig like tokenization scheme bpe algorithm part trainer encoder general token training phase raw training corpus library include implementation kudo richardson people use sentencepiece simply mean ulm chapter ords token usually roughly pre separate word example whitespace induce vocabulary set token token encoder raw test sentence encode token vocabulary learn training bpe training thebpe training algorithm iteratively merge frequent neighboring token create long long token algorithm begin vocabulary set individual character examine training corpus ﬁnd character frequently adjacent imagine original corpus character long vocabulary character frequent neighboring pair character merge add new merge token vocabulary replace adjacent corpus new vocabulary possible token abg corpus length frequent pair token merge lead vocabulary token cabg corpus length cab cab algorithm continue count merge create new long long character string kmerge create knovel token kis parameter algorithm result vocabulary consist original set character plus know symbol core algorithm additional complication practice instead run raw sequence character algorithm usually run inside word algorithm merge word boundary input corpus ﬁrst separate white space punctuation regular expression deﬁne later chapter give starting set string sponde character word white space usually attach start word count word count come corpus merge allow string let algorithm work tiny synthetic corpus explicitly mark space break corpus word lead whitespace count merge allow word boundary result look like follow list word start vocabulary character corpus vocabulary realize particularly likely exciting ubword tokenization byte pairencode bpe training algorithm ﬁrst count pair adjacent symbol frequent pair occur new frequency renew quency total occurrence merge symbol treat symbol count corpus vocabulary frequent pair total merge corpus vocabulary new new count merge total count get merge system essentially induce word initial preﬁx corpus vocabulary new new continue merge merge current vocabulary new new new new new new renew new new renew new new renew set function byte pair encoding string number merge vocab unique character initial set token character merge tokens ktime frequent pair adjacent token tnew new token concatenate update vocabulary replace occurrence trincwith tnew update corpus return figure training bpe algorithm take corpus break dividual character byte learn vocabulary iteratively merge token figure adapt bostrom durrett bpe encoder learn vocabulary bpe encoder tokenize test sentence encoder run test datum merge learn chapter ords token training datum run greedily order learn frequency test datum play role frequency training data ﬁrst segment test sentence word character apply ﬁrst rule replace instance ein test corpus second rule replace instance test corpus new end course merge simple recreate word training set merge create knowledge morpheme like preﬁx appear unseen combination like revisit orrearrange morpheme new initial space word internal appear start sentence word unseen training like anew course real setting bpe run ten thousand merge large input corpus produce vocabulary size token result word represent single token rare word unknown word represent multiple token english multilingual system token dominate english leave few token language discuss bpe practice example show simple bpe learn sequence ascii byte bpe work unicode input normally run bpe individual byte encode text unicode representation text series code point encode byte treat individual byte input bpe bpe likely begin rediscover byte common byte sequence use encode code point run bpe inside presegmente word help avoid problem possible value byte unknown ken possible bpe learn illegal sequence character boundary rare eliminate ﬁlter let example industrial application bpe tokenizer large system like openai tokenizer token comparatively large number use tat dat duong tiktokenizer visualizer number token give sentence example tokenization nonsense sentence visualizer use center dot indicate space visualization show color separate word course true tokenizer simply sequence unique token id case tereste follow token notice word token usually include lead space clitic like sare segment appear proper noun like jane count word frequent word like number tend segment chunk digit word like segment differently appear capitalize sentence initially token andhow appear space low case token relate preprocesse step mention brieﬂy language model usually create token pretokenization stage ﬁrst pretokenization ment input regular expression example break input space punctuation strip clitic break number set ule base tokenization use regular expression section possible change pretokenization allow bpe token span multiple word example superbpe algorithm ﬁrst induce regular bpe subword superbpe token enforce pretokenization run second stage bpe allow merge space punctuation result large set token efﬁcient fig preprint figure superbpe tokenizer encode text efﬁciently bpe gap grow large vocabulary size encode efﬁciency axis measure byte token number byte encode token average large corpus text text byte superbpe use token bpe use method efﬁciencie byte token respectively graph encoding efﬁciency bpe plateaus early exhaust valuable delimit word training datum fact bound gray dotted line show maximum achievable encoding efﬁciency bpe delimit word vocabulary hand superbpe dramatically well encode efﬁciency continue improve increase vocabulary size continue add common word sequence treat token vocabulary different gradient line different transition point learn subword superword token give immediate improvement superbpe well encode efﬁciency naive variant bpe use whitespace pretokenization perform language include multi word token promise beneﬁcial way lead short token sequence lower computational cost training inference offer representational advantage segment text semantically cohesive unit salehi work introduce superword tokenization algorithm produce vocabulary subword superword token use refer token bridge word method superbpe introduce pretokenization curriculum lar byte pair encoding bpe algorithm sennrich whitespace pretokenization initially enforce learning subword tokens conventional bpe disable second stage tokenizer transition learn superword token notably superbpe tokenizer scale well vocabulary size bpe quickly hit point diminish return begin add increasingly rare subword vocabulary superbpe continue discover common word sequence treat single token improve encode efﬁciency figure main experiment pretrain english lm scale scratch ﬁxe model size vocabulary size training compute vary algorithm learn vocabulary ﬁnd model train superbpe tokenizer consistently signiﬁcantly improve counterpart train bpe tokenizer efﬁcient inference time good superbpe model achieve average figure superbpe algorithm create large token allow second stage merge space figure liu tokenizer practice large language model gual train language training datum large language model vastly dominate english text multilingual bpe tokenizer tend use token english leave few language result well job tokenize english language tend word split short token example let look spanish sentence recipe plantain english translation english token word token word split multiple token contrast original word spanish encode token large number notice basic word break piece example hondo deep segment handondo similarly jugo nut jenjibre ginger spanish particularly low resource language oversegmenting low resource language individual character oversegmente tiny token cause problem stream processing language clear introduce transformer model chapter fragmentation lead poor tion meaning need long context high cost train model rust ahia rule base tokenization data base tokenization like bpe common way tion situation want constrain token word subword useful run parse algorithm english parser need grammatical word input useful linguistic application prior deﬁnition token chapter ords token interested study useful social science application orthographic word useful domain study rule base tokenization pre deﬁne standard implement rule plement kind tokenization let explore english word tokenization desideratum english want break tion separate token comma useful piece information parser period help indicate sentence boundary want punctuation occur word internally example like special character number need keep price date want segment price separate token url twitter hashtag nlproc email address number expression introduce complication addition appear word boundary commas appear inside number english digit tokenization differ language language like spanish french german example use comma mark decimal point space period english put comma example rule base tokenizer expand clitic contraction clitic mark apostrophe convert token towe clitic word stand cur attach word contraction occur alphabetic language include french pronoun article depend application tokenization algorithm tokenize tiword expression like new york orrock roll single token quire multiword expression dictionary sort rule base tokenization intimately tie name entity recognition task detect name date organization chapter commonly tokenization standard know penn treebank kenization standard parse corpora treebank release lin penn treebank tokenization guistic data consortium ldc source useful dataset standard separate clitic plus keeps hyphenate word gether separate punctuation save space show visible space token newline common output input san francisco base restaurant say charge output thesanfrancisco base restaurant theysaid practice tokenization run language processing need fast rule base word tokenization generally use ministic algorithm base regular expression compile efﬁcient ﬁnite state automata example fig show basic regular expression tokenize english tokenize function python base natural language toolkit nltk bird carefully design deterministic algorithm deal ambiguity arise fact apostrophe need tokenize differently genitive marker book cover quotative class say clitic like orpora text poster print cost pattern set flag allow verbose regexps abbreviation word optional internal hyphen currency percentage ellipsis separate token include pattern poster print cost figure python trace regular expression tokenization nltk python base natural language processing toolkit bird comment readability verbose ﬂag tell python strip comment whitespace figure chapter bird sentence segmentation rule base segmentation commonly kind tokenization cess sentence sentence segmentation step optionally appliedsentence segmentation text processing especially important apply nlp algorithm task detect structure like parse structure sentence segmentation depend language genre useful cue segment text sentence english write text tend tuation like period question mark exclamation point question mark exclamation point relatively unambiguous marker sentence boundary simple rule segment sentence appear period character hand ambiguous sentence boundary marker marker abbreviation like previous tence read show complex case ambiguity ﬁnal period abbreviation sentence ary marker reason sentence tokenization word tokenization address jointly english sentence tokenization method work ﬁrst decide base deterministic rule machine learning period word sentence boundary marker abbreviation dictionary help determine period commonly abbreviation narie hand build machine learn kiss strunk ﬁnal sentence splitter stanford corenlp toolkit man ple sentence splitting rule base deterministic consequence tokenization sentence end sentence end punctuation group character token abbreviation number optionally follow additional ﬁnal quote bracket corpora word appear particular piece text study produce speciﬁc speaker writer speciﬁc dialect speciﬁc language speciﬁc time speciﬁc place speciﬁc chapter ords token important dimension variation language nlp rithm useful apply language world language time writing accord online ethnologue catalog simon fennig important test algorithm guage particularly language different property contrast unfortunate current tendency nlp algorithm develop test english bender algorithm develop english tend develop ofﬁcial language large industrialized nation nese spanish japanese german etc want limit tool language furthermore language multiple variety ken different region different social group example processing text use feature african american english aae african aae american vernacular english variation english million people african american community king use nlp tool function feature variety twitter post use ture speaker african american english construction like mainstream american english mae talmbout corresponding mae mae talk example inﬂuence word segmentation blodgett jones common speaker writer use multiple language gle utterance phenomenon call code switching code switching enormously code switching common world example show spanish transliterate hindi code switching english solorio jurgens por primera vez veo actually hateful beautiful ﬁrst time actually hateful beautiful dost tha hega wory dherya rakhe remain friend worry faith dimension variation genre text algorithm process come newswire ﬁction book scientiﬁc article wikipedia religious text come speak genre like telephone conversation business meeting police body wear camera medical interview transcript television show movie come work situation like doctor note legal text parliamentary congressional proceeding text reﬂect demographic characteristic writer speaker age gender race socioeconomic class inﬂuence linguistic property text process ﬁnally time matter language change time guage good corpora text different historical period language situated develop computational model guage processing corpus important consider produce guage context purpose user dataset know detail good way corpus creator build datasheet gebru datasheet datum statement bender corpus datasheet speciﬁes property dataset like motivation corpus collect fund situation situation text write speak example task language originally speak conversation edit text social medium communication monologue egular expression language variety language include dialect region corpus speaker demographic age gender text author collection process big datum subsample sample datum collect consent datum pre processed metadata available annotation process annotation demographic annotator train datum annotate distribution copyright intellectual property restriction regular expression useful tool text processing computer science regular expression orregex language specify text string regexe inregular expression computer language text processing tool like unix grep editor like vim emac play important role pre tokenization step tokenization algorithm like bpe formally regular expression algebraic notation characterize set string practically use regex search string text specify change string key tokenization use regular expression search pattern astring string single line long text example python function string scan string return ﬁrst match inside pattern follow example generally highlight exact string match regular expression ﬁrst match use python syntax express regex raw string delimit double quote raw string treat backslashe literal character important regex pattern introduce use backslashe regular expression come different variant online regex tester help sure regex think character disjunction square bracket simple kind regular expression sequence simple character match substre buttercup string like string call little buttercup need use special character example want match character ple regular expression generally case sensitive match low case upper case match sandswe use character tion operator square brace string character inside bracescharacter disjunction speciﬁes disjunction character match example fig show match pattern contain morm pattern match string mary mary mary ann stop mona uomini solda digit plenty figure use bracket specify disjunction chapter ords token regular expression speciﬁes single digit awkward imagine type mean uppercase letter bracket dash specify character range pattern speciﬁes character range pattern speciﬁes character org example show fig regex match example pattern match upper case letter renche blossom low case letter bean impatient hoe single digit chapter rabbit hole figure use bracket plus dash specify range square brace specify single character use caret caret ﬁrst symbol open square brace result pattern negate example pattern match single character include special character true caret ﬁrst symbol open square brace occur usually stand caret fig show example regex match single character example pattern match upper case letter oyfn pripetchik ihave exquisite reason period resident djinn look pattern look figure caret negation mean backslash escape period counting optionality wildcard talk optional element like optional sif want match koala andkoala use square bracket allow allow use question markr mean precede character match color andcolour matcheskoala orkoala way talk element occur consider language certain sheep consist string look like follow baa baaa baaaa sheep language consist string follow arbitrarily follow exclamation point represent language use useful operator represent asterisk call kleene generally pronounce cleany star kleene star mean zero kleene currence immediately previous character regular expression mean string zero represent sheep language correctly match baor baaaaaa problem match noa orbawith egular expression kleene star mean zero occurrence instead sheep language want mean bfollowe aafollowe zero additional complex pattern repeat mean zero zero right square brace match string like aaaa orababab orbbbb string fye integer string digit use slightly short way specify character kleene mean occurrence immediately precede kleene character regular expression normal way specify sequence digit specify sheep language kleene kleene use explicit number ter enclose curly bracket operator mean exactly occurrence previous character expression match follow exactly follow important special character period awildcard expression period match single character newline wildcard kleene star mean string character example suppose want ﬁnd line lar word example rise appear twice specify regular expression mean rose sequence zero character kind fig summarize regex match zero occurrence previous char expression occurrence previous char expression zero occurrence previous char expression exactly noccurrence previous char expression single char string zero char figure counting wildcard anchor boundary anchor special character anchor regular expression particular place anchor string common anchor caret dollar sign start line pattern match word theonly start line caret use match start line indicate negation inside square bracket mean caret contexts allow system know function give caret suppose dollar sign match end line pattern useful pattern match space end line match line contain phrase dog ﬁnal period note use backslash prior example want mean period wildcard contrast regular expression match dog alsothe dog andthe dogo discuss special character deﬁne far need backslashe mean use literally anchor word boundary non word boundary match word thebut word chapter ords token regex match start line end line word boundary non word boundary figure anchor regular expression word purpose regex deﬁne base word programming language sequence digit underscore letter match string bottle beer wall follow space bottle beer wall follow number match dollar sign digit underscore letter note anchor boundary operator technically match string mean eat character string carat match start actually advance ﬁrst character pattern matchesthe aware fact space boundary match string right space space space character available match disjunction grouping precedence suppose need search text pet particularly interested cat dog case want search string cator string dog use square bracket search cat dog right thing need new operator thedisjunction operator call pipe pattern disjunction match string cator string dog need use disjunction operator midst large quence example suppose want search mention pet ﬁsh specify guppy andguppie simply match string guppy andie sequence like guppy precedence disjunction operator disjunction precedence operator apply speciﬁc pattern need use parenthesis operator enclose pattern parenthesis make act like single character purpose neighboring operator like pipe kleene pattern specify mean disjunction apply sufﬁxesyandie parenthesis operator useful counter like kleene unlike kleene operator apply default single character sequence suppose want match repeat instance string line column label form column column column expression match number column instead match single column follow number space star apply space precede sequence parenthesis write expression match word column follow number optional space pattern repeat zero time idea operator precedence require use parenthesis specify mean formalize operator precedence hierarchy regular expression follow table give orderoperator egular expression operator precedence high precedence low precedence parenthesis counter sequence anchor disjunction counter high precedence sequence match theeeee thethe sequence high dence disjunction match theoranybut thany ortheny pattern ambiguous way consider expression match text time match zero letter expression match ﬁrst letter onc oronce case regular expression match large string pattern greedy expand cover string greedy way enforce non greedy matching non greedy ing qualiﬁer operator kleene star match little text possible operator kleene plus match little text possible simple example suppose want write regex ﬁnd case english article simple incorrect pattern problem pattern miss word begin sentence capitalize lead follow pattern overgeneralize incorrectly return text theembedded word orthere need specify want instance word boundary side simple process go base ﬁxe kind error false positive string incorrectly match like orthere false false positive negative string incorrectly miss like address kind false negative error come language processing reduce overall error rate application involve antagonistic effort increase precision minimize false positive increase recall minimize false negative come precision recall precise deﬁnition chapter operator figure show useful alias common range finally certain special character refer special notation base backslash fig common newline character newline chapter ords token regex expansion match match digit party non digit blue moon alphanumeric underscore daiyu non alphanumeric whitespace space tab inconcord non whitespace concord figure alias common set character refer character special like mean literally special usage try match period star bracket paren literal meaning special character need precede backslash regex match pattern match asterisk period livingston presume question mark come lend hand newline tab figure character need escape backslash substitution capture group important use regular expression substitution want substitution place string regular expression help specify string replace replacement python use function similar function exist language environment pattern repl string take argument pattern search replacement replace string search replace example change instance cherry toapricot instre string convert upper case instance particular string substitution depend complex way string match pattern example suppose document date format yyyy want change format region yyyy match date specify replacement want swap date month value tool regular expression capture group capture group capture group use parenthesis capture store value match search reuse replacement set parenthesis thepattern want capture store numbered group group number left right repl refer group number command consider follow egular expression string parenthesis month digit day digit year digit store ﬁrst digit group second digit group ﬁnal digit group replstring use number operator refer ﬁrst second register result string like date convert date capture group useful substitution example use ﬁnd repetition need text processing example ﬁnd repeat word string use pattern search word capture group refer whitespace parenthesis double function regular expression group term specify order operator apply capture match occasionally need parenthesis grouping want capture result pattern case use non capturing group non capturing group speciﬁe put special command open parenthesis form pattern non capture group usually try capture long complex pattern match sequence date separate space want extract need use parenthesis order use counting operator ﬁrst want store useless information follow pattern store date group substitution capture group useful implement historically important chatbot like eliza weizenbaum recall eliza simulate rogerian psychologist carry conversation like follow user bug eliza think specific example user boyfriend come eliza boyfriend come user say depressed time eliza sorry hear depressed eliza work have series cascade regex substitution match change input line input uppercased substitution change instance mytoyour toyou way eliza repeat user utterance refer correctly user set substitution match replace pattern input turn input complete response example sorry hear think think specific chapter ords token lookahead assertion finally time need predict future look ahead text pattern match advance pointer text deal pattern occur check instead lookahead assertion use syntax see lookahead ous section non capture group operator pattern true ifpattern occur zero width match pointer advance see zero width anchor boundary marker like operator pattern turn true pattern match zero width advance pointer negative lookahead commonly parse plex pattern want rule special case example suppose want capture ﬁrst word line start letter use negative lookahead ﬁrst negative lookahead say line start tort match string move match pointer capture group capture ﬁrst word simple unix tool word tokenization english possible simple naive word tokenization frequency putation single unix command line church point useful need quick information text corpus use unix command systematically change particular character sort input line alphabetical order uniq collapse count adjacent identical line example let begin complete word shakespeare ﬁle use trto tokenize word change sequence alphabetic character newline mean alphabetic option complement non alphabet mean change non alphabetic character newline squeeze option replace result multiple consecutive change single output series non alphabetic character row squeeze single newline output command sonnet william shakespeare fair creature inimum editdistance word line sort line pass uniq collapse count sort uniq follow output aaron abbess aaron abate abate alternatively collapse upper case low case sort uniq output aaron abaissiez abandon abandon abase abash abate sort ﬁnd frequent word tosort mean sort numerically alphabetically mean sort reverse order high low sort uniq sort result frequent word shakespeare corpus short function word like article pronoun preposition unix tool sort handy build quick word count statistic corpus english complex generally turn sophisticated tokenization algorithm discuss minimum edit distance need way compare similar word string later chapter come commonly task like automatic chapter ords token recognition machine translation want know similar sequence word reference sequence word edit distance give way quantify intuition string similarity formally minimum edit distance string deﬁne theminimum edit distance minimum number editing operation operation like insertion deletion tion need transform string section introduce edit distance single word algorithm apply equally entire string gap intention andexecution example delete tuteeforn substitute xfort insertc substitute uforn easy look important visualization string distance alignment alignment string show fig give sequence alignment correspondence substring sequence ialign string nwithe beneath aligned string representation series symbol express operation list convert string string dfor deletion sfor substitution ifor insertion jjjjjjjjjj execution figure represent minimum edit distance string alignment ﬁnal row give operation list convert string string deletion substitution insertion assign particular cost weight operation levenshtein distance sequence simple weighting factor operation cost levenshtein assume substitution letter example tfort zero cost enshtein distance intention andexecution levenshtein propose alternative version metric insertion deletion cost substitution allow equivalent allow substitution give substitution cost substitution represent insertion deletion version levenshtein distance intention andexecution minimum edit distance algorithm ﬁnd minimum edit distance think search task search short path sequence edit string ndelinssubsti figure find edit distance view search problem space possible edit enormous search naively lot distinct edit path end state string inimum editdistance put path remember short path state time see dynamic programming dynamic programmingdynamic programming class algorithm ﬁrst introduce bellman apply table drive method solve problem combine solution subproblem commonly algorithm natural language processing use dynamic programming viterbi algorithm chapter cky algorithm parse chapter intuition dynamic programming problem large problem solve properly combine solution subproblem consider short path transform word represent minimum edit distance string intention andexecution show fig ndelete substitute substitute insert substitute figure path intention toexecution imagine string exention optimal path intuition dynamic programming exention optimal operation list optimal sequence include optimal path intention toexention short path intention toexention use instead result short overall path optimal sequence optimal lead contradiction minimum edit distance algorithm name wagner fischerminimum edit distance independently discover people historical note tion chapter let ﬁrst deﬁne minimum edit distance string give string source string xof length target string yof length deﬁne edit distance ﬁrst icharacter ﬁrst jcharacter edit distance xandyis use dynamic programming compute combine lution subproblem base case source substring length ibut target string go icharacter require idelete target substring length jbut source go character jcharacter require jinsert having compute small compute large previously compute small value value put take minimum possible path matrix arrive cost source cost target cost source mention version levenshtein distance tion cost substitution cost equivalent insertion plus deletion let use second version levenshtein distance chapter ords token insertion deletion cost in cost del substitution cost substitution identical letter zero cost version levenshtein computation source target source target algorithm summarize fig fig show result apply algorithm distance intention andexecution version levenshtein function min edit distance source target return min distance length source length target create distance matrix initialization zeroth row column distance string foreach row ifrom del cost source foreach column jfrom ins cost target recurrence relation foreach row ifrom foreach column jfrom del cost source sub cost source in cost target termination return figure minimum edit distance algorithm example class dynamic programming algorithm cost ﬁxe cost speciﬁc letter model fact letter likely serte assume cost substitute letter sub cost alignment know minimum edit distance useful algorithm like ing potential spelling error correction edit distance algorithm important way small change provide minimum cost ment string align string useful speech language processing speech recognition minimum edit distance alignment compute word error rate chapter alignment play role chine translation sentence parallel corpus corpus text language need match extend edit distance algorithm produce alignment start visualize alignment path edit distance matrix figure show path boldface cell boldface cell represent alignment pair letter string boldface cell occur inimum editdistance srcntar figure computation minimum edit distance intention andexecution algorithm fig levenshtein distance cost insertion tion substitution insertion go source target boldface cell column indicate deletion figure show intuition compute alignment path computation proceed step ﬁrst step augment minimum edit distance algorithm store backpointer cell backpointer cell point previous cell cell come enter current cell show schematic backpointer fig cell tiple backpointer minimum extension come multiple previous cell second step perform backtrace backtrace start backtrace cell ﬁnal row column follow pointer dynamic programming matrix complete path ﬁnal cell initial cell minimum distance alignment exercise ask modify minimum edit distance algorithm store pointer compute backtrace output alignment figure enter value cell mark neighbor cell come arrow table compute alignment minimum edit path backtrace start low right corner follow arrow sequence bold cell represent possible minimum cost alignment string levenshtein distance cost insertion deletion substitution diagram design gusﬁeld work example simple levenshtein distance algorithm fig allow arbitrary weight operation spell correction example substitution likely happen letter chapter ords token keyboard viterbi algorithm probabilistic extension minimum edit distance instead compute minimum edit distance string viterbi compute maximum probability alignment string discuss chapter summary chapter introduce fundamental concept token tokenization guage processing discuss linguistic level word morpheme character introduce unicode code point encoding introduce thebpe algorithm tokenization introduce regular expression minimum edit distance algorithm compare string summary main point cover idea word morpheme useful unit representation difﬁcult deﬁne formally system represent character script write language world character represent internally unique call code point encode ﬁle encode method like variable length encoding pair encoding orbpe standard way induce token drive way ﬁrst step large language model token roughly word morpheme sized small single character regular expression language powerful tool pattern matching basic operation regular expression include disjunction symbol anchor capture group substitution minimum edit distance string minimum number operation take edit minimum edit distance compute dynamic programming result alignment string historical note herdan law heap law herdan heap egghe baayen unicode draw ascii iso character encoding standard early draft work discussion engineer xerox apple early draft standard publish formal release unicode stanford begin iso draft extension self synchronize aspect famously outline placemat new jersey dinner ken thompson word tokenization text normalization algorithm apply beginning ﬁeld include stem like widely mer lovin application digital humanity like packard build afﬁx strip morphological parser ancient bpe originally text compression method propose gage apply subword tokenization context early neural machine translation nrich take openai radford default tokenization method include open source piece library kudo richardson nice public tation minbpe andrej karpathy popular lecture introduce bpe zdusfxrajke kleene ﬁrst deﬁne regular expression ﬁnite automaton base mcculloch pitts neuron ken thompson ﬁrst build regular expression compiler editor text search thompson editor edinclude command regular expression global regular sion print later unix grep utility nltk essential tool offer useful python library textbook description bird algorithm include text normalization corpus interface edit distance gusﬁeld example measure edit distance intention execution adapt kruskal publicly available package compute edit distance include unix diff nist sclite program nist autobiography bellman explain originally come term dynamic programming good year mathematical research secretary defense pathological fear hatred word research decide use word programming want idea dynamic stage think let word absolutely precise meaning dynamic impossible use word dynamic pejorative sense try think combination sibly pejorative meaning impossible think dynamic programming good congressman object exercise write regular expression following language set alphabetic string set low case alphabetic string end set string alphabet ais ately precede immediately follow write regular expression following language word mean alphabetic string separate word whitespace relevant punctuation line break forth set string consecutive repeat word bert humbert bug big bug string start beginning line integer end end line chapter ords token string word grotto word raven word like grottos merely contain word grotto write pattern place ﬁrst word english sentence register deal punctuation implement eliza like program substitution describe page want choose different domain rogerian chologist mind need domain program legitimately engage lot simple repetition compute edit distance insertion cost deletion cost substitution cost leda deal work edit distance grid figure drive close brief diver edit tance use version distance like implement minimum edit distance algorithm use hand compute result check code augment minimum edit distance algorithm output alignment need store pointer add stage compute gram language model uniformly charming cry smile associate bow perceive chaise wish random sentence generate jane austen trigram model predicting difﬁcult especially future old quip go predict easy like word go word example likely follow water walden pond beautifully conclude likely word blue orgreen orclear probably notrefrigerator northis chapter formalize intuition ducing gram language model orlm language model machine learn language model model predict upcoming word formally language model assign probability possible word equivalently give probability tion possible word language model assign probability entire sentence tell follow sequence high probability appear text sudden notice guy stand sidewalk set word different order guy notice sidewalk sudden stand want predict upcoming word main reason large language model build train predict word chapter large language model learn enormous language solely train predict upcoming word neighboring word probabilistic knowledge practical consider correct mar spell error like midterm mistype astheir oreverything improve improve improve phrase probable andhas improve thanhas improve language model help user select grammatical variant speech system recognize say soonish noti bassoon dish help know soonish probable sequence language model help augmentative tive communication trnka kane people use aac aac system physically unable speak sign instead use eye gaze movement select word menu word prediction suggest likely word menu chapter introduce simple kind language model gram chapter gram language model language model gram sequence nword gram bigram word sequence word like water orwater gram trigram word sequence word like water orwater walden bit terminological ambiguity use word gram mean probabilistic model estimate probability word give previous word assign probability entire sequence later chapter introduce powerful neural large guage model base transformer architecture chapter gram remarkably simple clear formalization use duce major concept large language modeling include training test set perplexity sampling interpolation gram let begin task compute probability word wgiven history suppose history water walden pond beautifully want know probability word blue water walden pond beautifully way estimate probability directly relative frequency count large corpus count number time water walden pond beautifully count number time follow blue answer question time see history time follow word follow water walden pond beautifully water walden pond beautifully blue water walden pond beautifully large corpus compute count estimate probability entire web big good estimate count entire sentence language creative new sentence invent time expect accurate count large object entire sentence reason need clever way estimate probability word wgiven history probability entire word sequence let start notation chapter continue refer word practice usually compute language model ken like bpe token page represent probability particular random variable xitake value use simpliﬁcation represent sequence nword expression string equivalent notation read element include joint probability word quence have particular value compute probability entire sequence like thing decompose probability chain rule ram bility apply chain rule word chain rule show link compute joint probability sequence compute conditional probability word give previous word tion suggest estimate joint probability entire sequence word multiply number conditional probability chain rule help know way compute exact probability word give long sequence precede word say estimate count number time word occur follow long string corpus language creative particular context occur markov assumption intuition gram model instead compute probability word give entire history approximate history word thebigram model example approximate probability word give bigram previous word conditional probability give precede word word instead compute ity water walden pond beautifully approximate probability use bigram model predict conditional probability word make follow approximation assumption probability word depend previous word call markov assumption markov model class probabilistic model markov assume predict probability future unit look far past generalize bigram look word past trigram look word past gram gram look word past let general equation gram approximation conditional probability word sequence use nhere mean chapter gram language model size mean bigrams mean trigram approximate probability word give entire context follow give bigram assumption probability individual word pute probability complete word sequence substitute estimate probability estimate bigram gram probability intuitive way estimate probability call maximum likelihood estimation ormle getmaximum likelihood estimationthe mle estimate parameter gram model get count corpus normalize count lie normalize bilistic model normalizing mean divide total count result probability fall sum example compute particular bigram probability word wngiven previous word compute count bigram ize sum bigram share ﬁrst word simplify equation sum bigram count start give word equal unigram count word reader moment convince let work example mini corpus sentence ﬁrst need augment sentence special symbol beginning sentence bigram context ﬁrst word need special end symbol sam sam like green egg ham calculation bigram probability corpus general case mle gram parameter estimation need end symbol bigram grammar true probability distribution symbol instead sentence probability sentence sum sentence probability sentence give length sum model deﬁne inﬁnite set probability distribution distribution sentence length exercise ram equation like estimate gram probability divide observe frequency particular sequence observed frequency preﬁx ratio call relative frequency say use relativerelative frequency frequency way estimate probability example maximum likelihood estimation mle mle result parameter set maximize likelihood training set tgiven model example suppose word chinese occur time corpus million word probability random word select text million word word chinese mle probability well possible estimate probability chinese occur situation turn corpus context chinese unlikely word probability make likely chinese occur time million word corpus present way modify mle estimate slightly well probability estimate section let example real tiny corpus draw defunct berkeley restaurant project dialogue system century answer question database restaurant berkeley california rafsky sample user query text normalize low casing punctuation stripe sample sentence website tell good cantonese restaurant close tell chez panisse look good place eat breakfast caffe venezia open day figure show bigram count bigram grammar normalize berkeley restaurant project sentence note majority value zero fact choose sample word cohere matrix select random set word sparse want eat chinese food lunch spend want eat chinese food lunch spend figure bigram count word berkeley rant project corpus sentence zero count gray cell show count column label word follow row label word cell row iand column want mean want follow time corpus figure show bigram probability normalization divide cell fig appropriate unigram row take following set unigram count want eat chinese food lunch spend chapter gram language model want eat chinese food lunch spend want eat chinese food lunch spend figure bigram probability word berkeley restaurant project corpus sentence zero probability gray useful probability compute probability sentence like want english food want chinese food simply multiply appropriate bigram probability gether follow want english food leave exercise compute probability want chinese food kind linguistic phenomenon capture bigram statistic bigram probability encode fact think strictly syntactic nature like fact come eatis usually noun adjective come tois usually verb fact personal assistant task like high probability sentence begin word cultural linguistic like high probability people look chinese versus english food deal scale large gram model practice language model large lead practical issue log probability language model probability store compute log space log probability probability deﬁnition lesslog probability equal probability multiply small product multiply gram result numerical underﬂow add log space equivalent multiply linear space combine log probability add add log probability instead multiplying probability result small computation storage log space convert probability need report probability end take exp logprob practice book use log mean natural log base valuate language model training testset long context pedagogical purpose describe gram model sufﬁcient training datum use trigram model trigram condition previous word gram gram model large gram gram gram need assume extra contexts left right sentence end example compute trigram probability beginning sentence use pseudo word ﬁrst trigram large gram dataset create like million frequent gram draw corpus contemporary american english coca curate billion word corpus american english davy google web gram corpus trillion word english web text franz brent google books ngrams corpora billion token chinese english french german hebrew italian russian spanish lin possible use extremely long range gram context inﬁni gram project liu allow gram length idea avoid expensive space time pre computation huge gram count ble instead gram probability arbitrary compute quickly inference time efﬁcient representation call sufﬁx array allow computing gram length enormous corpora trillion token efﬁciency consideration important build large gram language model standard quantize probability bit instead byte ﬂoat store word string disk represent memory bit hash represent gram special data structure like reverse try common prune gram language model example keep gram count great threshold entropy prune important gram stolcke efﬁcient language model toolkit like kenlm heaﬁeld heaﬁeld use sorted array use merge sort ciently build probability table minimal number pass large corpus evaluate language model training test set good way evaluate performance language model embed application measure application improve end end evaluation call extrinsic evaluation extrinsic evaluation way toextrinsic evaluation know particular improvement language model component go help task hand evaluate gram language model component task like speech recognition machine translation compare performance candidate language model run speech recognizer machine translator twice language model see give accurate transcription unfortunately run big nlp system end end expensive stead helpful metric quickly evaluate potential improvement language model intrinsic evaluation metric mea intrinsic evaluation sure quality model independent application section introduce perplexity standard intrinsic metric measure language model performance simple gram language model phisticate neural large language model chapter order evaluate machine learning model need distinct data set training set development set test set training set development set test chapter gram language model training set datum use learn parameter model simple gram language model corpus count normalize probability gram language model thet set different hold set datum overlap training set use evaluate model need separate test set unbiased estimate model train generalize apply new unknown dataset machine learning model perfectly capture training datum perform terribly datum use come time apply new datum problem measure quality gram model performance unseen test set test corpus choose training test set test set reﬂect language want use model go use language model speech recognition chemistry lecture test set text chemistry lecture go use system translate hotel book quest chinese english test set text hotel booking request want language model general purpose test set draw wide variety text case collect lot text different source divide training set test set important dividing carefully build general purpose model want test set consist text document author good measure general performance give corpus text want compare performance different gram model divide datum training test set train parameter model training set compare train model test set mean test set standard answer simple whichever language model assign high probability test set mean accurately predict test set well model give bilistic model well model well predict detail test datum assign high probability test datum evaluation metric base test set probability important let test sentence training set suppose try compute probability particular test sentence test sentence training corpus mistakenly assign artiﬁcially high probability occur test set situation training test set data contamination training test set introduce bias make probabilitiesdata contamination look high cause huge inaccuracy perplexity probability base metric introduce train test set test language model test set time make different change implicitly tune characteristic notice change model well reason want run model test set number time sure model ready reason normally instead dataset call developmentdevelopment test test set devset testing dataset end test test set good model divide datum training development test set want test set large possible small test set accidentally representative want training datum possible minimum want pick small test set give statistical valuate language model perplexity measure statistically signiﬁcant difference potential model important devset draw kind text test set goal measure test set evaluate language model perplexity say evaluate language model base assign high probability test set well model well predict upcoming word surprised assign high probability word occur test set perfect language model correctly guess word corpus assign probability word probability zero give test corpus well language model assign high probability bad language model fact use raw probability metric evaluate language model reason probability test set sequence depend number word token probability test set get small long text useful metric word normalize length compare text different length metric function probability call perplexity evaluate large language model gram model theperplexity abbreviate ppl language model perplexity test set inverse probability test set probability test set normalize number word token reason call word token perplexity normalize number word nby take nth root test set perplexity use chain rule expand probability perplexity nvuutny note inverse high probability word sequence low perplexity low perplexity model datum well model minimize perplexity equivalent maximize test set probability accord language model perplexity use inverse probability turn inverse arise original deﬁnition perplexity cross entropy rate information theory interest explanation advanced section remember perplexity inverse relationship probability detail compute perplexity test set wdepend guage model use perplexity wwith unigram language model geometric mean inverse unigram probability perplexity nvuutny chapter gram language model perplexity wcompute bigram language model geometric mean inverse bigram probability perplexity nvuutny generally use word sequence entire sequence word test set sequence cross sentence boundary vocabulary include sentence token eos separate end sentence marker include probability computation include token sentence total count word tokens mention perplexity function text guage model give text different language model different itie perplexity compare different language model example train unigram bigram trigram model million word wall street journal newspaper compute perplexity model wsj test set unigrams bigram correspond equation trigram table show perplexity million word test set accord language model unigram bigram trigram perplexity information gram give word sequence high probability gram assign string trigram model surprised unigram model well idea word come assign high probability high probability low perplexity show perplexity relate inversely probability test sequence accord model low perplexity tell language model well predictor test set note compute perplexity language model construct knowledge test set perplexity artiﬁcially low perplexity language model comparable use identical vocabulary intrinsic improvement perplexity guarantee extrinsic provement performance language processing task like speech recognition machine translation nonetheless perplexity usually correlate task improvement commonly convenient evaluation metric possible model improvement perplexity conﬁrme end end evaluation real task perplexity weight average branching factor turn perplexity think weighted average ing factor language branch factor language number possible word follow word example consider mini artiﬁcial example use begin end token include end sentence marker beginning sentence marker count end sentence token follow directly begin sentence token probability want probability fake transition inﬂuence ample sentence language model language deterministic probability word follow word vocabulary consist color branch factor language let probabilistic version let word follow equal train training set equal count color test set red red red red blue let ﬁrst convince compute perplexity artiﬁcial color language test set test set perplexity aontis red red red blue suppose redwas likely training set different bha follow probability expect perplexity test set red red red red blue language model bto low time color red predictable high probability probability test set high perplexity inversely relate probability perplexity low branching factor perplexity weight branching factor small red red red blue sample sentence language model important way visualize kind knowledge language model embodie sample sample distribution mean choose random point sample accord likelihood sample language model resent distribution sentence mean generate sentence choose sentence accord likelihood deﬁne model likely generate sentence model think high probability likely generate sentence model think low probability technique visualize language model sampling ﬁrst suggest early shannon miller selfridge simple visualize work unigram case imagine word english language cover number line word cover chapter gram language model figure visualization sample distribution sample sentence edly sample unigram blue bar represent relative frequency word order frequent frequent choice order arbitrary number line show cumulative probability choose random number fall interval correspond word expectation random number fall large interval frequent word high small interval rare word polyphonic proportional frequency fig show visualization unigram compute text book choose random value ﬁnd point probability line print word interval include choose value continue choose random number generate word randomly generate token use technique generate bigram ﬁrst generate dom bigram start bigram probability let second word bigram choose random bigram start draw accord bigram probability generalizing overﬁtte training set gram model like statistical model dependent training corpus implication probability encode speciﬁc fact give training corpus implication gram well well job model training corpus increase value use sampling method prior section visualize fact intuition increase power high order gram fig show random sentence generate unigram bigram trigram gram model train shakespeare work long context coherent sentence unigram tence coherent relation word tion bigram sentence local word word coherence especially consider punctuation word trigram sentence begin look lot like shakespeare gram sentence look little like speare word directly king john knock shakespeare oeuvre large corpora gram probability matrix ridiculously sparse possible bigram number possible gram generator choose ﬁrst gram seven possible word element period idea dependence training set let look lm train completely different corpus wall street journal wsj newspaper eneralize training set swallow confess hear save trail device rote life gram hill late speak leg ﬁrst enter dost stand forth thy canopy forsooth palpable hit king henry live king follow gram mean sir confess sort trim captain fly rid news price sadness parting tis gram shall forbid brand renown king henry seek traitor gloucester exeunt watch great banquet gram figure sentence randomly generate gram model compute shakespeare work character map low case punctuation mark treat word output hand correct capitalization improve readability wsj english expect overlap gram genre fig show sentence generate unigram bigram trigram model train million word wsj issue year foreign new exchange september recession exchange new endorse acquire executivesgram december way preserve hudson corporation taylor complete major central planner gram point ﬁve percent old corporation live information frequently ﬁshe point ninety point billion dollar percent rate interest store mexico gram brazil market condition figure sentence randomly generate gram model compute million word wall street journal lower case character treat tion word output hand correct capitalization improve readability compare example pseudo shakespeare fig model english like sentence overlap generate sentence little overlap small phrase statistical model pretty useless predictor training set test set different shakespeare wsj deal problem build gram model step sure use training corpus similar genre task try accomplish build language model translate legal document need training corpus legal document build language model question answer system need training corpus question equally important training datum appropriate dialect orvariety especially process social medium post speak transcript ple tweet use feature african american english aae variation language african american community king feature include word like ﬁnna auxiliary verb mark immediate future tense occur variety spelling like denfor tweet like blodgett chapter gram language model bore den phone ﬁnna die tweet english base language like nigerian pidgin markedly ferent vocabulary gram pattern american english jurgens wizard wat gan sef mornin tweet afternoon tweet nyt gin dey tweet beta placement wiv twitter possible testset nonetheless word see fore happen word jurafsky occur training set pop test set answer word unseen normally run nlp algorithm word subword tokens subword tion like bpe algorithm chapter word model sequence know small subword necessary sequence token correspond individual letter convenience refer word chapter language model vocabulary normally set token word way test set contain unseen token smoothing interpolation backoff problem maximum likelihood estimate probability ﬁnite training corpus miss perfectly acceptable english word quence case particular gram occur training datum appear test set training corpus word ruby slipper happen phrase ruby slipper unseen sequence zero sequence occur training set zero occur test set problem reason presence mean underestimate probability word sequence occur hurt performance application want run datum second probability word test set probability test set perplexity deﬁne base inverse probability test set word context zero probability compute perplexity divide zero standard way deal putative zero probability gram ally non zero probability call smooth ordiscounte smooth smooth discount algorithm shave bit probability mass frequent event unseen event introduce simple smoothing algorithm laplace add smoothing stupid backoff gram interpolation laplace smooth simple way smoothing add gram count normalize probability count zero count count algorithm call laplace smoothing laplace smoothing perform usedlaplace smooth modern gram model usefully introduce concept smoothing algorithm give useful baseline practical smooth algorithm task like text classiﬁcation appendix let start application laplace smooth unigram probability recall unsmoothed maximum likelihood estimate unigram moothing interpolation backoff word wiis count cinormalize total number word tokens laplace smooth merely add count alternate onesmoothing vword vocabulary add cremente need adjust denominator account extra observation happen pvalue increase denominator plaplace intuition unigram case let smooth berkeley restaurant project bigram figure show add smoothed count bigram fig want eat chinese food lunch spend want eat chinese food lunch spend figure add smoothed bigram count word berkeley restaurant project corpus sentence previously zero count gray figure show add smoothed probability bigrams fig compute recall normal bigram probability compute normalize row count unigram count add smoothed bigram count need augment unigram count denominator number total word type vocabulary follow equation make explicit unigram count denominator sum bigram start add vof add total vto denominator plaplace unigram count give page need augment result smoothed bigram probability fig useful visualization technique reconstruct adjusted count matrix smoothing algorithm change original count adjusted count count divide result smoothed probability adjusted count easy compare directly mle count laplace probability equally express adjust count divide non smoothed denominator plaplace chapter gram language model want eat chinese food lunch spend want eat chinese food lunch spend figure add smoothed bigram probability word berp corpus sentence compute previously zero probability gray rearrange term solve figure show reconstructed count compute want eat chinese food lunch spend want eat chinese food lunch spend figure add reconstitute count word berp corpus sentence compute previously zero count gray note add smoothing big change count pare fig original count fig change probability space unsmoothed case smoothed case look count deﬁne ratio new old count show strikingly count preﬁx word reduce discount bigram want tois discount chinese food factor sharp change occur probability mass move zero add smoothing alternative add smoothing bit probability mass see unseen event instead add count add fractional count algorithm call add smoothing add add add smoothing require method choose example optimize devset add useful task include text classiﬁcation turn work moothing interpolation backoff language modeling generate count poor variance inappropriate discount gale church language model interpolation alternative source knowledge draw solve problem zero frequency gram try compute example particular trigram instead estimate probability bigram probability similarly count compute look unigram word context help generalize context model learn common way use gram hierarchy call interpolation interpolation compute new probability interpolate weight combine gram bigram unigram probability simple linear interpolation bine different order gram linearly interpolate estimate trigram probability mix unigram bigram trigram probability weight thel sum make equivalent weighted average slightly sophisticated version linear interpolation lweight put conditioning context way particularly accurate count particular bigram assume count trigram base bigram trustworthy trigram higher trigram weight interpolation equation show equation interpolation context condition weight lambda take argument prior word context lvalue set simple interpolation conditional lation learn hold corpus hold corpus additional hold training corpus call hold training datum use set choose lvalue maximize hood hold corpus gram probability search lvalue plug high probability hold set way ﬁnd optimal set way use emalgorithm iterative learning algorithm converge locally optimal jelinek mercer stupid backoff alternative interpolation backoff backoff model gram need backoff corpora generally set hyperparameter special parameter unlike regular count learn training datum discuss hyperparameter chapter chapter gram language model zero count approximate back continue back reach history count backoff model correct probability distribution discount high order gram discount save probability mass low order gram practice instead discount common use simple non discounted backoff algorithm call stupid backoff brent stupid backoff stupid backoff give idea try language model true probability distribution discounting high order probability high order gram zero count simply backoff low order gram weigh ﬁxed context independent weight algorithm produce probability distribution follow brent refer count count count backoff terminate unigram score count brent ﬁnd value work advance perplexity relation entropy introduce perplexity section way evaluate gram model test set well gram model assign high probability test datum perplexity normalize version probability test set perplexity measure actually arise information theoretic concept cross entropy explain mysterious property perplexity inverse probability example relationship entropy entropy entropy measure information give random variable xrange predict word letter part speech set particular probability function entropy random variable xis log principle compute base use log base result value entropy measure bit intuitive way think entropy lower bind number bit encode certain decision piece information optimal code scheme consider example standard information theory textbook cover thomas imagine want place bet horse race far way yonker racetrack like send short message bookie tell horse bet way encode message use binary representation horse number code horse horse horse horse code spend day bet horse code bit average send bit race well suppose spread actual distribution bet place represent prior probability horse dvanced perplexity srelation entropy horse horse horse horse entropy random variable xthat range horse give lower bind number bit bit code average bit race build short encoding probable horse long encoding probable horse example encode likely horse code remain horse horse equally likely see length binary code horse number horse take bit code average entropy case horse probability entropy choice horse bit compute entropy single variable use entropy involve sequence grammar example compute entropy sequence word wng way variable range sequence word example compute entropy random variable range quence word length nin language las follow deﬁne entropy rate think word entropy rate entropy entropy sequence divide number word measure true entropy language need consider sequence inﬁnite length think language stochastic process lthat produce sequence word allow wto represent sequence word entropy rate deﬁne lim chapter gram language model shannon mcmillan breiman theorem algoet cover cover thomas state language regular certain way exact stationary ergodic lim single sequence long instead sum possible sequence intuition shannon mcmillan breiman theorem long sequence word contain short quence short sequence reoccur long sequence accord probability stochastic process say stationary probability assign stationary sequence invariant respect shift time index word probability distribution word time tis probability distribution time markov model gram stationary example bigram piis dependent shift time index dependent natural language stationary appendix probability upcoming word dependent event arbitrarily distant time dependent statistical model approximation correct distribution entropy natural language summarize make incorrect convenient simplifying tion compute entropy stochastic process take long sample output compute average log probability ready introduce cross entropy cross entropy useful cross entropy know actual probability distribution pthat generate datum allow use model approximation cross entropy monpis deﬁne lim draw sequence accord probability distribution sum log probability accord follow shannon mcmillan breiman theorem stationary godic process lim mean entropy estimate cross entropy model distribution pby take single sequence long instead sum possible sequence make cross entropy useful cross entropy bound entropy model mean use simpliﬁed model mto help estimate true tropy sequence symbol draw accord probability accurate mis close cross entropy true entropy difference measure accurate model model accurate model ummary low cross entropy cross entropy low true entropy model err underestimate true entropy ﬁnally ready relation perplexity cross entropy see cross entropy deﬁne limit length observe word sequence go inﬁnity approximate cross entropy rely sufﬁciently long sequence ﬁxed length approximation cross entropy model sequence word wis theperplexity model pon sequence word wis formally deﬁne perplexity raise power cross entropy perplexity summary chapter introduce language modeling gram model classic model allow introduce basic concept language modeling language model offer way assign probability sentence sequence word token predict word token precede word token gram simple kind language model markov model estimate word ﬁxed window previous word gram model train count training corpus normalize count maximum likelihood estimate gram language model evaluate test set perplexity perplexity test set accord language model function probability test set inverse test set probability accord model normalize length language model mean generate sentence e sentence accord likelihood deﬁne model algorithm provide way estimate probability event unseen training commonly smooth algorithm gram include smoothing rely low order gram count polation historical note underlying mathematic gram ﬁrst propose markov call markov chain bigram trigram predict upcoming letter pushkin eugene onegin vowel sonant markov classiﬁe letter compute bigram chapter gram language model trigram probability give letter vowel give previous letter shannon apply gram compute approximation english word sequence base shannon work markov model commonly engineering linguistic psychological work model word sequence series extremely inﬂuential paper start chomsky include chomsky miller chomsky noam chomsky argue ﬁnite state markov process possibly useful engineering heuristic incapable complete cognitive model human grammatical edge argument lead linguist computational linguist ignore work statistical modeling decade resurgence gram language model come fred jelinek league ibm thomas watson research center inﬂuence shannon james baker cmu inﬂuence prior classiﬁed work leonard baum colleague topic lab like institute defense analysis ida declassiﬁe independently lab successfully gram speech recognition system time baker jelinek baker bahl jelinek term language model perplexity ﬁrst technology ibm group jelinek colleague term language model pretty modern way mean entire set linguistic inﬂuence word sequence ability include grammar semantic discourse speaker characteristic particular gram model add smoothing derive laplace law succession ﬁrst apply engineering solution zero frequency problem jeffrey base early add suggestion johnson problem algorithm summarize gale church wide variety different language model smooth technique propose include good ture apply gram smoothing ibm katz adas church gale bell discount witten bell variety class base gram mod class base gram el information word class start late chen goodman perform number carefully control experiment compare ferent algorithm parameter chen goodman goodman inter alia show advantage modiﬁed interpolate kneser ney standard baseline gram language model turn century especially show caches class base model vide minor additional improvement srilm stolcke kenlm heaﬁeld heaﬁeld publicly available toolkit build gram language model large language model base neural network gram able solve major problem gram number eter increase exponentially gram order increase gram way generalize training example test set example use tical word neural language model instead project word continuous space word similar context similar representation introduce transformer base large language model chapter way introduce feedforward language model bengio schwenk chapter recurrent language model mikolov chapter exercise write equation trigram probability estimation modify write non zero trigram probability sam corpus page calculate probability sentence want chinese food probability fig useful probability page smoothed table fig assume additional smoothed probability probability compute previous exercise high unsmoothed smooth explain give follow corpus modiﬁed chapter sam sam sam like green egg sam bigram language model add smoothing include count like token suppose use end symbol train unsmoothed bigram grammar follow training corpus end symbol demonstrate bigram model assign single probability tribution sentence length show sum probability possible word sentence alphabet bgi sum probability possible word sentence alphabet suppose train trigram language model add smoothing give corpus corpus contain word type express formula mating word follow bigram term gram count use notation denote number time trigram occur corpus bigram unigrams give follow corpus modiﬁed chapter sam sam sam like green egg sam use linear interpolation smooth maximum likelihood gram model maximum likelihood unigram model include count like token write program compute unsmoothed unigrams chapter gram language model run gram program different small corpora choice use email text newsgroup compare statistic corpora difference common unigram interesting difference bigrams add option program generate random sentence add option program compute perplexity test set give training set number consist zero digit following test set unigram regression text classiﬁcation sus remota aginas est escrito que los animale dividen pertenecientes emperador incluidos esta clasiﬁcaci embalsamados que agitan como locos amaestrados innumerables lechones dibujados con pincel ısimo pelo camello sirenas etc etera fabulosos que acaban romper jarr perros sueltos que lejos parecen moscas borge classiﬁcation lie heart language processing intelligence nize letter word face sort mail assign grade homework example assign category input challenge classiﬁcation famously highlight fabulist jorge luis borge imagine ancient mythical encyclopedia classiﬁed animal belong emperor embalm one train suckle pig mermaid fabulous one stray dog include classiﬁcation tremble mad innumerable one draw ﬁne camel hair brush break ﬂow vase resemble ﬂie distance luckily class use language processing easy deﬁne borge chapter introduce logistic regression algorithm classiﬁcation apply text categorization task assign label ortext categorization category text document focus text categorization task ment analysis categorization sentiment positive negative orientationsentiment analysis writer express object review movie book product express author sentiment product editorial political text express sentiment action candidate extract sentiment relevant ﬁeld marketing politic binary task label text indicate positive negative stance word like awesome andlove orawful andridiculously informative sample extract movie restaurant review awesome caramel sauce sweet toasty almond love place awful pizza ridiculously overprice text classiﬁcation task spam detection assign email spam detection class spam ornot spam task language e language text write authorship attribution task ofauthorship attribution determine text author relevant humanistic forensic chapter ogistic regression make classiﬁcation important language modeling view classiﬁcation word think class predict word classify context far class word intuition underlie large language model algorithm classiﬁcation introduce chapter logistic regression equally important number way logistic regression close relationship neural network chapter neural network view series logistic regression classiﬁer stack second logistic regression introduce idea fundamental neural network language model like sigmoid andsoftmax function logit sigmoid softmax logitand key gradient descent algorithm learn finally logistic regression important analytic tool social natural science machine learning classiﬁcation goal classiﬁcation single input input tion extract useful feature property input classify observation observation set discrete class input output come ﬁxed set output class goal return predict class hatorcircumﬂex notation yi refer hat estimated predict value output class refer set cinstead sentiment analysis input xmight review text output set ymight set set language input text need know language write output set yis set language nig way classiﬁcation method use rule handwritten human example rule like word love appear handwritten rule component modern nlp system write list positive negative word sentiment analysis rule fragile situation datum change time task complex interaction different feature like example negation rule hard human come rule successful situation method introduce later ask large language model type introduce chapter prompt model label text prompt powerful weakness language model hallucinate able explain choose class reason common way classiﬁcation use vise machine learning supervise machine learning paradigm insupervise machine achine learning classification addition input set output class label training set learn algorithm talk training set chapter locus computing gram statistic supervise machine learn training set label mean contain set input observation observation ciate correct output supervision signal generally refer training set minput output pair input xi text case text classiﬁcation hand label associated class correct label training set use superscript parenthesis refer individual observation instance training set sentiment classiﬁcation training set set sentence text correct sentiment label goal learn training set classiﬁer capable mapping new input xto correct class learn ﬁnd feature training sentence word like awesome awful probabilistic classiﬁer subset machine learn classiﬁer addition give answer class observation additionally tell probability observation class distribution class useful information downstream decision avoid make discrete decision early useful combine system algorithm achieve supervise machine learn task naive baye support vector machine neural network ﬁne tune language el logistic regression advantage discuss introduce machine learning classiﬁer component feature representation input input observation vector feature generally refer feature ifor input simpliﬁed notation multiclass classiﬁcation classiﬁcation function compute estimate class introduce sigmoid andsoftmax tool classiﬁcation objective function want optimize learn usually ing minimize loss function correspond error training example introduce cross entropy loss function algorithm optimize objective function introduce tic gradient descent algorithm high level logistic regression supervise machine ing classiﬁer phase training train system case logistic regression mean e weight wandb introduce stochastic gradient descent cross entropy loss test give test example xwe compute probability class return high probability label logistic regression classify observation class like positive sentiment negative sentiment class mathematic class case simple ﬁrst describe special case logistic regression section begin moid function turn multinomial logistic regression class use softmax function section chapter ogistic regression sigmoid function goal binary logistic regression train classiﬁer binary decision class new input observation introduce sigmoid classiﬁer help decision consider single input observation represent vector feature sample feature subsection classiﬁer output ycan mean observation member class observation member class want know probability observation member class decision positive sentiment versus negative sentiment feature represent count word document probability document positive sentiment probability document negative ment logistic regression solve task learn training set vector weight bias term weight wiis real number associate input feature weight wirepresent important input feature classiﬁcation decision positive provide evidence stance classiﬁe belong positive class negative provide evidence instance classiﬁe belong negative class expect sentiment task word awesome high positive weight abysmal negative weight bias term call intercept bias term intercept real number add weighted input decision test instance learn weight training classiﬁer ﬁrst multiplies xiby weight sum weighted feature add bias term result single number zexpresse weighted sum evidence class rest book represent sum dot product notation dot product linear algebra dot product vector aandb write sum product correspond element vector notice represent vector boldface notation following equivalent formation note force zto legal probability lie fact weight real value output negative zrange create probability pass zthrough sigmoid function sigmoid sigmoid function name look like call logistic tion give logistic regression sigmoid follow equation logistic function show graphically fig rest book use notation exp mean sigmoid number advantage take real value number map lassification logistic regression figure sigmoid function real value map range nearly linear outli value squashed want probability nearly linear ﬂatten end tend squash outli value differentiable section handy learn apply sigmoid sum weighted feature number probability need sure case sum follow sigmoid function property express finally terminological point input sigmoid function score call logit logit function logit inverse sigmoid logit function log odd ratiop lnp term logit forzi way remind sigmoid turn range probability implicitly interpret zas real value number speciﬁcally log odd classiﬁcation logistic regression sigmoid function prior section give way instance xand compute probability decision class apply test instance give yes probability decision boundary decision chapter ogistic regression decision let example apply logistic regression classiﬁer language task sentiment classiﬁcation suppose binary sentiment classiﬁcation movie review text like know assign sentiment class review document doc represent input observation feature input show follow table fig show feature sample mini test document var deﬁnition value fig lexicon word lexicon word count pronoun count doc hokey virtually surprise writing second rate enjoyable thing cast great nice touch music overcome urge couch start dance suck figure sample mini test document show extract feature vector let assume moment learn real value weight feature weight correspond feature discuss section weight learn weight example indicate important feature number positive lexicon word great nice enjoyable etc positive sentiment decision importance negative lexicon word note positive mean negative word negatively associate positive sentiment decision twice important positive word give feature input review lassification logistic regression put classiﬁcation task feature logistic regression apply sort nlp task property input feature consider task period disambiguation decide periodperiod disambiguation end sentence word classify period class eo end sentence eos use feature like express current word low case positive weight feature express current word abbreviation dictionary prof negative weight feature express combination tie example period follow upper case word likely eos word previous word capitalize period likely shortening word street follow street low upper designing versus learn feature classic model feature design hand examine training set eye linguistic intuition literature supplement insight error analysis training set early version system consider feature interaction complex feature arefeature interaction combination primitive feature see feature period biguation period word likely end sentence previous word capitalize feature create automatically viafeature template abstract speciﬁcation feature example bigramfeature template template period disambiguation create feature pair word occur period training set feature space sparse create feature gram exist position training set feature generally create hash string description user description feature breakfast hash unique integer ithat feature number clear prior paragraph design feature hand quire extensive human effort reason recent nlp system avoid design feature instead focus representation learning way learn ture automatically unsupervised way input introduce method representation learning chapter chapter scale input feature different input feature extremely different range value common rescale comparable range chapter ogistic regression standardize input value center result zero mean standard standardize deviation transformation call score miz score mean value feature xiacross mobservation input dataset andsiis standard deviation value feature xiacross input dataset replace feature xiby new feature icompute follow mmx mmx alternatively normalize input feature value lie normalize have input datum comparable range useful compare value feature datum scaling especially important large neural network help speed gradient descent process example show equation logistic regression single example tice course want process entire test set example let suppose test set consist mtest example like classify continue use notation page superscript value parenthesis refer example index set datum ing test case test example feature vector usual represent vector matrix bold way compute output value loop compute test example time foreach input ﬁrst test example separately compute dicte follow turn slightly modify original equation efﬁciently use matrix arithmetic assign class example matrix operation pack input feature vector input xinto single input matrix row ii row vector consist feature vector example vector assume example ffeature ultinomial logistic regression weight xwill matrix shape follow introduce bas vector length mwhich consist scalar bias term brepeate mtime vector output scalar input feature vector represent weight vector column vector compute output single matrix multiplication addition convince compute thing loop example ﬁrst entry output vector correctly note reorder xandwfrom order appear multiplication come properly shape show modern compiler compute hardware compute matrix operation efﬁciently make computation fast important training testing large dataset note way keep xandwin original order choose deﬁne xdifferently matrix column vector vector input example instead row vector shape conventionally represent input row multinomial logistic regression need class want way sentiment classiﬁcation positive negative neutral assign label introduce chapter like speech word choose different part speech name entity type phrase choose tag like person location organization large language model predict word jvjpossible word vocabulary jvj way classiﬁcation case use multinomial logistic regression call softmax multinomial logistic regressiongression old nlp literature maxent ﬁer multinomial logistic regression want label observation class kfrom set kclasse stipulation class correct call hard classiﬁcation observation chapter ogistic regression multiple class let use following representation output yfor input xwill vector length class cis correct class set set element yto vector like thisy rest call hot vector job classiﬁer produce estimate vector class value ykwill classiﬁer estimate probability softmax multinomial logistic classiﬁer use generalization sigmoid call softmax function compute softmax function take vector softmax value map probability distribution value range value sum like sigmoid exponential function vector zof dimensionality softmax deﬁne softmax softmax input vector vector softmax denominatorpk normalize value probability example give vector result rounded softmax like sigmoid softmax property squash value input large tend push probability suppress probability small input finally note sigmoid refer vector score input softmax logit apply softmax logistic regression apply softmax logistic regression input moid dot product weight vector wand input vector bias need separate weight vector wkand bias bkfor class probability output class ykcan compute form make compute output arately instead common set equation efﬁcient putation modern vector processing hardware represent ultinomial logistic regression set kweight vector weight matrix wand bias vector row kof wcorrespond vector weight shape kthe number output class fthe number input feature bias vector bha value koutput class represent weight way compute vector output probability kclasse single elegant equation softmax work matrix arithmetic estimate score ﬁrst output class softmax correctly turn helpful interpretation weight matrix wis row wkas prototype class weight vector wkthat learn represent class prototype kind template vector similar high dot product dot product act similarity function logistic regression learn exemplar representation class incoming vector assign class kthey similar class doumbouya fig show difference binary multinomial logistic regression illustrate weight vector versus weight matrix computation output class probability feature multinomial logistic regression feature multinomial logistic regression act like feature binary logistic sion difference mention need separate weight vector bias kclasse recall binary exclamation point feature page binary classiﬁcation positive weight feature inﬂuence classiﬁer positive sentiment negative weight inﬂuence negative sentiment absolute value indicate important feature multinomial logistic regression contrast separate weight class feature evidence individual class way multiclass sentiment classiﬁcation example assign document class neutral feature relate mation mark negative weight document positive weight feature deﬁnition feature weight dependent input text output class dependence explicit represent feature self function input class notation represent feature single weight use kind notation description crf chapter chapter ogistic regression binary logistic regression featurevector lexiconword greatweight vector multinomial logistic regression featurevector lexiconword matrixthese red weightsare row correspondingto weight vector weight class prototype class figure binary versus multinomial logistic regression binary logistic regression use single weight vector scalar output multinomial logistic regression kseparate weight vector correspond kclasse pack single weight matrix vector output omit bias ﬁgure clarity learning logistic regression parameter model weights wand bias learn logistic regression instance supervised classiﬁcation know correct label observation system produce system estimate true want learn parameter mean andb yfor training observation close possible true require component foreshadow introduction chapter ﬁrst metric close current label true gold label measure similarity usually talk opposite thedistance system output gold output distance thelossfunction cost function section introduce loss loss function commonly logistic regression neural cross loss function thecross entropy loss second thing need optimization algorithm iteratively update weight minimize loss function standard algorithm gradient descent introduce stochastic gradient descent algorithm following section describe algorithm simple case binary logistic sion section turn multinomial logistic regression section cross entropy loss function need loss function express observation close classiﬁer output correct output ydiffer true loss function prefer correct class label ing example likely call conditional maximum likelihood estimation choose parameter log probability true ylabel training datum give observation result loss function negative log likelihood loss generally call cross entropy loss entropy loss let derive loss function apply single observation like learn weight maximize probability correct label discrete outcome bernoulli distribution express probability classiﬁer produce observation following keep mind simpliﬁes simpliﬁes log side turn handy mathematically hurt value maximize probability maximize log probability ylog describe log likelihood maximize order turn loss function need minimize ﬂip sign result cross entropy loss lce finally plug deﬁnition let loss function right thing example fig want loss small model estimate close correct big model confused ﬁrst let suppose correct gold label sentiment example fig positive case model chapter ogistic regression give example high probability positive negative plug right equation drop lead follow loss use log mean natural log base speciﬁed contrast let pretend instead example fig actually negative reviewer go line movie terrible beg case model confused want loss high plug left equation drop sure loss ﬁrst classiﬁer loss second classiﬁer minimize negative log probability want perfect classiﬁer assign probability correct outcome probability incorrect outcome mean yequal high yis close well classiﬁer low yis close bad classiﬁer yequal instead high close well classiﬁer negative log true yequal true equal convenient loss metric go negative log loss inﬁnity negative log inﬁnite loss loss function ensure probability correct answer maximize probability incorrect answer minimize sum increase probability correct answer come expense incorrect answer call entropy loss formula cross entropy true probability distribution yand estimate distribution know want minimize section ﬁnd minimum gradient descent goal gradient descent ﬁnd optimal weight minimize loss function deﬁne model explicitly represent fact cross entropy loss function lceis parameterized weight machine learning general refer parameter learn case logistic regression goal ﬁnd set weight minimize loss function average example argmin mmx radient descent shall ﬁnd minimum loss function gradient descent method ﬁnd minimum function ﬁgure direction space parameter function slope rise steeply move opposite direction intuition hike canyon try descend quickly river look direction ﬁnd direction ground slope steep walk downhill direction logistic regression loss function conveniently convex convex convex tion minimum local minima stick gradient descent start point guarantee ﬁnd minimum contrast loss multi layer neural network non convex gradient descent stick local minima neural network training ﬁnd global mum algorithm concept gradient design direction vector let ﬁrst consider visualization case parameter system single scalar show fig give random initialization wat value assume loss function lhappene shape fig need algorithm tell iteration left make right make reach minimum wloss loss stepof gradientdescent figure ﬁrst step iteratively ﬁnde minimum loss function move win reverse direction slope function slope negative need win positive direction right superscript learn step initial value value second step gradient descent algorithm answer question ﬁnde gradient gradient loss function current point move opposite direction gradient function variable vector pointing direction great increase function gradient multi variable generalization slope function variable like fig informally think gradient slope dotted line fig show slope hypothetical loss function point slope dotted line negative ﬁnd minimum gradient descent tell opposite direction move win positive direction magnitude gradient descent value slope learning rate high fast learn learn chapter ogistic regression rate mean wmore step change parameter learn rate time gradient slope single variable example let extend intuition function scalar variable wto variable want left right want know dimensional space nparameter gradient vector express directional component sharp slope ndimension imagine weight dimension weight wand bias gradient vector orthogonal component tell ground slope thewdimension bdimension fig show visualization value dimensional gradient vector take red point actual logistic regression parameter vector wis long input feature vector xcan long need weight wifor dimension variable bias gradient component tell slope respect variable dimension express slope partial loss function essentially ask small change variable wiinﬂuence total loss function formally gradient multi variable function fis vector component express partial derivative fwith respect able use invert greek delta symbol ñto refer gradient represent dependence qmore obvious ﬁnal equation update qbase gradient figure visualization gradient vector red point dimension wand show red arrow plane pointing direction look minimum opposite direction gradient recall gradient point direction increase radient descent gradient logistic regression order update need deﬁnition gradient recall logistic regression cross entropy loss function turn derivative function observation vector xis interested reader section derivation equation equation equivalent form note equation gradient respect single weight resent intuitive value difference true yand estimated observation multiply correspond input value stochastic gradient descent algorithm stochastic gradient descent online algorithm minimize loss function compute gradient training example nudge qin right direction opposite direction gradient online algorithm process input example example wait see entire input stochastic gradient descent call stochastic choose single random example time section discuss version gradient descent batch example fig show algorithm learning rate ahyperparameter adjust high hyperparameter learner step large overshoot minimum loss function low learner step small long minimum common start high learning rate slowly decrease function iteration kof training notation hkcan mean value learn rate iteration discuss hyperparameter detail chapter short special kind parameter machine learning model unlike regular eter model weight like wandb learn algorithm training set hyperparameter special parameter choose algorithm designer affect algorithm work work example let walk single step gradient descent algorithm use simpliﬁed version example fig see single observation correct value positive review feature vector consist feature count positive lexicon word count negative lexicon chapter ogistic regression function stochastic gradient descent loss function function parameterize set training input set training output label small random value repeat til caption training tuple random order optional report tuple compute estimate output compute loss far true output qto maximize loss way instead return figure stochastic gradient descent algorithm step compute loss mainly report current tuple need compute loss order compute gradient algorithm terminate converge gradient norm progress halt example loss start go hold set weight initialize logistic regression small random value neural network chapter let assume initial weight bias set initial learning ratehis single update step require compute gradient multiply learning rate mini example parameter gradient vector sion compute ﬁrst gradient follow gradient compute new parameter vector move opposite direction gradient step gradient descent weight shift note observation xhappene positive example expect see negative example high count negative word weight shift negative radient descent mini batch training stochastic gradient descent call stochastic choose single random example time move weight improve performance single example result choppy movement common compute gradient batch training instance single instance example batch training compute gradient entire dataset batch training see example batch training offer superb estimate rection weight cost spend lot time process single example training set compute perfect direction compromise mini batch training train group mexample mini batch hap dataset mis size dataset batch gradient descent tic gradient descent mini batch training advantage computational efﬁciency mini batch easily vectorize choose size batch base computational resource allow process ple mini batch parallel accumulate loss possible individual batch training need deﬁne mini batch version cross entropy loss function deﬁne section gradient section let extend entropy loss example mini batch size continue use notation ith training feature training label respectively assumption training example independent label logmy cost function mini batch mexample average loss example mmx mmx mini batch gradient average individual gradient mmx instead sum notation efﬁciently compute gradient matrix form follow vectorization see page matrix xof size minput batch vector yof correct chapter ogistic regression learning multinomial logistic regression loss function multinomial logistic regression generalize loss function binary logistic regression kclasse recall cross entropy loss binary logistic regression repeat loss function multinomial logistic regression generalize term non zero non zero kterm mention multinomial regression represent andˆyas vector true label yis vector kelement correspond class correct class element ybee classiﬁer produce estimate vector kelement element ˆykof represent estimate probability loss function single example generalize binary logistic gression sum log koutput class weight cator function turn negative log probability correct class ˆyk cis correct class cis correct class correct class class let correct vector ytake value value mean term sum term correspond true class cross entropy loss simply log output probability correspond correct class negative log likelihood loss log likelihood loss course gradient descent need loss need gradient gradient single example turn similar gradient binary logistic regression see let consider piece gradient derivative single weight class weight ith element input partial derivative loss respect derivative turn difference true value class probability classiﬁer output class weight value input xicorresponde ith element valuation precision recall measure vector class return case gradient softmax regression introduce neural network chapter time discuss derivation gradient equation evaluation precision recall measure introduce method evaluate text classiﬁcation let ﬁrst consider simple binary detection task example spam detection goal label text spam category positive spam category negative item email document need know system call spam need know email actually spam human deﬁne label document try match refer human label gold label gold label imagine ceo delicious pie company need know people say pie social medium build system detect tweet concern delicious pie positive class tweet delicious pie negative class tweet case need metric know spam detector pie tweet detector evaluate system detect thing start build confusion matrix like show fig confusion matrixconfusion matrix table visualize algorithm perform respect human gold label dimension system output gold label cell label set possible outcome spam detection case example true positive document spam indicate human create gold label system correctly say spam false negative document spam system incorrectly label non spam right table equation accuracy ask percentage observation spam pie example mean email tweet system label correctly accuracy natural metric generally use text classiﬁcation task accuracy work class unbalanced spam large majority email tweet mainly pie explicit imagine look million tweet let discuss love hatred pie tweet completely unrelated imagine simple classiﬁer stupidly classiﬁed tweet pie classiﬁer true negative false negative accuracy amazing accuracy level surely happy classiﬁer course fabulous pie classiﬁer completely useless ﬁnd single customer comment look word accuracy good metric goal chapter ogistic regression true positivefalse negativefalse positivetrue negativegold positivegold negativesystempositivesystemnegativegold standard labelssystemoutputlabelsrecall figure confusion matrix visualize binary classiﬁcation system form gold standard label discover rare completely balanced frequency common situation world instead accuracy generally turn metric show fig precision andrecall measure percentage item precision system detect system label positive fact positive positive accord human gold label precision deﬁne precision true positive true positive false positive recall measure percentage item actually present input recall correctly identiﬁe system recall deﬁne recall true positive true positive false negative precision recall help solve problem useless pie classiﬁer classiﬁer despite have fabulous accuracy terrible recall true positive false negative recall convince precision ﬁnde relevant tweet equally problematic precision recall unlike accuracy emphasize true positive ﬁnde thing suppose look way deﬁne single metric incorporate aspect precision recall simple combination measure van measure rijsbergen deﬁne thebparameter differentially weight importance recall precision base need application value favor recall value favor precision precision recall equally ance frequently metric call measure come weighted harmonic mean precision recall harmonic mean set number reciprocal arithmetic mean valuation precision recall measure rocal harmonicmean measure harmonic mean harmonic mean value close minimum value arithmetic mean weigh low number heavily conservative situation evaluate class describe text classiﬁcation task class lot classiﬁcation task language processing class sentiment analysis generally class positive negative neutral class common task like speech tagging word sense disambiguation semantic role labeling emotion detection luckily naive baye algorithm multi class classiﬁcation algorithm labelssystemoutputrecallu recall figure confusion matrix class categorization task show pair class document assign need slightly modify deﬁnition precision recall sider sample confusion matrix hypothetical way email rization decision urgent normal spam show fig matrix show example system mistakenly label spam document urgent show compute distinct precision recall value class order derive single metric tell system bine value way macroaverage compute performance macroaverage class average class microaverage collect microaverage cision class single confusion matrix compute precision recall table fig show confusion matrix class separately show computation microaveraged macroaverage precision ﬁgure show microaverage dominate frequent class case spam count pool macroaverage well reﬂect statistic small class appropriate performance class equally chapter ogistic regression pooledclass spamclass normalclass urgent figure separate confusion matrix class previous ﬁgure show pool sion matrix microaveraged macroaverage precision test set cross validation training testing procedure text classiﬁcation follow see language modeling section use training set train model use thedevelopment test set call devset tune parameter development test set devset general decide good model come think good model run hitherto unseen test set report performance use devset avoid overﬁtte test set have ﬁxed ing set devset test set create problem order save lot datum training test set devset large representative well use datum training use datum test cross validation cross validation cross validation choose number partition datum kdisjoint subset call fold choose kfold test set train fold classiﬁer remain fold compute error rate test set repeat fold test set train fold sampling process ktime average test set error rate krun average error rate choose train different model datum test model time average value call fold cross validation fold cross validation problem cross validation data testing need corpus blind examine datum suggest possible feature general go peek test set cheating cause overestimate mance system look corpus understand go important design nlp system reason mon create ﬁxed training set test set fold cross validation inside training set compute error rate normal way test set show fig tatistical significance test training setteste figure fold cross validation statistical signiﬁcance testing building system need compare performance system know new system build well old well system describe literature domain statistical hypothesis testing section introduce test statistical signiﬁcance nlp classiﬁer draw especially work dror kirkpatrick suppose compare performance classiﬁer aandbon metric accuracy want know new sentiment classiﬁer aget high previous sentiment classiﬁer bon particular test setx let score system aget test set performance difference aandbonx like know mean logistic regression classiﬁer high naive baye classiﬁer call effect size effect size big dmean aseem way well small dmean aseem little well check positive suppose ﬁnd ais high certain ais well amight accidentally well bon particular need want know superiority bis likely hold check test set set circumstance paradigm statistical hypothesis testing test formalize hypothesis hypothesis call null hypothesis suppose actually null hypothesis tive zero mean ais well like know conﬁdently rule hypothesis instead support ais well create random variable xrange test set ask likely null hypothesis correct test chapter ogistic regression encounter value find repeat experiment great time formalize likelihood value probability value assume null hypothesis true see see great true example value probability aisnotbett huge let aha respectable andbha terrible surprised extremely unlikely occur fact true value low unlikely large difais fact well small surprising true ais well value high small value mean difference observe unlikely null hypothesis reject null hypothesis count small common use value like threshold value mean value probability observe dwe saw assume true reject null hypothesis assume ais well result ais well statistically signiﬁcant ifstatistically signiﬁcant thedwe see probability threshold reject null hypothesis compute probability need value nlp erally use simple parametric test like test anov familiar parametric test assumption distribution test statistic normality generally hold case nlp usually use non parametric test base sample artiﬁcially create sion experimental setup example lot different test set measure give distribution set threshold like distribution delta small delta observe value bility see big see reject null hypothesis agree sufﬁciently surprising difference ais well algorithm common non parametric test nlp approximate domization noreen bootstrap test describe bootstrapapproximate randomization show pair version test common nlp pair test compare set observation align pair observation set pair observation pen naturally compare performance system test set pair performance system aon individual observation performance system bon pair bootstrap test thebootstrap test efron tibshirani apply metric bootstrap test cision recall bleu metric machine translation word bootstrappe refer repeatedly draw large number sample bootstrappe ment call bootstrap sample original set intuition bootstrap test create virtual test set observed test set edly sample method make assumption sample representative tatistical significance test consider tiny text classiﬁcation example test set xof document ﬁrst row fig show result classiﬁer test set document label possibility right wrong right wrong wrong right slash letter mean classiﬁer get answer wrong ﬁrst document correct class second document get right get wrong assume simplicity metric accuracy accuracy create large number virtual test set size fig show couple example create virtual test set repeatedly time select cell row xwith replacement example create ﬁrst cell ﬁrst virtual test set happen randomly select second cell xrow copy value new cell create second cell time sample randomly choosing original xwith replacement figure pair bootstrap test example bpseudo test set create initial true test set pseudo test set create sample time replacement individual sample single cell document gold label correct incorrect performance classiﬁer course real test set example bneed large bt set provide sampling distribution statistic aha accidental advantage way compute advantage follow version lay berg kirkpatrick assume well expect estimate test set zero negative high value surprising assume well measure exactly surprising observed circumstance compute value count test set expect zero value value bbx use notation mean xis true generally true expect value test set assume well true bootstrappe test set create draw sample distribution mean happen create original test set happen bias favor measure surprising observe actually compute value count test set chapter ogistic regression expect value value bbx bbx example test set threshold test set ﬁnd accidentally well result value small indicate delta find sufﬁciently surprising unlikely happen accident reject null hypothesis conclude ais well function bootstrap test set num sample value calculate well algorithm draw bootstrap sample size select member xat random add calculate well algorithm value sample algorithm beat expectation return value observed dis probably accidental figure version pair bootstrap algorithm berg kirkpatrick algorithm bootstrap show fig give test set number sample count percentage bbootstrap test set percentage act side empirical value avoid harm classiﬁcation important avoid harm result classiﬁer harm exist naive baye classiﬁer classiﬁcation algorithm introduce later chapter class harm representational harm crawford blodgett harm harm cause system demean social group example petuate negative stereotype example kiritchenko mad examine performance sentiment analysis system pair sentence identical contain common african ican ﬁrst like shaniqua common european american ﬁrst like stephanie choose caliskan study discuss chapter find system assign low sentiment negative emotion sentence african american name reﬂecte perpetuate stereotype associate african americans negative emotion popp nterprete model task classiﬁer lead representational harm harm silence example important text classiﬁcation task icity detection task detect hate speech abuse harassment othertoxicity detection kind toxic language goal classiﬁer help reduce etal harm toxicity classiﬁer cause harm example researcher show widely toxicity classiﬁer incorrectly ﬂag toxic sentence non toxic simply mention identity like woman park blind people hutchinson gay people dixon dias oliva simply use linguistic feature characteristic variety like african american vernacular english sap davidson false positive error lead silencing discourse group model problem cause bias problem training datum general machine learn system replicate amplify bias training datum problem cause label example bias human labeler resource like lexicon model component like pretraine embedding model architecture like model train optimize mitigation bias example carefully consider training datum source important area research currently general solution reason important introduce nlp model study kind factor clear way release model card mitchell model card version model model card document machine learning model information like training algorithm parameter training datum source motivation preprocesse evaluation datum source motivation preprocesse intend use user model performance different demographic group ronmental situation interpreting model want know correct classiﬁcation observation want know classiﬁer decision want decision interpretable interpretability hard deﬁne strictly interpretable core idea human know algorithm reach sion feature logistic regression human design way understand classiﬁer decision understand role feature play decision logistic regression combine statistical test likelihood ratio test wald test investigate particular feature signiﬁcant test inspect magnitude large weight wassociate feature help interpret classiﬁer decision make enormously important build transparent model furthermore addition use classiﬁer logistic regression nlp ﬁeld widely analytic tool test hypothesis effect explanatory variable feature text classiﬁcation want know logically negative word likely chapter ogistic regression ciate negative sentiment negative review movie likely discuss cinematography necessary control tential confound factor inﬂuence sentiment movie genre year length review word e relationship nlp extract linguistic feature non linguistic outcome hospital readmission political outcome product sale need control confound age patient county voting brand product case logistic regression allow test feature associate outcome effect feature advanced regularization numquam ponenda est pluralita sine necessitate plurality propose need william occam problem learn weight model perfectly match training datum feature perfectly predictive outcome happen occur class assign high weight weight feature attempt perfectly detail training set fact perfectly model noisy factor accidentally correlate class problem call overﬁtte good model able generalize training overﬁtte generalize datum unseen test set model overﬁts poor generalization avoid overﬁtte new regularization term add loss regularization tion result following loss batch mexample slightly rewrite maximize log probability minimize loss remove mterm affect argmax argmax qmx new regularization term penalize large weight setting weight match training datum perfectly use weight high value penalize setting match datum little small weight high regularization strength parameter low model weight reduce reliance training datum common way compute regularization term ularization quadratic function weight value name use regularization square norm weight value norm theeuclidean distance vector qfrom origin qconsist nweight dvanced regularization regularize loss function argmax regularization linear function weight value name regularization sum absolute value weight manhattan distance manhattan distance distance walk point city street grid like new york regularize loss function argmax kind regularization come statistic regularization call lasso regression tibshirani regularization call ridge regression lasso ridge commonly language processing regularization easy optimize simple derivative derivative regularization complex derivative jqji non continuous zero prefer weight vector small weight prefer sparse solution large weight weight set zero regularization lead sparser weight vector far few feature regularization bayesian interpretation constraint prior weight look regularization view laplace prior weight regularization correspond assume weight distribute accord gaussian distribution mean gaussian normal distribution away value mean low probability scale variance gaussian prior weight say weight prefer value gaussian weight qji jexp multiply weight gaussian prior weight e follow constraint argmax qmy jexp log space assume correspond argmax qmx form chapter ogistic regression advance derive gradient equation section derivation gradient cross entropy loss tion lcefor logistic regression let start quick calculus refresher derivative second elegant derivative sigmoid finally chain rule derivative suppose compute derivative chain rule composite function derivative derivative respect derivative respect want know derivative loss function respect single weight need compute weight bias chain rule rely derivative log rearrange term plug derivative sigmoid chain rule time end ummary summary chapter introduce logistic regression model classiﬁcation logistic regression supervise machine learning classiﬁer extract real value feature input multiplie weight sum pass sum sigmoid function generate probability threshold decision logistic regression class positive negative sentiment multiple class multinomial logistic regression ample ary text classiﬁcation speech labeling etc multinomial logistic regression use softmax function compute bilitie weight vector wand bias learn label training set loss function cross entropy loss minimize minimize loss function convex optimization problem iterative algorithm like gradient descent ﬁnd optimal weight avoid overﬁtte logistic regression useful analytic tool ability transparently study importance individual feature historical note logistic regression develop ﬁeld statistic analysis binary datum particularly common medicine cox start late widely linguistic formal foundation study linguistic variation sankoff labov nonetheless logistic regression common natural language cesse appear simultaneously direction ﬁrst source neighbor ﬁeld information retrieval speech processing use regression lend statistical technique nlp early use logistic regression document routing ﬁrst nlp application use lsi embedding word representation sch time early logistic regression develop ply nlp ibm research maximum entropy modeling ormaximum entropy maxent berger seemingly independent statistical literature der apply language modeling rosenfeld speech tagging ratnaparkhi parse ratnaparkhi coreference resolution kehler text classiﬁcation nigam variety source cover kind text classiﬁcation task sentiment analysis pang lee liu zhang stamatato survey authorship attribute algorithm language tion jauhiainen jaech important early neural system task newswire indexing test case text ﬁcation algorithm base collection newswire article man aggarwal zhai text classiﬁcation classiﬁcation general cover machine learn textbook hastie chapter ogistic regression witten frank bishop murphy non parametric method compute statistical signiﬁcance ﬁrst nlp muc competition chinchor early speech recognition gillick cox bisani ney description bootstrap draw description berg kirkpatrick recent work focus issue include multiple test set multiple metric søgaard dror feature selection method remove feature unlikely generalize feature generally rank informative tion decision common metric information gain tell bit ofinformation gain information presence word give guess class feature selection metric include pointwise mutual information gini index yang pedersen comparison guyon elisseeff introduction feature selection exerciseschapter ﬁsh ﬁsh forget net meaning meaning forget word chapter asphalt los angeles famous occur mainly freeway middle city patch asphalt brea tar pit asphalt preserve million fossil bone ice age tocene epoch fossil smilodon saber toothe tiger instantly recognizable long canine million year ago completely different saber tooth tiger call thylacosmilus live argentina part south ica thylacosmilus marsupial smilodon placental mammal lacosmilus long upper canine like smilodon protective bone ﬂange low jaw similarity mammal example parallel convergent evolution particular context environment lead evolution similar structure different specie gould role context important similarity biological kind organism word word occur similar contexts tend similar meaning link similarity word distribute similarity mean call distributional hypothesis hypothesis wasdistributional hypothesis ﬁrst formulate linguist like joo harris firth notice word synonym like oculist andeye doctor tend occur environment near word like eyeorexamine meaning difference word correspond roughly difference environment harris chapter introduce embedding vector representation meaning embedding word learn directly word distribution text embedding lie heart large language model modern application static bedding introduce underlie powerful dynamic contextualized embedding likebert chapter chapter linguistic ﬁeld study embedding meaning call vector semantic embedding ﬁrst example book representationvector semantic learning automatically learn useful representation input text findingrepresentation learning self supervise way learn representation language instead ing representation hand feature engineering important principle modern nlp bengio chapter mbedding lexical semantic let begin introduce basic principle word meaning represent meaning word gram model chapter classical nlp application representation word string letter index vocabulary list representation different tradition philosophy see introductory logic class meaning word represent spell word small capital letter represent meaning dog dog cat cat apostrophe dog represent meaning word capitalize pretty unsatisfactory model see version joke originally semanticist barbara partee carlson meaning life life surely well want model word mean sort thing tell word similar ing catis similar dog antonyms cold opposite hot positive connotation happy negative connotation sad represent fact meaning buy sell payoffer differ spective underlying purchasing event buy probably sell likely pay generally model word meaning allow draw inference address meaning relate task like question answering dialogue section summarize desiderata draw result linguistic study word meaning call lexical semantic return tolexical semantic expand list appendix chapter lemma sense let start look word choose mouse deﬁne dictionary simpliﬁed online dictionary wordnet mouse numerous small rodent hand operate device control cursor form mouse lemma call citation form form lemma citation form mouse lemma word mouse dictionary separate deﬁnition inﬂected form like mouse similarly sing lemma sing sing sing language inﬁnitive form lemma verb spanish dormir sleep lemma duerme sleep speciﬁc form sing orcarpet orse orduerme call wordform wordform example show lemma multiple meaning lemma mouse refer rodent cursor control device aspect meaning mouse aword sense fact lemmas bepolysemous multiple sense interpretation difﬁcult search mouse info look pet widget chapter appendix discuss problem polysemy introduce word sense disambiguation task determine sense word particular context synonymy important component word meaning relationship tween word sense example word sense mean exical semantic identical sense word nearly identical sense word synonym synonym include pair synonym couch sofa vomit throw ﬁlbert hazelnut car automobile formal deﬁnition synonymy word sense word synonymous substitutable sentence change truth condition sentence situation sentence true substitution pair word like car automobile ter truth preserving word identical meaning probably word absolutely identical meaning fundamental tenet semantic call principle contrast girard eal clarkprinciple contrast state difference linguistic form associate ference meaning example word scientiﬁc context inappropriate hiking guide water appropriate genre difference meaning word practice word onym describe relationship approximate rough synonymy word similarity word synonym word lot similar word catis synonym dog cat anddog certainly similar word move synonymy similarity useful shift talk relation word sense like synonymy relation word like similarity deal word avoid have commit particular representation word sense turn simplify task notion word similarity useful large semantic task know similarity similar word help compute similar meaning phrase sentence important component task like question ing paraphrasing summarization way get value word similarity ask human judge similar word number dataset result experiment example dataset hill give value scale like example range near synonyms vanish disappear pair scarcely common hole agreement vanish disappear belief impression muscle bone modest ﬂexible hole agreement word relatedness meaning word relate way similarity class connection call word relatedness budanitsky relatedness hirst traditionally call word association psychology association consider meaning word coffee andcup coffee similar cup share practically feature coffee plant beverage cup manufacture object particular shape coffee cup clearly relate associate participate everyday event event drinking coffee cup similarly scalpel andsurgeon similar relate eventively surgeon tend use scalpel common kind relatedness word belong semantic ﬁeld semantic ﬁeld set word cover particular semantic semantic ﬁeld domain bear structured relation example word chapter mbedding relate semantic ﬁeld hospital surgeon scalpel nurse thetic hospital restaurant waiter menu plate food chef house door roof kitchen family bed semantic ﬁeld relate topic model like latent topic model dirichlet allocation lda apply unsupervised learning large set text induce set associate word text semantic ﬁeld topic model useful tool discover topical structure document appendix introduce relation sense like hypernymy oris antonymy opposite meronymy relation connotation finally word affective meaning orconnotation word connotation connotation different meaning different ﬁeld use mean aspect word meaning relate writer reader emotion ment opinion evaluation example word positive connotation wonderful negative connotation dreary word meaning similar way vary connotation consider difference connotation fake knockoff forgery hand copy replica reproduction innocent positive connotation naive negative connotation word describe positive evaluation great love ative evaluation terrible hate positive negative evaluation language call sentiment see appendix word sentiment play role sentiment tant task like sentiment analysis stance detection application nlp language politic consumer review early work affective meaning osgood find word vary important dimension affective meaning valence pleasantness stimulus arousal intensity emotion provoke stimulus dominance degree control exert stimulus word like happy orsatisﬁed high valence unhappy noye low valence excited high arousal calm low arousal controlling high dominance awe orinﬂuence low dominance word represent number correspond value dimension valence arousal dominance courageous music heartbreak cub osgood notice number represent meaning word model represent word point dimensional space vector dimension correspond word rating scale revolutionary idea word meaning resent point space meaning heartbreak represent point ﬁrst expression vector tic model introduce vector semantic intuition vector semantic standard way represent word meaning nlp helpingvector ector semantic theintuition model aspect word meaning see previous section root model lie big idea converge osgood idea mention use point dimensional space represent connotation word proposal linguist like joo harris firth deﬁne meaning word distribution language use mean neighboring word grammatical environment idea word occur similar distribution neighboring word similar similar meaning example suppose know meaning word ongchoi cent borrowing cantonese following contexts ongchoi delicious sauteed garlic ongchoi superb rice ongchoi leave salty sauce suppose see context word contexts spinach saute garlic rice chard stem leave delicious collard greens salty leafy green fact ongchoi occur word like riceandgarlic anddelicious salty word like spinach chard collard greens suggest ongchoi leafy green similar leafy implement intuition computationally count word context ongchoi figure dimensional sne visualization dimensional bedding word close word sweet show word similar ing nearby space visualization create tensorboard embed projector idea vector semantic represent word point sional semantic space derive different way bution word neighbor vector represent word call embedding embedding word embed derive historically mathematical sense ping space structure meaning shift end chapter fig show visualization embedding learn algorithm show location select word neighbor sweet project fact ipomoea aquatica relative morning glory call water spinach chapter mbedding dimensional space dimensional space note near neighbor sweet semantically relate word like honey candy juice chocolate idea similar word near high dimensional space important offer enormous power language model nlp application example sentiment classiﬁer chapter depend word appear training test set represent word embedding classiﬁer assign sentiment long see word similar meaning vector semantic model like one show fig learn automatically text supervision chapter begin simple pedagogical model embedding meaning word deﬁne vector count nearby word introduce model helpful way understand concept vector mean vector representation word meaning ticate variant like idf model introduce chapter important method understand method result long vector sparse zero word simply occur context introduce model family construct short dense vector useful semantic property introduce cosine standard way use embedding pute semantic similarity word sentence document important tool practical application simple count base embedding important attribute vector space flocation location location randall munroe hover let introduce ﬁrst way compute word vector embedding pl vector model meaning base occurrence matrix way resent word occur deﬁne particular kind occurrence matrix word context matrix row matrix represent wordword context matrix vocabulary column represent word cabulary appear nearby matrix dimensionality cell record number time row target word column context word occur nearby training corpus mean nearby implement method let start simple context window word let word left word right cell represent number time training corpus column word occur word window row word let work word cherry strawberry digital tion word take single instance corpus word window instance traditionally follow cherry pie traditional dessert mixed strawberry rhubarb pie apple pie computer peripheral personal digital assistant device usually computer include information available internet occurrence word large corpus count context word word context occurrence matrix imple count embedding context occurrence matrix large word vocabulary sincejvj count occur word cabulary dimensionality let instead sketch process small scale imagine go look word consider follow context word computer pie furthermore let assume count occurrence mini corpus look fig compute hand count context word word cherry strawberry digital information computer pie cherry strawberry digital information figure occurrence vector word count window show potential context word dimension vector cherry outline red note real vector vastly dimension sparse hopefully count match show fig cell sent number time particular word deﬁne row occur ular context deﬁne word column row vector represent word review basic linear algebra vector heart list array number cherry represent vector list ﬁrst row vector fig information represent list fourth row vector avector space collection vector characterize dimension vector space dimension vector dimensional vector space element dimension space loosely refer vector dimensional space dimensional vector element dimension example fig choose document vector dimension page real term document matrix document vector dimensionality jvj vocabulary size ordering number vector space indicate different dimension document vary dimension vector correspond number time pieoccur context second dimension correspond number time word computer occur notice vector information anddigital value computer dimension reality compute word vector single context window instead compute entire corpus let real count look like let look vector compute way fig show subset word word occurrence matrix word impossible visualize jvjpossible context word page textbook subset dimension count compute wikipedia corpus davy note fig word cherry andstrawberry similar pieandsugar tend occur window word like digital conversely digital andinformation similar strawberry think vector document point jvj dimensional space document fig point dimensional space fig show spatial chapter mbedding aardvark computer datum result pie sugar cherry strawberry digital information figure occurrence vector word wikipedia corpus show dimension hand pick pedagogical purpose vector digital outline red note real vector vastly dimension sparser zero value dimension datainformation figure spatial visualization word vector digital andinformation show dimension correspond word datum andcomputer note thatjvj dimensionality vector generally size cabulary word frequent word training corpus keep word frequent generally helpful number zero sparse vector representation efﬁcient algorithm store compute sparse matrix possible apply kind weight function count cell popular weighting idf introduce chapter historically wide variety weighting intuition let examine detail put word similarity cosine measure similarity measure similarity target word vandw need metric take vector dimensionality word dimension lengthjvj document dimension length jdj give measure similarity far common similarity metric cosine angle vector cosine like measure vector similarity nlp base thedot product operator linear algebra call inner product dot product inner product dot product dot product act similarity metric tend high vector large value dimension alternatively vector osine measure similarity zero different dimension orthogonal vector dot product represent strong dissimilarity raw dot product problem similarity metric favor long vector vector length deﬁne vector length jvj vuutnx dot product high vector long high value dimension frequent word long vector tend occur word high occurrence value raw dot product high frequent word problem like similarity metric tell similar word regardless frequency modify dot product normalize vector length divide dot product length vector normalize dot product turn cosine angle vector follow deﬁnition dot product vector aandb jajjbjcosq jajjbj cosq thecosine similarity metric vector vandwthu compute cosine cosine jvjjwj vuutnx ivuutnx application pre normalize vector divide length create unit vector length compute unit vector aby unit vector divide byjaj unit vector dot product cosine cosine value range vector point direction orthogonal vector vector point opposite direction raw frequency value non negative cosine vector range let cosine compute word cherry ordigital close meaning information raw count follow shorten table pie datum computer cherry digital information model decide information way close digital cherry result sensible fig show chapter mbedding pie dimension computer figure rough graphical demonstration cosine similarity show vector word cherry digital information dimensional space deﬁne count word computer andpienearby ﬁgure cosine highlight angle note angle digital andinformation small angle cherry andinformation vector similar cosine large angle small cosine maximum angle vector small cosine angle compute word similarity task like ﬁnde word paraphrase tracking change word meaning automatically discover meaning word different corpora example ﬁnd similar word target word wby compute cosine wand word sorting look previous section see represent word sparse long vector dimension correspond word vocabulary introduce powerful word representation embedding short dense vector unlike vector see far embedding short number dimension drange large vocabulary size ddimension clear interpretation vector dense instead vector entry sparse zero count function count value value number negative turn dense vector work well nlp task sparse vector completely understand reason ition represent word dimensional dense vector require classiﬁer learn far few weight represent word dimensional tor small parameter space possibly help generalization ing overﬁtte dense vector well job capture synonymy example sparse vector representation dimension synonyms like car andautomobile dimension distinct unrelated sparse vector fail capture similarity word cara neighbor word automobile neighbor section introduce method compute embedding skip gram skip gram negative sampling call sgn skip gram algorithm sgn algorithm software package call algorithm loosely refer mikolov mikolov method fast efﬁcient train easily available line code pretraine embedding embedding static bedding mean method learn ﬁxed embed word thestatic embedding vocabulary chapter introduce method learn dynamic contextual embedding like popular family bert representation vector word different different contexts intuition instead count word cur near apricot instead train classiﬁer binary prediction task word wlikely near apricot actually care prediction task instead learn classiﬁer weight word embedding revolutionary intuition use run text implicitly supervise training datum classiﬁer word cthat occur near target word apricot act gold correct answer question word clikely near apricot method call self supervision avoid need self supervision sort hand label supervision signal idea ﬁrst propose task neural language modeling bengio collobert show neural language model neural network learn predict word prior word use word run text supervision signal learn embed representation word prediction task neural network chapter simple model neural network language model way simpliﬁes task make binary classiﬁcation instead word diction second simpliﬁes architecture train logistic regression classiﬁer instead multi layer neural network hidden layer demand sophisticated training algorithm intuition skip gram treat target word neighboring context word positive example randomly sample word lexicon negative sample use logistic regression train classiﬁer distinguish case use learn weight embedding classiﬁer let start think classiﬁcation task turn train imagine sentence like following target word apricot assume window context word lemon tablespoon apricot jam pinch goal train classiﬁer give tuple target word wpaired candidate context word example apricot jam apricot aardvark return probability cis real context word true forjam false aardvark probability word cis real context word wis minus classiﬁer compute probability intuition gram model base probability embed similarity word likely chapter mbedding occur near target embed vector similar target embed compute similarity dense embedding rely intuition vector similar high dot product cosine normalize dot product word similarity dot product probability number range element embedding negative dot product negative turn dot product probability use logistic orsigmoid function fundamental core logistic regression model probability word cis real context word target word sigmoid function return number probability need total probability possible event cis context word context word sum estimate probability word real context word equation give probability word context word window skip gram make simplifying assumption context word independent allow multiply probability summary skip gram train probabilistic classiﬁer give test target word wand context window lword assign probability base similar context window target word probability base apply logistic sigmoid function dot product embedding target word context word compute probability need embedding target word context word vocabulary fig show intuition parameter need skip gram actually store embedding word word target word consider context parameter need learn matrix wandc contain embedding jvjword vocabulary turn learn embedding real goal train classiﬁer ﬁrst place principle target matrix context matrix use different vocabulary simplify assume share vocabulary target wordscontext noiseword figure embedding learn skipgram model algorithm store bedding word target embed call input embed context embed call output embed parameter qthat gorithm learn matrix jvjvector dimension form concatenate matrix target embedding wand embedding learn skip gram embedding learn algorithm skip gram embedding take input corpus text choose vocabulary size begin assign random embed vector vocabulary word proceed iteratively shift bed word wto like embedding word occur nearby text like embedding word occur nearby let start consider single piece training datum lemon tablespoon apricot jam pinch example target word context word window result positive training instance left positive example pos apricot tablespoon apricot apricot jam apricot anegative examples neg neg apricot aardvark apricot seven apricot apricot forever apricot apricot dear apricot coaxial apricot train binary classiﬁer need negative example fact gram negative sampling sgn use negative example positive example ratio set parameter instance create knegative sample consist target wplus noise word cneg noise word random word lexicon constrain target word table right show setting negative example negative training set positive example noise word choose accord weighted unigram probability ais weight sample accord unweighted bility mean unigram probability choose word thea noise word unigram probability chapter mbedding choose aardvark practice common set use weighting count set give well performance give rare noise word slightly high probability rare word illustrate intuition help work probability example event increase probability rare event bfrom give set positive negative training instance initial set embedding goal learning algorithm adjust embedding maximize similarity target word context word pair positive example minimize similarity negative example consider word context pair knoise word express goal follow loss function lto minimize ﬁrst term express want classiﬁer assign real context word cposa high probability neighbor second term express want assign noise word cnegia high probability non neighbor multiply assume independence want maximize dot product word actual context word minimize dot product word knegative sample neighbor word minimize loss function stochastic gradient descent fig show intuition step learn gradient need derivative respect different embedding turn derivative following leave wcmove apricot jam close increase cpos waardvark apricot matrix apartdecrease apricot jam zebrazebraaardvarkjamapricot cposmatrixtolstoymove apricot tolstoy apartdecrease figure intuition step gradient descent skip gram model try shift bedding target embedding apricot close high dot product context embedding nearby word jam low dot product context embedding noise word occur nearby tolstoy andmatrix proof exercise end chapter update equation go time step stochastic gradient descent pos neg negi logistic regression learning algorithm start randomly tialized wandcmatrice walk training corpus gradient descent wandcso minimize loss make date recall skip gram model learn twoseparate embedding word thetarget embed wiand context embed store matrix thetarget embeddingcontext embed target matrix wand context matrix common add represent word iwith vector alternatively throw away matrix represent word iby vector simple count base method like idf context window size affect performance skip gram embedding experiment tune parameter lon chapter mbedding kind static embedding kind static embedding extension fasttext fasttext bojanowski address problem present far good way deal unknown word word appear test corpus unseen training corpus related problem word sparsity language rich morphology form noun verb occur rarely fasttext deal problem subword model represent word plus bag constituent gram special boundary symbol add word example word represent sequence plus character gram whe ere skipgram embed learn constituent gram word represent sum embedding constituent gram unknown word present sum constituent gram fasttext open source library include pretraine embedding language available widely static embed model glove pennington short global vector model base capture global corpus statistic glove base ratio probability word word occurrence matrix turn dense embedding like actually elegant ematical relationship count base embedding see implicitly optimize function count matrix particular ppmi ing levy goldberg visualize embedding dimension long dimension late economist martin shubik visualize embedding important goal help understand apply improve model word meaning visualize example dimensional vector rohde gonnerman plaut modeling word mean lexical occurrence headhandface dogamerica cateyeeurope footchinafrance chicagoarm finger noselegrussia mouseafrica atlantaearshoulderasia cow bullpuppylionhawaii montrealtokyotoemoscow tooth nashvillebrazilwrist kittenankle turtle oyster figure multidimensional scaling noun ankle shoulder arm leg hand foot head nose finger toe face ear eye tooth dog cat puppy kitten cow mouse turtle oyster lion bull chicago atlanta montreal nashville tokyochina russia africa asia europe america brazil moscow francehawaiifigure hierarchical clustering noun class distance base vector correlation simple way visualize meaning word wembedde space list similar word wby sort vector word vocabulary cosine vector example close word froguse particular embedding compute glove algorithm frog toad litoria dae rana lizard eleutherodactylus pennington visualization method use cluster algorithm hierarchical representation word similar embed space uncaptioned ﬁgure left use hierarchical clustering embed vector noun visualization method rohde emantic property embedding probably common visualization method project dimension word dimension fig show visualization fig projection method call sne van der maaten hinton semantic property embedding section brieﬂy summarize semantic property embedding study different type similarity association parameter vector semantic model relevant sparse ppmi vector dense vector size context window collect count generally word target word total context word choice depend goal representation short context window tend lead representation bit syntactic information come immediately nearby word vector compute short context window similar word target word wtend semantically similar word part speech vector compute long context window high cosine word target word wtend word topically related similar example levy goldberg show skip gram window similar word word hogwart harry potter series name ﬁctional school sunnydale buffy vampire slayer orevernight vampire series window similar word hogwart word topically related harry potter series dumbledore malfoy half blood useful distinguish kind similarity association word sch pedersen word ﬁrst order occurrenceﬁrst order occurrence call syntagmatic association typically nearby write ﬁrst order associate book orpoem word second order occurrence call paradigmatic association similarsecond order occurrence neighbor write second order associate word like say orremarke analogy relational similarity semantic property embedding ability capture relational meaning important early vector space model cognition rumelhart abrahamson propose parallelogram modelparallelogram model solve simple analogy problem form problem system give problem like apple apple tree grape ﬁll word vine parallelogram model lustrate fig vector word apple word add vector grape near word point return early work sparse embedding scholar show sparse vector el meaning solve analogy problem turney littman parallelogram method receive modern attention cess glove vector mikolov levy goldberg pennington example result expression vector close similarly result vector close embed model chapter mbedding treeapplegrapevine figure parallelogram model analogy problem rumelhart abrahamson location find subtract add ing representation relation like male capital comparative show fig glove figure relational property glove vector space show project vector dimension close offset capture comparative superlative morphology pennington mean algorithm give vector ﬁnd parallelogram method xdistance distance function euclidean distance caveat example close value return lelogram algorithm glove embed space usually fact input word morphological variant cherry red potato return potato orpotatoe instead brown explicitly exclude furthermore embed space perform task involve frequent word small distance certain relation like relate country capital verb noun inﬂecte form parallelogram method embedding work relation linzen gladkova schluter ethayarajh peterson argue parallelogram method general simple model human cognitive process form analogy ia embedding embedding historical semantic embedding useful tool study mean change time compute multiple embed space text write particular time period example fig show visualization change meaning english word century compute build separate ding space decade historical corpora like google gram lin corpus historical american english davy chapter dynamic social representation word mean figure dimensional visualization semantic change english sgn vector section visualization algorithm gay shift mean cheerful frolicsome refer homosexuality century broadcast refer cast seed rise television radio meaning shift transmit signal awful undergo process pejoration shift mean awe mean terrible appalling adverbial actually eag shift objective statement world sorry car actually break subjective statement believe actually indicate surprise disbelief computational linguistic study number recent work analyze semantic change computational method use latent semantic analysis analyze word meaning broaden narrow time man historical case study semantic change perform similar set scale case study temporal topic model information base embedding find semantic change uncover method reasonable agreement human judgment word embed method detect linguistic change point finally historical occurrence test synonym tend change similar way figure sne visualization semantic change word english vector modern sense word grey context word put recent modern time point embed space early point put early historical embed space visualization change word gayfrom meaning relate cheerful frolicsome refer homosexuality development modern transmission sense broadcast original sense sow seed pejoration word awful shift mean awe mean terrible appalling hamilton bias embedding addition ability learn word meaning text embedding alas reproduce implicit bias stereotype latent text prior section show embedding roughly model relational ity queen close word king man woman imply analogy man queen embed analogy exhibit gender stereotype example bolukbasi ﬁnd close occupation computer programmer man woman embedding train news text homemaker embedding similarly suggest analogy father doctor mother nurse result crawford blodgett allocational harm system allo allocational harm cat resource job credit unfairly different group example algorithm use embedding search hire potential programmer doctor incorrectly downweight document woman name turn embedding reﬂect statistic input amplify bias gendere term gendere embed space theybias ampliﬁcation input text statistic zhao ethayarajh jia bias exaggerated actual labor employment statistic garg embedding encode implicit association property human reasoning implicit association test greenwald measure chapter mbedding ple association concept like ﬂower insect attribute like pleasantness unpleasantness measure difference latency label word method people united states show associate african american name unpleasant word european american name male name mathematic female name art old people name ant word greenwald nosek nosek caliskan replicate ﬁnding implicit association glove vector cosine similarity instead human latency example african american name like leroy shaniqua high glove cosine unpleasant word european american name brad greg courtney high cosine pleasant word problem embedding example sentational harm crawford blodgett harm cause byrepresentational harm system demean ignore social group embed aware gorithm use word sentiment exacerbate bias african americans recent research focus way try remove kind bias example develop transformation embed space remove der stereotype preserve deﬁnitional gender bolukbasi zhao change training procedure zhao sort debiase reduce bias embedding eliminate debiase gonen goldberg remain open problem historical embedding measure bias past garg embedding historical text measure association tween embedding occupation embedding name tie gender example relative cosine similarity woman name versus man occupation word like librarian carpenter century find cosine correlate empirical historical percentage woman ethnic group occupation historical embedding cat old survey ethnic stereotype tendency experimental participant associate adjective like industrious superstitious chinese ethnicity correlate cosine chinese name adjective embedding train text able document historical gender bias fact embedding adjective relate competence smart wise thoughtful resourceful high cosine male male word show bias slowly decrease return later chapter question role bias natural language processing evaluate vector model important evaluation metric vector model extrinsic evaluation task vector nlp task see improve mance model speak human associate ﬂower pleasantness insect ness instruct push green button ﬂower daisy iris lilac pleasant word love laughter pleasure red button insect ﬂea spider mosquito unpleasant word abuse hatred ugly fast incongruous condition push red button ﬂower unpleasant word green button insect pleasant ummary nonetheless useful intrinsic evaluation common metric test performance similarity compute correlation algorithm word similarity score word similarity rating assign human finkelstein commonly set rating noun pair example plane car average score hill complex dataset quantiﬁes similarity cup mug relatedness cup coffee include concrete abstract adjective noun verb pair toefl dataset set question consist target word additional word choice task choose correct synonym example levy close meaning impose believe request correlate landauer dumais dataset present word context slightly realistic intrinsic similarity task include context stanford contextual word similarity scw dataset huang word context wic dataset pilehvar camacho collado offer rich evaluation scenario scw give human judgment pair word sentential context wic give target word sentential context different sense appendix semantic textual similarity task agirre agirre evaluate performance sentence level similarity algorithm consist set pair sentence pair human label similarity score task evaluation analogy task discuss page system solve problem form give have ﬁnd littman number set tuple create task mikolov mikolov gladkova cover morphology city child lexicographic tion leg teapot encyclopedia relation beijing ireland draw task dataset different relation gen embed algorithm suffer inherent variability example randomness initialization random negative sampling algorithm like produce different result dataset dividual document collection strongly impact result embedding tian hellrich hahn antoniak mimno bedding study word association particular corpora good practice train multiple embedding bootstrap sampling document average result antoniak mimno summary vector semantic word model vector point high dimensional space call embedding chapter focus static ding word map ﬁxed embed vector semantic model fall class sparse anddense sparse model dimension correspond word vocabulary vand cell function occurrence count word context orterm term trix row target word vocabulary column context term chapter mbedding dense vector model typically dimensionality gorithm like skip gram popular way compute dense embedding skip gram train logistic regression classiﬁer compute probability word likely occur nearby text probability compute dot product embedding word skip gram use stochastic gradient descent train classiﬁer learn embedding high dot product embedding word occur nearby low dot product noise word important embed algorithm include glove method base ratio word occurrence probability sparse dense vector word document similarity compute function dot product vector cosine vector normalize dot product popular metric historical note idea vector semantic arise research distinct ﬁeld linguistic psychology computer science contribute fundamental aspect model idea meaning relate distribution word context widespread linguistic theory distributionalist like zellig harris martin joos firth semiotician like thomas sebeok joos linguist meaning morpheme deﬁnition set conditional probability occurrence context morpheme idea meaning word model point dimensional semantic space come psychologist like charles osgood study people respond meaning word assign ue scale like happy sad orhard soft osgood propose meaning word general model point multidimensional euclidean space similarity meaning word model distance point space ﬁnal intellectual source early ﬁeld call mechanical indexing know information retrieval knownmechanical indexing vector space model information retrieval salton sparck jones researcher demonstrate new way deﬁne meaning word term vector switzer reﬁned method word similarity base sure statistical association word like mutual information giuliano idf sparck jones show meaning document represent vector space word time cordier show factor analysis word association probability form dense vector representation word philosophical underpinning distributional way thinking come late writing philosopher wittgenstein skeptical possibility build completely formal theory mean deﬁnition word wittgenstein suggest instead meaning word use language wittgenstein instead logical guage deﬁne word draw denotation truth value note idea deﬁne word people speak derstande day day interaction preﬁgure movement embody experiential model linguistic nlp glenberg robertson lake murphy bisk bender koll distantly related idea deﬁne word vector discrete ture root far descarte leibniz wierzbicka wierzbicka middle century begin work hjelmslev hjelmslev originally ﬂeshe early model generative grammar katz fodor idea arise represent ing semantic feature symbol represent sort primitive feature example word like hen rooster orchick common describe chicken different age sex representable chicken adult rooster female chicken adult dimension vector model meaning deﬁne word abstractly related idea small ﬁxed number hand build dimension nonetheless attempt certain dimension bed model contribute speciﬁc compositional aspect meaning like early semantic feature use dense vector model word meaning term ding grow latent semantic indexing lsi model deerwester recast lsa latent semantic analysis deerwester lsa singular value decomposition svd apply term document matrix svd cell weight log frequency normalize entropy ﬁrst dimension lsa embedding singular value decomposition svd method ﬁnde important dimension data set sion datum vary lsa quickly widely apply cognitive model landauer dumais task like spell checking jones martin language modeling bellegarda coccaro rafsky bellegarda morphology induction schone jurafsky schone jurafsky multiword expression mwe schone sky essay grading rehder related model taneously develop apply word sense disambiguation sch lsa lead early use embedding represent word tic classiﬁer logistic regression document router sch idea svd term term matrix term document matrix model meaning nlp propose soon lsa sch apply low rank dimensional embedding produce svd task word sense disambiguation analyze result semantic space suggest possible technique like drop high order dimension sch number alternative matrix model follow early svd work include probabilistic latent semantic indexing plsi hofmann latent dirichlet allocation lda blei non negative matrix tion nmf lee seung lsa community ﬁrst word embed landauer variant mathematical meaning mapping space mathematical structure lsa word embed describe mapping space sparse count vector latent space chapter mbedding svd dense vector word originally mean mapping space metonymically shift mean result dense vector latent space sense currently use word decade bengio bengio show neural language model develop embedding task word prediction collobert weston collobert weston collobert demonstrate embedding represent word meaning number nlp task turian compare value different kind embedding different nlp task mikolov show recurrent neural net language model idea simplify hidden layer neural net language model create gram cbow algorithm propose mikolov negative sampling training algorithm propose mikolov numerous survey static embedding parameterization bullinaria levy bullinaria levy lapesa evert kiela clark levy man chapter deep understanding role vector information retrieval include compare query ment detail idf issue scale large dataset kim clear comprehensive tutorial cruse useful introductory linguistic text lexical semantic exerciseschapter network character behave complicated manner number unit large alan ture intelligent machine page neural network fundamental computational tool language ing old call neural origin lie mcculloch pitts neuron mcculloch pitt simpliﬁed model biological neuron kind compute element describe term propositional logic modern use language processing long draw early biological inspiration instead modern neural network network small compute unit take vector input value produce single output value chapter introduce neural net apply classiﬁcation architecture introduce call feedforward network computation proceed feedforward atively layer unit use modern neural net call deep learning modern network deep layer deep learning neural network share mathematic logistic regression neural network powerful classiﬁer logistic regression minimal neural network technically single hidden layer show learn function neural net classiﬁer different logistic regression way logistic regression apply regression classiﬁer different task develop rich kind feature template base domain knowledge work neural network common avoid use rich derive feature instead build neural network raw token input learn induce feature process learn classify see example kind representation learning embedding chapter lot example start study deep transformer network net deep particularly good representation learning reason deep neural net right tool task offer sufﬁcient datum learn feature automatically chapter introduce feedforward network classiﬁer ﬁrst hand build feature embedding study chapter subsequent chapter introduce kind neural model importantly transformer andattention chapter recurrent neural network chapter convolutional neural network chapter chapter introduce paradigm neural large language chapter eural network unit building block neural network single computational unit unit take set real value number input perform computation produce output heart neural unit take weighted sum input tional term sum call bias term give set input unit bias term set correspond weight bias weighted sum zcan represent iwixi convenient express weighted sum vector notation recall linear algebra vector heart list array number vector talk zin term weight vector scalar bias input vector replace sum convenient dot product deﬁne zi real value number finally instead linear function output neural unit apply non linear function ftoz refer output function theactivation value unit model single unit activation activation node fact ﬁnal output network generally cally value yis deﬁne discuss popular non linear function fbelow sigmoid tanh rectiﬁed linear unit relu pedagogically convenient start sigmoid function see chapter sigmoid sigmoid show fig number advantage map output range useful squash outlier differentiable see section handy learn figure sigmoid function take real value map range nearly linear outli value squashed substituting give output neural unit nit fig show ﬁnal schematic basic neural unit example unit take input value compute weighted sum multiply value weight respectively add bias term pass result sum sigmoid function result number figure neural unit take input bias bthat represent weight input clamp produce output include convenient intermediate variable output summation output sigmoid case output unit yis deep network reserve yto mean ﬁnal output entire network leave aas activation individual node let walk example intuition let suppose unit follow weight vector bias unit follow input vector result output ywould practice sigmoid commonly activation function function similar well tanh function show fig tanh tanh variant sigmoid range simple activation function commonly tiﬁe linear unit call relu show fig relu zi positive activation function different property useful ent language application network architecture example tanh function nice property smoothly differentiable mapping outlier value mean rectiﬁer function hand nice property chapter eural network figure tanh relu activation function result close linear sigmoid tanh function high value zresult value ythat saturate extremely close saturate derivative close zero derivative cause problem learn section train network propagate error signal ward multiplying gradient partial derivative layer network gradient cause error signal small small small training problem call vanish gradient gradient rectiﬁer problem derivative relu high value close xor problem early history neural network realize power neural work real neuron inspire come combine unit large network clever demonstration need multi layer network proof minsky papert single neural unit compute simple function input consider task computing elementary logical function input like xor reminder truth table function xor example ﬁrst show perceptron simple neural perceptron unit binary output simple step function non linear activation function output yof perceptron compute follow weight input bias bas hexor problem easy build perceptron compute logical function binary input fig show necessary weight figure weights wand bias bfor perceptron compute logical function input show bias special node value multiply bias weight logical weight bias weight logical weight bias weight weight bias inﬁnite number possible set weight bias implement function turn possible build perceptron compute logical xor worth spend moment try intuition important result rely understand tron linear classiﬁer dimensional input perceptron equation equation line put standard linear format line act decision boundary dimensional space output assign alldecision boundary input lie line output input point lie line input decision boundary hyperplane instead line idea separate space category fig show possible logical input line draw possible set parameter classiﬁer notice simply way draw line separate positive case xor negative case xor linearly separablelinearly separable function course draw boundary curve function single line solution neural network xor function calculate single perceptron culate layered network perceptron unit network simple perceptron let compute xor layer relu base unit follow goodfellow fig show ﬁgure input process layer neural unit middle layer call unit output layer call unit set weight bias show allow network correctly compute xor function let walk happen input multiply input value appropriate weight sum add bias vector apply rectiﬁed linear transformation output hlayer multiply weight sum add bias case result value reader work computation remain possible input pair result yvalue input chapter eural network xor figure function xor represent input axis input axis fill circle represent perceptron output white circle perceptron output way draw line correctly separate category xor figure style russell norvig figure xor solution goodfellow relu unit layer call hidden layer number arrow represent weight wfor unit represent bias bas weight unit clamp bias weight unit gray instructive look intermediate result output hide node show previous paragraph hvector input fig show value hlayer input notice hide representation input point case xor output merge single point merger make easy linearly separate positive negative case xor word view hidden layer network form representation input example stipulate weight fig real example weight neural network learn automatically error agation algorithm introduce section mean hidden layer learn form useful representation intuition neural network matically learn useful representation input key advantage return later eedforward neural network original new linearly separable space figure hidden layer form new representation input show representation hidden layer compare original input representation xin notice input point collapse input point make possible linearly separate positive negative case xor goodfellow feedforward neural network let walk slightly formal presentation simple kind neural network feedforward network feedforward network multilayerfeedforward network network unit connect cycle output unit layer pass unit high layer output pass low layer chapter introduce network cycle call recurrent neural network historical reason multilayer network especially feedforward network call multi layer perceptron ormlp technical misnomer multi layer perceptron mlp unit modern multilayer network perceptron perceptron simple step function activation function modern network unit kind non linearity like relus sigmoid point stick simple feedforward network kind node input unit hidden unit output unit fig show picture input layer xis vector simple scalar value see fig core neural network hidden layer hforme hide unit hide layer neural unit describe section take weighted sum input apply non linearity standard architecture layer isfully connected mean unit layer take input output fully connect unit previous layer link pair unit adjacent layer hide unit sum input unit recall single hide unit parameter weight vector bias represent parameter entire hide layer combine weight vector bias unit iinto single weight matrix wand single bias vector bfor layer fig element wjiof weight matrix wrepresent weight connection ith input unit xito jth hidden unit advantage single matrix wfor weight entire layer hide layer computation feedforward network chapter eural network input layerhidden layeroutput figure simple layer feedforward network hide layer output layer input layer input layer usually count enumerate layer efﬁciently simple matrix operation fact computation step multiply weight matrix input vector add bias vector apply activation function sigmoid tanh relu activation function deﬁne output hidden layer vector following ple use sigmoid function sas activation function notice apply sfunction vector apply scalar allow activation function apply vector element wise let introduce constant represent dimensionality vector matrix refer input layer layer network number input xis vector real number dimension formally column vector dimensionality let hidden layer layer output layer layer hide layer dimensionality hide unit different bias value weight matrix wha dimensionality moment convince matrix multiplication compute value see section result value esis form representation input role output layer new representation hand compute ﬁnal output output value number case goal network sort classiﬁcation decision focus case classiﬁcation binary task like sentiment classiﬁcation gle output node scalar value yis probability positive versus negative sentiment multinomial classiﬁcation assign speech tag output node potential speech output value probability speech value output node sum output layer vector ythat give probability distribution output eedforward neural network let happen like hidden layer output layer weight matrix let model include bias vector bin output layer simplify eliminate bias vector example weight matrix multiply input vector produce intermediate output node weight matrix uha dimensionality element ji weight unit jin hidden layer unit iin output layer output classiﬁer vector real value number need classiﬁcation vector probability convenient function normalize vector real value mean normalize convert vector encode probability distribution number lie sum softmax function see page softmax chapter generally vector zof dimensionality softmax deﬁne softmax example give vector softmax function normalize probability distribution show round softmax recall softmax create probability distribution vector real value number compute sum weight time feature multinomial version logistic regression chapter mean think neural network classiﬁer hide layer build vector hwhich hidden layer representation input run standard multinomial logistic regression feature network develop contrast chapter feature mainly design hand feature template neural network like multinomial logistic regression layer deep neural network like layer layer gistic regression classiﬁer intermediate layer have possible activation function tanh relu sigmoid instead sigmoid continue use sfor convenience mean activation function form feature feature template prior layer network induce feature representation ﬁnal equation feedforward network single hide layer take input vector output probability distribution ize weight matrix wanduand bias vector softmax remember shape variable output vector network layer network traditionally count input layer numbering layer count output layer terminology logistic regression layer chapter eural network detail feedforward network let set notation easy talk deep network depth use superscript square bracket mean layer ber start input layer mean weight matrix ﬁrst hide layer mean bias vector ﬁrst hide layer mean number unit layer use stand activation function tend relu tanh intermediate layer softmax output layer use mean output layer mean combination previous layer output weight bias layer input refer input xmore generally represent layer net follow note notation equation computation layer algorithm compute forward step layer feedforward network give input vector simply useful ﬁnal set activation right ﬁnal softmax layer generally unnormalized value ﬁnal vector vector score right ﬁnal softmax logit logit need non linear activation function reason use linear activation function layer neural network result network exactly equivalent single layer network let true imagine ﬁrst layer network purely linear layer rewrite function network compute generalize number layer non linear activation function multilayer network notational variant single layer network different set weight lose representational power multilayer eedforward network nlp lassification replace bias unit describe network use slightly simpliﬁed notation represent exactly function refer explicit bias node instead add dummy node layer value layer input layer dummy node layer dummy node associated weight weight represent bias value example instead equation like use instead vector xhaving value new dummy value instead compute hjas follow instead use value fig show visualization figure replace bias node show continue show bias bwhen learning algorithm section go forward book ﬁgure equation use simpliﬁed notation explicit bias term feedforward network nlp classiﬁcation let apply feedforward network nlp classiﬁcation task practice simple feedforward network way text classiﬁcation real cation use sophisticated architecture like bert chapter eural network chapter nonetheless see feedforward network text classiﬁer let introduce key idea play role rest book e idea embed matrix representation pooling representation learning introduce idea let start classiﬁer make minimal change sentiment classiﬁer see chapter like hand build feature pass classiﬁer produce class probability difference use neural network instead logistic regression classiﬁer neural net classiﬁer hand build feature let begin simple layer sentiment classiﬁer take logistic sion classiﬁer chapter correspond layer network add hidden layer input element xican scalar feature like fig lexicon word total dfeature output layer ˆycould node positive negative node positive negative neutral case estimate probability positive sentiment probability negative probability neutral sulting equation see layer network continue use sto stand non linearity sigmoid relu xii hand design feature softmax fig show sketch architecture mention early add hide layer logistic regression classiﬁer allow network represent non linear interaction feature well sentiment classiﬁer layeroutput layer lexiconword figure feedforward network sentiment analysis traditional hand build feature input mbedding input neural net classifier vectorizing parallelizing inference show classify single example practice want efﬁciently classify entire test set mexample vectorize process see logistic regression instead loop example use matrix multiplication entire computation entire test set pack input feature vector input xinto single input matrix row row vector consist feature input example vector dimensionality input feature vector xwill matrix shape model input row vector column vector need slightly modify xis shape shape reorder multiply xandwand transpose wso correctly multiply yield matrix hof shape bias vector bfrom shape replicate matrix shape need similarly reorder step transpose finally output matrix ˆywill shape erally dois number output class row iof output matrix ˆyconsisting output vector ﬁnal equation compute output class distribution entire test set softmax book ordering like important aware shape weight matrix participate give equation embedding input neural net classiﬁer hand build feature traditional way design classiﬁer tion neural network nlp use hand build human engineer feature input instead draw deep learning ability learn feature datum represent token embedding section represent token static glove embedding see compute chapter static embed mean token represent ﬁxed vector train big dictionary want refer token grab embed dictionary apply neural model task language modeling chapter situation complex use ful kind embed call contextual embed contextual embedding different time word occur different context furthermore network learn embedding task word prediction let explore text classiﬁcation domain static embedding feature instead hand design feature let focus inference stage keep original order product instead input matrix xrepresent input column vector instead row vector make shape represent input row vector convenient common neural network chapter eural network learn embedding input token ding vector dimension dthat represent input token dictionary static embedding store embedding embed matrixembedde matrix row embed matrix represent token vocabulary row vector dimensionality eha row ken vocabulary eha shape embed matrix eplay role embedding input neural nlp system include transformer base large language model introduce chapter give input token string like dessert great ﬁrst convert token vocabulary index create ﬁrst tokenize input bpe sentencepiece representation dessert great use indexing select correspond row row row row way think select token embedding embed matrix represent input token hot vector shape dimension word vocabulary recall hot vector hot vector element element dimension word index vocabulary value word dessert index vocabulary show multiplying hot vector non zero element simply select relevant row vector word result embed word depict fig figure select embed vector word multiply embed matrix ewith hot vector index extend idea represent entire input token sequence matrix hot vector ninput position show fig figure select embed matrix input sequence token ids wby tiplye hot matrix correspond wby embed matrix need classify input represent dow ntoken single class like positive negative common way pass embedding classiﬁer nation andpooling input shape reshape mbedding input neural net classifier byconcatenate input vector long vector shape pass input classiﬁer let decision give lot information cost pretty large network second pool thenembedding single embed pass single pool pool embed classiﬁer pooling give information present original embedding advantage small ﬁcient especially useful task care original word order let example pool sentiment task concatenation language modeling task pool input embedding sentiment let begin see pooling work sentiment classiﬁcation task intuition pooling sentiment exact position input word like great ﬁrst word second word important identity word pool function way turn set embedding single embedding example text ninput word token want turn thenrow embedding dimensionality single embed dimensionality way pool simple mean pooling take mean mean pooling sum embedding divide nnx equation classiﬁer assume mean pooling softmax architecture sketch fig shape relevant matrix option pooling like max pooling case max pool dimension element wise max input element wise max set nvector new vector kth element max kth element nvector concatenate input embedding language modeling sentiment sis see generate output vector probability class positive negative neutral give input window ninput token ﬁrst pool token embedding single embed vector let consider language modeling predict upcoming word prior word task give window ninput token task predict token follow window sketch simple feedforward neural language model draw algorithm ﬁrst introduce bengio feedforward language model introduce important concept large language modeling return chapter chapter neural language model advantage gram language el chapter neural language model handle long history chapter eural network dessert layeroutput input input layer embeddingsone hot matrixshare wordsoutput probabilitiesweightsweightssoftmax pool embed figure feedforward network sentiment analysis pool embedding input word timestep network compute dimensional embedding context word multiply hot vector embed matrix pool result nembedding single embed represent context window layer generalize well context similar word far accurate prediction hand neural net language model slow plex need vast amount energy train interpretable gram model small task gram language model right tool feedforward neural language model feedforward network take input time representation number previous word etc output probability distribution possible word like gram feedforward neural approximate probability word give entire prior context approximating base previous word following example use gram example neural net estimate probability neural language model represent word prior context ding word identity gram language model embedding allow neural language model generalize well unseen datum example suppose see sentence training sure cat get mbedding input neural net classifier see word get feed word dog test set preﬁx forgot sure dog get word gram language model predict fed cat get dog get neural know cat dog similar embedding able generalize cat context assign high probability fed see dog layer layer houtput layer ysoftmax input layerone hot sharedacross figure forward inference feedforward neural language model timestep tthe network compute dimensional embedding context token multiply hot vector embed matrix concatenate embed embed eis multiply weight matrix wand activation function apply element wise produce hidden layer multiply weight matrix softmax layer predict output node ithe probability word wtwill vocabulary word context window size nas page practice language modeling require long context prediction task require output vector express jvjprobabilitie probability value possible token vocabulary token output vector task language modeling long difference language modeling instead pool embedding ninput token create single ding concatenate input long input vector predict token help know precede token order fig show language modeling task sketch short context window page embed vector concatenate produce embed layer multiply weight matrix wto duce hidden layer weight matrix uto produce output layer softmax give probability distribution word example value output node probability word wtbee vocabulary word index word ﬁsh example equation simple feedforward neural language model chapter eural network size give hot input vector input context word softmax note use semicolon mean concatenation vector form embed layer eby concatenate embedding context vector return idea neural network language modeling chapter chapter introduce transformer language model training neural net feedforward neural net instance supervise machine learning know correct output yfor observation system produce system estimate true goal training procedure learn parameter layer ithat yfor training observation close possible true general draw method introduce chapter logistic regression reader comfortable chapter proceeding explore algorithm simple generic network network design sentiment language modeling need loss function model distance system output gold output common use loss function logistic regression cross entropy loss second ﬁnd parameter minimize loss function use gradient descent optimization algorithm introduce chapter gradient descent require know gradient loss function vector contain partial derivative loss function respect parameter logistic regression observation directly compute derivative loss function respect individual worb neural network million parameter layer hard compute partial derivative weight layer loss attach later layer partial loss intermediate layer answer algorithm call error backpropagation backward differentiation loss function thecross entropy loss neural network see forcross entropy loss logistic regression neural network binary classiﬁer sigmoid ﬁnal layer loss function logistic regression loss see network classify class loss function exactly loss multinomial regression see chapter rain neural net page let brieﬂy summarize explanation convenience class need represent yandˆyas vector let assume hard classiﬁcation class correct true label yis vector kelement correspond class correct class element ybeing recall vector like value equal rest call hot vector classiﬁer produce estimate vector kelement element ˆykof represent estimate probability loss function single example xis negative sum log output class weight probability simplify equation let ﬁrst rewrite equation tion evaluate condition bracket true erwise make obvious term sum term correspond true class word cross entropy loss simply negative log output bility correspond correct class negative log likelihood loss negative log likelihood loss cis correct class plug softmax formula kthe number class cis correct class let think negative log probability loss function perfect siﬁer assign correct class iprobability incorrect class ability mean high close well classiﬁer close bad classiﬁer negative log ability beautiful loss metric go negative log loss inﬁnity negative log inﬁnite loss loss function insure probability correct answer maximize probability incorrect answer minimize sum increase probability correct answer come expense incorrect answer number kof class output vector ycan small large task way sentiment class positive negative neutral task decide speech word noun verb adjective etc kis set possible part speech tagset tagset deﬁne chapter task language modeling classiﬁer try predict word set class set word chapter eural network compute gradient compute gradient loss function compute gradient require partial derivative loss function respect parameter network weight layer sigmoid output logistic regression simply use derivative loss logistic regression derive section network weight layer softmax output multinomial logistic regression use derivative softmax loss show particular weight wkand input derivative correct update weight layer deep network compute gradient weight complex compute derivative respect weight parameter appear way early layer network loss compute end network solution compute gradient algorithm call error agation orbackprop rumelhart backprop invent spe error propagation cially neural network turn general procedure call backward differentiation depend notion computation graph let work subsection computation graph computation graph representation process compute mathematical expression computation break separate operation model node graph consider compute function component addition multiplication operation explicit add name dande intermediate output result series computation represent graph node operation recte edge show output operation input fig simple use computation graph compute value function give input ﬁgure assume input show result forward pass compute forward pass computation graph apply rain neural net operation leave right pass output computation input node passabc figure computation graph function value input node show forward pass computation backward differentiation computation graph importance computation graph come backward pass compute derivative need weight update example goal compute derivative output function lwith respect input variable small change aaffect backwards differentiation make use chain rule calculus let chain rule mind suppose compute derivative composite function derivative derivative respect derivative respect chain rule extend function compute derivative composite function derivative intuition backward differentiation pass gradient ﬁnal node node graph fig show backward computation node node take upstream gradient pass parent node right input compute local gradient gradient output respect input use chain rule multiply compute downstream gradient pass early node let compute derivative need computation graph directly compute need use chain rule chapter eural network gradientdownstream gradientlocal gradient figure node like ehere take upstream gradient multiply local gradient gradient output respect input use chain rule compute downstream gradient pass prior node node multiple local gradient multiple input require ﬁve intermediate follow make use fact derivative sum sum derivative backward pass compute partial edge graph right left chain rule begin compute downstream gradient node node multiply upstream local gradient gradient output respect output send node annotate graph way input variable forward pass conveniently compute value forward intermediate variable need like dande compute derivative fig show backward pass figure computation graph function show backward pass tion rain neural net backward differentiation neural network course computation graph real neural network complex fig show sample computation graph layer neural network assume binary classiﬁcation sigmoid output unit simplicity function computation graph compute backward pass need compute loss loss function binary sigmoid output output rephrase figure sample computation graph simple layer neural net hide layer input unit hide unit adjust notation bit avoid long equation node mention function compute result variable right node multiply node mean value compute sum node feed product bias term weight need update need know partial derivative loss function show teal order backward pass need know derivative function graph see section derivative sigmoid chapter eural network need derivative activation function derivative tanh derivative relu drelu start computation compute derivative loss function lwith respect leave rest computation exercise reader chain rule let ﬁrst take derivative repeat derivative sigmoid finally use chain rule continue backward computation gradient pass dient product node teal node leave exercise reader detail learn optimization neural network non convex optimization problem plex logistic regression reason good practice successful learning derivative actually undeﬁned point convention treat ummary logistic regression initialize gradient descent weight bias have value neural network contrast need initialize weight small random number helpful normalize input value mean unit variance form regularization prevent overﬁtte important dropout randomly drop unit connection dropout network training hinton srivastava iteration training update parameter mini batch mini batch gradient descent repeatedly choose probability pand unit replace output zero probability renormalize rest output layer tuning hyperparameter important parameter neural hyperparameter work weight wand bias learn gradient descent hyperparameter thing choose algorithm designer optimal ue tune devset gradient descent learn training set hyperparameter include learn rate mini batch size model architecture number layer number hide node layer choice activation function regularize gradient descent architectural variant adam kingma finally modern neural network build computation graph malism easy natural gradient computation parallelization vector base gpu graphic processing unit pytorch paszke tensorflow abadi popular interested reader consult neural network textbook detail tion end chapter summary neural network build neural unit originally inspire biological neuron simply abstract computational device neural unit multiplie input value weight vector add bias apply non linear activation function like sigmoid tanh rectiﬁed linear unit fully connect feedforward network unit layer iis connect unit layer cycle power neural network come ability early layer learn representation utilize later layer network neural network train optimization algorithm like gradient scent backpropagation backward differentiation computation graph compute gradient loss function network language model use neural network probabilistic classiﬁer compute probability word give previous nword neural language model use pretraine embedding learn ding scratch process language chapter eural network historical note origin neural network lie mcculloch pitts neuron loch pitt simpliﬁed model biological neuron kind puting element describe term propositional logic late early number lab include frank rosenblatt cornell bernard widrow stanford develop research neural network phase see development perceptron rosenblatt transformation threshold bias notation use widrow hoff ﬁeld neural network decline show single perceptron unit unable model function simple xor minsky papert small work continue decade major revival ﬁeld come practical tool build deep network like error backpropagation widespread rumelhart wide variety neural network relate ture develop particularly application psychology cognitive ence rumelhart mcclelland mcclelland elman rumelhart mcclelland elman term connectionist connectionist lel distribute processing feldman ballard smolensky principle technique develop period dational modern work include idea distribute representation hinton recurrent network elman use tensor compositionality smolensky large neural network begin apply practical guage processing task like handwriting recognition lecun speech recognition morgan bourlard early improvement computer hardware advance optimization training technique possible train large deep network lead modern term deep learning hinton bengio cover related history chapter chapter number excellent book neural network include goodfellow nielsen language model know time believe know know agatha christie move finger literature fantastic abound inanimate object magically endow gift speech ovid statue pygmalion mary shelley story frankenstein continually reinvent story create have chat legend ﬁnishe sculpture moses michelangelo think lifelike tap knee command speak surprising language mark humanity sentience conversation fundamental arena language ﬁrst kind guage learn child kind engage constantly teach learn dere lunch talk family friend chapter introduce large language model orllm computational agent teract conversationally people fact llm design interaction people strong implication design use implication clear computational system year ago eliza weizenbaum eliza design simulate rogerian psychologist illustrate number important issue chatbot example people deeply emotionally involved conduct personal tion extent ask weizenbaum leave room type issue emotional engagement privacy mean need think carefully deploy language model consider effect people interact chapter begin introduce computational principle llm discuss implementation transformer architecture following chapter central new idea make llm possible idea pretraine let begin think idea learn text basic way llm train know ﬂuent speaker language bring enormous edge bear comprehension production knowledge embody form obviously vocabulary rich representation word meaning usage make vocabulary useful lens explore acquisition knowledge text people machine estimate size adult vocabulary vary widely language example estimate vocabulary size young adult speaker american english range depend resource chapter arge language model estimate deﬁnition mean know word ple consequence fact child learn word day single day arrive observe vocabulary level time year age empirical estimate vocabulary growth late tary high school consistent rate child achieve rate vocabulary growth research suggest bulk knowledge sition happen product reading reading process rich contextual processing learn word time isolation fact point learn rate vocabulary growth exceed rate new word appear learner suggest time read word strengthen understanding word associate fact consistent distributional hypothesis chapter propose aspect meaning learn solely text counter life base complex association word word occur word word occur tional hypothesis suggest acquire remarkable amount edge text knowledge bring bear long initial acquisition course ground real world interaction modality help build powerful model text remarkably useful modern nlp revolution possible large language model learn knowledge language context world simply teach predict word base context large corpus text chapter formalize idea pretraining learn knowledge language world iteratively pretraine predict token vast amount text result pretraine model large language model large language model exhibit remarkable performance natural language task knowledge learn pretraine language model learn word prediction consider example kind knowledge think model pick ing predict word ﬁll underbar correct answer show blue think example read ahead paragraph rose dahlia peony surround ﬂower room big enormous square root author room virginia woolf professor say ﬁrst sentence model learn ontological fact like rose dahlia peony kind ﬂower second model learn enormous mean scale big scale sentence system learn math sentence fact world historical author finally sentence model expose sentence repeatedly learn associate professor male pronoun kind association cause model act unfairly different people large language model see chapter language model simply computational system predict word previous word give context preﬁx word language model assign probability distribution possible word fig sketch idea course see language model see gram language el chapter brieﬂy touch feedforward network apply input transformer figure large language model neural network take input context preﬁx output distribution possible word modeling chapter large language model large version example chapter introduce bigram trigram language el predict word previous word handful word contrast large language model predict word give context thousand ten thousand word fundamental intuition language model model predict text assign distribution follow word generate text bysampling distribution recall chapter sampling mean choose word distribution transformer transformer figure turn predictive model give probability distribution word generative model repeatedly sample distribution result left right call autoregressive language model token generate get add context preﬁx generate token fig show example fig language model give text preﬁx generate possible completion model select wordall add context use update context new predictive distribution select distribution generate notice model conditioning prime context subsequently generate output kind set iteratively predict generate word leave chapter arge language model right early word call causal orautoregressive language el introduce alternative non autoregressive model like bert mask language model predict word information left right chapter idea computational model generate text code speech image constitute important new area call generative apply generative llm generate text vastly broaden scope nlp historically focus algorithm parse understand text erate rest chapter nlp task model word prediction large language model think right way motivate introduce idea prompt language model introduce speciﬁc algorithm generate text language model like greedy decode andsampling introduce detail pretraining way language model self train iteratively teach guess word text prior word sketch stage language model training instruction tuning call supervised ﬁnetuning sft alignment concept return chapter evaluate model let begin talk different kind language model architecture language model architecture sketch left right autoregressive language model language model architecture deﬁne chapter actually common architecture architecture encoder decoder encoder decoder fig give schematic picture wwwwwwwwwwwwwwwwwwwwwencoderdecoderencoder decoder figure architecture language model decoder encoder encoder decoder arrow sketch information ﬂow architecture decoder token input generate token output encoder token input produce encoding vector representation token output encoder decoder token input generate series tokens output thedecoder architecture introduce take input series decoder token iteratively generate output token time decoder architecture create large language model like gpt claude llama mistral information ﬂow decoder go left right mean onditional generation text theintuition predict word prior word decoder generative model mean give input token generate novel output token discuss decoder rest chapter chapter encoder take input sequence token output vector encoder sentation token encoder usually mask language model mean train mask word learn predict look round word side mask language model like bert roberta bert family encoder model encoder model generative model generate text instead encoder model create classiﬁer example input text output label example sentiment topic class ﬁnetune train supervised datum introduce encoder model chapter theencoder decoder take input sequence token output decoder token make different decoder model decoder loose relationship input token output token map different kind token encoder decoder output token different token set long short input token example encoder decoder architecture machine translation input token language output token language probably different length input encoder decoder architecture speech recognition input token represent speech output token represent text introduce encoder decoder architecture machine translation chapter speech recognition chapter architecture build kind neural network widely network type today transformer introduce chapter transformer input token process column layer layer compose series different kind subnetwork chapter introduce early architecture relevant lstm kind recurrent neural network recent architecture state space model focus transformer book purpose chapter architecture agnostic treat network implement coder black box input black box sequence token output box distribution token sample scribe mechanism learn decode network agnostic manner conditional generation text intuition fundamental intuition underlie language model want language model conditional generation text weconditional generation mean decoder language model discuss chapter conditional generation task generate text condition input piece text llm input piece text prompt llm continue generate text token token condition prompt subsequently generate token generate model ﬁrst computing probability token wifrom prior context sample distribution generate chapter arge language model talk future section detail section goal establish intuition simply compute probability token help llm sort different language relate task imagine want classiﬁcation task like sentiment analysis treat conditional generation give language model context like sentiment sentence like jackie chan compare conditional probability following token positive follow token negative high sketch fig compare probability sentiment sentence like jackie chan sentiment sentence like jackie chan token positive probable sentiment transformer decoder sentiment sentence like jackie chan prob figure computing probability token positive andnegative occurring preﬁx tence positive token negative probable sentiment negative intuition help perform task like question answer system give question textual answer cast task question answer token prediction give language model question token like suggest answer come like write book origin specie ask language model compute probability distribution possible token give preﬁx compute follow probability write book origin specie look token whave high probability fig suggest expect charles likely choose charle add preﬁx compute probability token preﬁx write book origin specie charles darwin probable token select prompt simple idea contextual generation powerful powerful language model specially train answer question rompte charles transformer decoder write book origin specie figure answer question compute probability token preﬁx state question example correct token charles high probability follow instruction extra training call instruction tuning tuning base language model train predict word continue train special dataset instruction appropriate response datum set example question answer command response example carry conversation discuss detail instruction tuning chapter language model instruction tuned good follow instruction answer question carry conversation prompt aprompt text string user issue language model prompt model useful prompt user prompt string pass language model iteratively generate token condition prompt process ﬁnde effective prompt task know prompt engineering engineering suggest introduce conditional generation prompt question like transformer network possibly ture format like transformer network prompt instruction like translate following sentence hindi chop garlic finely explicit prompt specify set possible answer lead well performance example prompt template sentiment analysis prespeciﬁes potential answer prompt consist review plus incomplete statement human think input negative positive sentiment choice positive negative assistant believe good answer prompt use number sophisticated prompt characteristic speciﬁes allowable choice end prompt open parenthesis strongly suggest answer note speciﬁes role language model assistant include label example prompt improve performance example demonstration task prompt example demonstration call shot prompting contrast zero shot prompt shot zero shot mean instruction include label example example fig chapter arge language model show example question demonstration shot prompting example draw computer science question mmlu dataset describe section evaluate language model example demonstration computer science question mmlu dataset describe section follow multiple choice question high school computer science let python answer large asymptotically answer output statement python error aab answer figure sample shot prompt mmlu test high school computer science correct answer demonstration generally draw label training set select hand choice demonstration optimize timizer like dspy khattab automatically choose set tion increase task performance prompt dev set number demonstration need large example ing return example cause model overﬁt exact example primary beneﬁt demonstration demonstrate task format output demonstrate right answer particular question fact demonstration incorrect answer improve system min webson pavlick prompt way language model generate text prompt view learning signal especially clear prompt demonstration demonstration help language model learn perform novel task example new task kind learning different pretraine method set language model weight gradient descent method describe weight model update prompt change context activation network kind learn take place prompt context learning learning improve model performance reduce lossin context learning involve gradient base update model underlie parameter large language model generally system prompt single text prompt system prompt ﬁrst instruction language model deﬁne task role set overall tone context system prompt silently prepende user text example minimal system prompt create multi turn assistant conversation following include special eneration sample system helpful knowledgeable assistant answer concisely correctly user want know capital france actual text language model context conditional generation system helpful knowledgeable assistant answer concisely correctly user capital france fact modern language model long context ten sand tokens make powerful conditional generation look far prompt text mean system prompt prompt general long example system prompt language model anthropic claude word long include sentence like follow claude concise response simple question provide thorough response complex open end question claude able explain difficult concept idea clearly illustrate explanation example thought experiment metaphor claude provide information chemical biological nuclear weapon casual emotional empathetic advice drive conversation claude keep tone natural warm empathetic claude care people avoid encouraging facilitate self destructive behavior claude provide bullet point response use markdown bullet point sentence long human request possible create system prompt task like follow prompt create general grammar checker anthropic task text provide rewrite clear grammatically correct version preserve original meaning closely possible correct spelling mistake punctuation error verb tense issue word choice problem grammatical mistake user prompt system grammar particular piece text case system prompt prepende user prompt query entire string take context conditional generation guage model generation sampling token language model generate chapter arge language model generation depend probability token let remind self probability distribution come internal network language model transformer alternative like lstms state space model generate score call logit real value number token cabulary score vector uis normalize softmax legal probability distribution see logistic regression chapter logit vector uof shape give score possible token pass softmax vector shape assign probability token vocabulary show follow equation softmax fig show example softmax compute pedagogical pose simpliﬁed vocabulary word transformer decoder figure take logit vector uand softmax create probability vector give probability distribution token need select token generate task choose token generate base model itie call decode mention decode language decode model left right manner right leave language like arabic read right left repeatedly choose token condition previous choice call autoregressive generation autoregressive generation greedy decode simple way generate token generate likely token give context call greedy decode greedy algorithm onegreedy decoding make choice locally optimal turn good choice hindsight greedy decoding time step generation turn logit probability distribution token choose output wtthe token vocabulary high probability argmax ˆwt fig show example model choose generate autoregressive model predict value time tbase linear function value time language model linear layer non linearity loosely refer generation technique autoregressive token generate time step condition token select network previous step alternative like mask language model chapter non causal predict token base past future eneration sampling transformer decoder figure greedy decoding choose high probability word practice use greedy decode large language model major problem greedy decoding token choose deﬁnition extremely predictable result text generic repetitive greedy decoding predictable deterministic context identical probabilistic model greedy decoding result generate exactly string chapter extension greedy decode call beam search work task like machine translation constrained generate text language condition speciﬁc text language task people prefer text generate sampling method introduce bit diversity generation random sampling common method decode large language model involve pling recall chapter sample distribution mean choose sampling dom point accord likelihood sample language model represent distribution follow token mean choose token generate accord probability assign model likely generate token model think high probability likely generate token model think low probability randomly select token generate accord probability context deﬁne model generate iterate think roll die choose token accord result probability see chapter model course likely generate high probability token like greedy algorithm generate token small chance general likely generate token model think high probability context likely generate token model think low probability sample language model ﬁrst suggest early shannon miller selfridge see chapter page generate text unigram language model repeatedly randomly ple token accord probability reach pre determined length select end sentence token generate text large language model generalize model bit step sample token accord probability condition previous choice use large language model probability model tell chapter arge language model algorithm call random sampling orrandom multinomial samplingrandom sampling sample multinomial distribution word formalize random sampling follow generate sequence token wnguntil hit end sequence token mean choose xby sample distribution eos transformer decoder wordthe figure random multinomial sampling randomly choose word accord probability alas turn random sampling work problem random sampling go generate sensible high probable token odd low probability token tail distribution low probability add rare token stitute large portion distribution choose result generate weird sentence word greedy decoding boring random sampling dom need greedily choose choice time stray far low probability event standard sampling method modify random sampling dress issue describe common temperature sampling talk andtop chapter temperature sampling idea temperature sampling reshape probability distribution temperature sampling crease probability high probability token decrease probability low probability token result likely generate probability token likely generate token high probability implement intuition simply divide logit temperature etertbefore pass softmax low temperature sampling instead compute probability distribution vocabulary rectly logit following repeat softmax instead ﬁrst divide logit compute probability vector yas softmax rain large language model normally convert logit softmax show fig use temperature parameter ﬁrst scale logit fig abcdlogitsprobabilitie softmax latexit latexit uyabcdlogitsprobabilitie softmaxwithtemperature latexit latexit figure normal softmax temperature scaling add temperature scaling softmax ﬁrst dividing temperature parameter divide tincrease high probability element decrease low probability element vector vocabulary item tis normal softmax tis close distribution change low tis large score pass softmax dividing small fraction result make score large recall useful property softmax tend push high value low value large number pass softmax result distribution increase probability probability token decrease probability low probability token make distribution greedy contrast tapproache probability likely word approach result greedy decoding intuition temperature sampling come thermodynamic system high temperature ﬂexible explore possible state system low temperature likely explore subset low energy well state low temperature sampling smoothly increase probability probable token decrease probability rare token fig show schematic example simpliﬁed vocabulary token show different temperature value inﬂuence probability compute initial logit normal softmax set increase probability candidate set increase probability date get close greedy decoding fig option situation want toﬂatten word probability distribution instead make greedy temperature sampling help situation case high temperature sampling case use train large language model learn language model algorithm datum train language model train stage show fig chapter arge language model greedysoftmax output temperature close uniform low uniform figure see different value tchange result probability initial logit temperature sampling simpliﬁed example token vocabulary ﬁrst stage model train incrementally predict word enormous text corpora model use cross entropy loss call language modeling loss loss agate way network training datum usually base clean part web result model good dicte word generate text tuning call supervised ﬁnetuning sft second stage model train cross entropy loss follow instruction example answer question summary write code translate tence train special corpus lot text contain instruction correct response instruction call preference alignment ﬁnal stage model train maximally helpful harmful model give preference datum consist context follow potential continuation label usually people accepted reject continuation model train reinforcement learning reward base algorithm produce accept continuation reject continuation introduce pretraine save instruction tuning preference alignment chapter self supervise training algorithm pretraine intuition pretraine large language model idea self train self train orself supervision see chapter learn word representation like self training language modeling corpus text ing material time step task model predict word ﬁrst poorly task case know correct answer rain large language model pretraine datum pretraine llm pretraine instruction tuning preference alignment translate english chinese ﬂight sentiment sentence movie greatsummarize hawaii electric urge caution crew replace utility pole overnight highway instruction tune llm align llminstruction datapreference datahuman embezzle embezzling felony help assistant start create fake expense report figure stage train large language model pretraining instruction tuning preference alignment word corpus time well well predict correct word model self supervised add special gold label datum natural sequence word supervision simply train model minimize error predict true word training sequence practice train language model mean set parameter underlying architecture transformer introduce chapter weight matrix feedforward attention component like neural architecture train error backpropagation gradient descent need loss function minimize pass network loss function use language modeling cross entropy loss function see twice chapter chapter recall cross entropy loss measure difference predict probability distribution correct distribution probability distribution token vocabulary make loss case language modeling correct distribution ytcome know word represent hot vector correspond vocabulary entry actual word entry cross entropy loss language modeling determine probability model assign correct token token multiply zero ﬁrst term loss generality time tthe cross entropy loss simpliﬁed negative log probability model assign word training sequence formally ˆyto mean vector estimate token probability language model word position tof input model take input correct quence tokens use compute probability distribution chapter arge language model possible token compute model loss token word ignore model predict word instead use correct sequence tokens model estimate probability token idea model correct tory sequence predict word feed model good guess previous time step call teacher force teacher force fig illustrate general training approach step give precede token language model produce output distribution tire vocabulary training probability assign correct word calculate cross entropy loss item sequence loss batch average cross entropy loss entire sequence negative log ability formally length ttx weight network adjust minimize average cross entropy loss batch gradient descent fig error backpropagation computation graph compute gradient training adjust weight network transformer model introduce chapter weight include embed matrix ethat contain embedding word embedding learn successful predict upcoming word longandthanksfortrue tokenallce lossper token solongandthanksfor input token yallllmŷbackpropŷbackpropŷbackpropŷbackpropŷbackprop figure training llm token position model pass probability estimate possible word negative log model probability estimate correct token loss backpropagate model train weight include embedding loss average token batch detail training course depend speciﬁc network architecture implement model detail speciﬁcally transformer model chapter pretraine corpora large language model large language model mainly train text scrape web augment carefully curate datum training corpora large likely contain natural example helpful nlp task question answer pair example faq list translation sentence language document summary rain large language model web text usually take corpora automatically crawl web page like thecommon crawl series snapshot entire web produce common crawl proﬁt common crawl billion webpage version common crawl datum exist colossal clean crawl corpus raffel corpus billion token english ﬁltere way deduplicate remove non natural language like code sentence offensive word blocklist corpus consist large patent text document wikipedia news site dodge wikipedia play role lot language model training corpora book pile gao english text corpus construct pile publicly release code contain large text scrape web book wikipedia fig show composition dolma large open corpus english create public tool contain trillion token similarly consist web text academic paper code book encyclopedic material social medium soldaini figure pile corpus show size different component color code academic article pubmed arxiv patent uspta internet webtext clude subset common crawl wikipedia prose large corpus book dialogue include movie subtitle chat datum misc figure gao filtering quality safety pretraine datum draw web ﬁltere quality andsafety quality ﬁlter classiﬁer assign score document quality course subjective different quality ﬁlter train different way value high quality reference corpora like wikipedia book particular website avoid website lot pii tiﬁable information adult content filter remove boilerplate text frequent web kind quality ﬁltering deduplication level remove duplicate document duplicate web page duplicate text quality ﬁltering generally improve language model formance longpre llama team safety ﬁltering subjective decision include toxicity tion base run shelf toxicity classiﬁer mixed result problem current toxicity classiﬁer mistakenly ﬂag non toxic datum chapter arge language model generate speaker minority dialect like african american english problem model train datum somewhat toxic bad detect toxicity longpre issue question well safety ﬁltere portant open problem large dataset scrape web train language model pose ethical legal question copyright text large dataset like collection tion book copyright country like united state fair use doctrine allow copyright content transformative use clear remain true language el generate text compete market text train henderson datum consent owner website indicate want site crawl web crawler ﬁle term service recently sharp increase number site indicate want large language model builder crawl site training datum longpre clear legal status indication different country restriction retroactive effect large pretraining dataset unclear privacy large web dataset privacy issue contain private information like phone number email address ﬁlter try remove website likely contain large amount personal mation ﬁltering sufﬁcient return privacy question section skew training datum disproportionately generate author developed country likely skew result generation perspective topic group finetuning vast pretraine datum large language model include text domain want apply new domain task appear sufﬁciently pretraine datum example want language model specialize legal medical text multilingual language model know language beneﬁt datum particular language interest case simply continue train model relevant datum new domain language gururangan process take fully pretraine model run additional training pass cross entropy loss new datum call ﬁnetune word ﬁnetuning mean process ﬁnetune take pretraine model adapt parameter new datum chapter number different way word ﬁnetuning base exactly parameter update method describe continue train new datum end pretraine datum call continue pretraine pretraine fig sketch valuate large language model fine tuning datum pretraining datapretraine fine tuning pretraine lmfine tune figure pretraining ﬁnetuning pre train model ﬁnetune particular domain dataset different way ﬁnetune depend exactly parameter update ﬁnetune datum parameter parameter parameter speciﬁc extra circuitry future chapter evaluate large language model evaluate language model accuracy predict unseen text perform task like answer question translate text factor like fast run energy use fair explore section perplexity ﬁrst see chapter way evaluate language model measure predict unseen text well language model well predict upcoming word surprised assign high probability word occur test set want know language model well model text assign high probability practice deal probability log space assign high log likelihood talk predict word time compute ity token wifrom prior context course see chapter chain rule allow compute probability token compute probability text compute probability text multiply conditional bilitie token text result log likelihood text useful metric compare good language model text log likelihood logny use metric log likelihood evaluate language model reason probability test set sequence depend number word token fact probability test set chapter arge language model small long text clear chain rule tiplye probability probability deﬁnition zero product small small useful metric token normalize length compare text different length function probability call perplexity length normalize metric perplexity recall page perplexity model qon unseen test set inverse probability qassign test set probability test set normalize test set length token test set ntoken perplexity visualize perplexity compute function probability compute new word use chain rule expand tion probability test set nvuutny note inverse high probability word sequence low perplexity low perplexity model datum well model minimize perplexity equivalent maximize test set probability accord language model perplexity use inverse probability inverse arise original deﬁnition perplexity cross entropy rate information theory interest explanation section remember perplexity inverse relationship probability caveat perplexity depend number token nin text sensitive difference tokenization algorithm mean hard exactly compare perplexity produce language model different tokenizer reason perplexity well compare language model use tokenizer downstream task reasoning world knowledge perplexity measure kind accuracy accuracy predict word kind accuracy downstream task want apply language model like question answer machine translation reasoning measure accuracy task discussion task speciﬁc evaluation future chapter machine translation chapter information retrieval chapter speech recognition chapter brieﬂy introduce metric mechanism measure racy answer question focus multiple choice question dataset mmlu massive multitask language understanding commonly dataset mmlu knowledge reasoning question area include medicine matic computer science law accuracy answer choice question useful proxy model ability reason factual valuate large language model example mmlu question microeconomic mmlu microeconomic example reason government discourage regulate lie producer surplus lose consumer surplus gain monopoly price ensure productive efﬁciency cost society allocative efﬁciency monopoly ﬁrm engage signiﬁcant research development consumer surplus lose high price low level output fig show way mmlu turn question prompt test language model case show example prompt demonstration mmlu mathematic prompt follow multiple choice question high school mathematic number list answer compute answer dap yap yap bap dap equal bap answer figure sample shot prompt mmlu test high school mathematic correct answer take performance mmlu metric language model quality problem true evaluation base public dataset problem datum contamination datum contamination datasetdata contamination test test set kind make way training set example large language model train web mmlu web model incorporate mmlu question training question evaluation metric overstate performance language model way mitigate data contamination available exact training datum train model report training overlap speciﬁc test set zhang factor evaluate language model accuracy thing care evaluate model dodge ethayarajh jurafsky inter alia example care big model long take train inference limit time limited memory gpu run model ﬁxed memory economic bit rusty correct answer chapter arge language model size big model use energy prefer model use energy reduce environmental impact model reduce ﬁnancial cost build deploy target evaluation factor measure performance normalize give compute memory budget directly measure energy usage model kwh kilogram emit strubell henderson liang feature language model evaluation measure fairness know language model bias exhibit gendere racial stereotype decrease performance language certain demographic group language model evaluation benchmark measure strength bias stereoset nadeem realtoxicityprompt gehman bbq parrish want language model performance equally fair different group ple choose evaluation fair rawlsian sense maximize welfare bad group rawl hashimoto sagawa finally kind leaderboard like dynabench kiela general evaluation protocol like helm liang return later chapter introduce evaluation metric speciﬁc task like question answering information retrieval ethical safety issue language model ethical safety issue key think design artiﬁcial agent large language model mary shelley depict center novel frankenstein problem create artiﬁcial agent consider ethical humanistic concern large language model unsafe way example llm prone say thing false problem call hallucination language hallucination model train generate text dictable coherent training rithm see far way enforce text generate correct true cause enormous lem application fact ter related symptom language el suggest unsafe action example directly suggest user dangerous illegal thing like harm er user seek information language model safety critical situation like ask medical advice emergency situation indicate intention self harm incorrect advice dangerous life threatening problem predate large language model example bickmore give ipant medical problem pose pre llm commercial dialogue system siri alexa google assistant ask determine action base system response propose action actually take thical safety issue language model lead harm death return issue hallucination factuality ter introduce propose mitigation method like retrieval augment generation chapter discuss safety tuning alignment system harm user verbally attack create tational harm blodgett example generate abusive harmful stereotype cheng negative attitude brown sheng demean particular group people abuse stereotype cause psychological harm user gehman pletely non toxic prompt lead large language model output hate speech abuse user liu test system respond pair late user turn identical mention different gender race find example simple change like word instead sentence cause system respond offensively tive sentiment hofmann find llm likely discriminate people particular dialect like african american glish problem predate large language model microsoft tay tay chatbot example take ofﬂine hour go live begin post message racial slur conspiracy theory personal attack user tay learn bias action training datum include user purposely teach system repeat kind language neff nagy important ethical safety issue privacy privacy cern beginning computing weizenbaum design chatbot eliza experiment computational therapy weizenbaum ple deeply emotionally involved conduct personal conversation eliza chatbot extent ask weizenbaum leave room type weizenbaum suggest want store eliza conversation people immediately point violate people privacy user likely personal information large language model common current llm use case personal advice support zao sander human like system user likely disclose private information likely worry harm disclosure ischen discuss pretraine datum likely private information like phone number address problematic large language model leak information training datum adversary extract training data text language model person phone number address henderson carlini problematic large language model train extremely sensitive private dataset electronic health record related safety issue emotional dependence reeve nass people tend assign human characteristic computer interact way typical human human interaction interpret utterance way speak human aware talk computer llm signiﬁcant inﬂuence people cognitive emotional state lead problem like emotional dependence llm issue emotional engagement privacy mean need think carefully impact llm people interact addition ability harm user way llm carry additional harmful activity especially agent base paradigm chapter arge language model possible language model directly interact world language model malicious actor generate text fraud phishing propaganda disinformation campaign socially harmful activity brown mcgufﬁe newhouse large language model generate text emulate online extremist risk plifye extremist movement attempt radicalize recruit course see section issue llm stem pretraine corpora scrape web include harm datum send potential copyright violation bias training datum ampliﬁed language model see embed model chapter ampliﬁed find way mitigate ethical safety issue important current research area nlp important step carefully analyze datum pretrain large language model way understand safety issue toxicity discrimination privacy fair use make extremely important language model include datasheet page model card page give replicable information corpora train open source model specify exact training datum active area research mitigate problem abuse toxicity like detect respond appropriately toxic contexts wolf dinan value sensitive design carefully consider possible harm advance man friedman hendry important dinan number suggestion good practice system design ple getting inform consent participant training interact deploy llm important study interactional property llm involve human participant researcher work issue institutional review board irb institution irb help protect safety experimental participant summary chapter introduce large language model summary main point cover large language model system predict word vious word give context preﬁx word use prediction conditionally generate text major architecture language model encoder decoder encoder decoder know large language model generate text decoder model describe encoder chapter encoder decoder chapter nlp task question answer sentiment analysis cast task word prediction address large language model instruct language model prompt text string user issue language model model useful iteratively generate token condition prompt process ﬁnde effective prompt task know prompt neere choice word generate large language model sampling distribution possible note common sampling approach temperature sampling lie tween greedy decoding generate probable word dom sampling generate random word accord probability temperature sampling increase probability high probability word decrease probability low probability word sample new distribution large language model pretraine predict word dataset billion word generally scrape web dataset need ﬁltere quality balanced domain upsampling downsampling pretraine algorithm rely cross entropy loss minimize tive log probability true word language model evaluate perplexity evaluation accuracy proxy downstream task like mmlu question answer dataset metric factor like fairness energy use language model numerous ethical safety issue include cination unsafe instruction bias stereotype misinformation ganda violation privacy copyright historical note discuss chapter early language model gram guage model develop roughly simultaneously independently fred linek colleague ibm thomas watson research center james baker cmu jelinek ibm team ﬁrst coin term guage model mean model way kind linguistic property grammar semantic discourse speaker characteristic inﬂuenced word sequence itie jelinek contrast language model acoustic model capture acoustic phonetic characteristic phone sequence gram language model widely year wide variety nlp task like speech recognition machine translation multiple component model contexts gram model grow long gram model commonly efﬁcient toolkit stolcke heaﬁeld root neural large language model lie multiple place application jelinek group ibm research inative classiﬁer language model roni rosenfeld dissertation feld ﬁrst apply logistic regression maximum entropy maxent model language modeling ibm lab publish fully form version rosenfeld model integrate sort mation logistic regression predictor include gram information feature context include distant gram pair associate word call trigger pair rosenfeld model preﬁgure modern language model statistical word predictor train self supervise manner simply learn predict upcoming word corpus ﬁrst use pretraine embedding model word meaning lsa lsi model deerwester recall history section chapter arge language model chapter lsa latent semantic analysis term document matrix train corpus singular value decomposition apply ﬁrst dimension vector embed represent word landauer ﬁrst word embed addition development idea pretraine embedding lsa community develop way combine lsa embedding gram integrate language model bellegarda coccaro jurafsky inﬂuential series paper develop idea neural language model bengio bengio bengio yoshua gio colleague draw central idea line self supervise language modeling work discriminatively train word predictor train embedding like maxent model rosenfeld bengio model word run text supervision signal like lsa model gio model learn embed unlike lsa model process language modeling bengio model neural guage model neural network learn predict word prior word learn embedding prediction process neural language model extend way year importantly form rnn language model mikolov mikolov rnn language model ﬁrst neural model accurate surpass performance traditional gram language model soon mikolov mikolov propose simplify hidden layer neural net language model create pretraine word embedding static embed model like lsa instantiate particular model pretraining representation train pretraine dataset representation task dai peters reframe idea propose model pretraine language model objective identical model frozen directly apply language modeling ﬁnetune language model objective example elmo bilstm self supervise large pretraine dataset language model objective ﬁnetune speciﬁc dataset freeze weight add task speciﬁc head elmo work particularly inﬂuential appearance ment clear community language model general solution nlp problem transformer ﬁrst apply encoder decoder vaswani mask language modeling devlin chapter chapter radford show transformer base toregressive language model perform zero shot nlp task like summarization question answer technology language model apply domain task like vision speech genetic term foundation model foundation model time general term use large language model technology domain area element compute essarily word bommasani broad survey sketch portunitie risk foundation model special attention large language true art memory art attention samuel johnson idler september chapter introduce transformer standard architecture e large language model discuss prior chapter transformer base large language model completely change ﬁeld speech language processing subsequent chapter textbook use previous chapter focus chapter use transformer model left right call causal autoregressive language ing give sequence input token predict output token conditioning prior context transformer neural network speciﬁc structure include mechanism call self attention ormulti head attention think way build contextual representation token meaning attend integrate information surround token help model learn token relate large span stackedtransformerblockssolongandthanksforlongandthanksfornext tokenall input logitslogitslogitslogitslogit figure architecture left right transformer show input token encode pass set stack transformer block language model head predict token fig sketch transformer architecture transformer major component center column transformer block block multilayer network multi head attention layer feedforward network layer multi head attention develop historically rnn attention mechanism ter deﬁne attention scratch chapter ransformer normalization step map input vector xiin column input token output vector set nblock map entire context window input vector window output vector length column contain stacked block column block precede input encode component cesse input token like word thank contextual vector representation embed matrix eand mechanism encode token position column follow language modeling head take embed ﬁnal transformer block pass unembedde matrix uand softmax vocabulary generate single token column transformer base language model complex detail fold chapter chapter discuss language model arepretraine token generate sample section introduce multi head attention rest transformer block input encoding language model head component transformer chapter introduce mask language modeling bert family bidirectional encoder model chapter show instruction tune language model perform nlp task align model human preference ter introduce machine translation encoder decoder architecture use encoder decoder architecture chapter attention recall chapter static embedding sentation word meaning vector irrespective context wordchicken example represent ﬁxed vector static vector word itmight encode pronoun animal inanimate entity context itha rich meaning consideritin sentence chicken cross road itwa tired chicken cross road itwa wide chicken reader know chicken tired road reader know road compute meaning sentence need meaning itto associate chicken ﬁrst sentence associate road second sensitive context furthermore consider read leave right like causal language model cesse sentence word chicken cross road point know thing itis go end refer representation itat point aspect chicken androad reader try guess happen fact word rich linguistic relationship word far away pervade language consider example key cabinet areon table ﬁrst example itcorefer chicken second itcorefer road return chapter ttention walk pond notice tree bank phrase key subject sentence english language agree grammatical number verb case plural english use singular verb like iswith plural subject like key discuss agreement chapter know bank refer pond river ﬁnancial institution context include word like pond discuss word sense chapter point example contextual word help pute meaning word context far away sentence graph transformer build contextual representation word meaning tual embedding integrate meaning helpful contextual word acontextual embedding transformer layer layer build rich rich contextualized tion meaning input token layer compute representation token iby combine information ifrom previous layer mation neighbor token produce contextualized representation word position attention mechanism transformer weigh combine representation appropriate token context layer kto build representation token layer kself attention distributioncolumn correspond input token figure self attention weight distribution athat computation representation word itat layer compute representation attend differently word layer darker shade indicate high self attention value note transformer attend highly column correspond tokens chicken androad sensible result point itoccur plausibly corefer chicken road like representation itto draw representation early word figure adapt uszkoreit fig show schematic example simpliﬁed transformer uszkoreit ﬁgure describe situation current token itand need compute contextual representation token layer transformer draw representation layer prior token ﬁgure use color represent attention distribution contextual word token chicken androad high attention weight mean put representation draw heavily representation chicken androad useful build ﬁnal representation sinceitwill end coreferre chicken orroad let turn attention distribution represent chapter ransformer attention formally say attention computation way compute vector representation token particular layer transformer selectively attend integrate information prior token previous layer attention take input representation xicorresponding input token position context window prior input produce output causal left right language model context prior word process model access xia representation prior token context window context window consist thousand token token contrast chapter generalize attention look ahead future word fig illustrate ﬂow information entire causal self attention layer attention computation happen parallel token position self attention layer map input sequence output sequence length attentionattentionself figure information ﬂow causal self attention process input model attend input include simpliﬁed version attention heart attention weight sum context vector lot complication add weight computed get sum pedagogical purpose let ﬁrst describe simpliﬁed intuition attention attention output aiat token position simply weighted sum representation use mean xjshould contribute simpliﬁed version jxj ji scalar weigh value input xjwhen sum input compute shall compute aweighting attention weight prior embed proportionally similar current token output attention sum embedding prior token weight similarity current token embed compute similarity score viadot product map vector scalar value range large score similar vector compare normalize score softmax create vector weight simpliﬁed version softmax fig compute compute score normalize softmax result probability weight indicate proportional relevance current position ttention softmax weight likely high xiis similar result high dot product context word similar softmax assign weight word use weight avalue compute weighted sum simpliﬁed attention equation demonstrate attention base approach computing compare xito prior vector normalize score probability distribution weight sum prior vector ready remove simpliﬁcation single attention head query key value matrix see simple intuition attention let introduce actual attention head attention head version attention transformer word head head transformer refer speciﬁc structured layer attention head allow distinctly represent different role input embed play course attention process current element compare precede input refer role query query role precede input compare current element determine similarity weight refer role key key ﬁnally value precede element get weight sum value compute output current element capture different role transformer introduce weight matrix weight project input vector xiinto tation role query key value give projection compute similarity current ment xiwith prior element use dot product current element query vector qiand precede element keyvector furthermore result dot product arbitrarily large positive negative value exponentiate large value lead numerical issue loss gradient training avoid scale dot product factor relate size embedding divide square root dimensionality query key vector replace simpliﬁed ensue softmax calculation result jremain output calculation head iis base weighted sum value vector ﬁnal set equation compute self attention single attention output vector aifrom single input vector version attention compute aiby sum value prior element weight similarity keyto query current element softmax head jvj head chapter ransformer sum weight value turn weight generate key query value compare query withthe key output self weigh value divide scalar score reshape figure calculate value element sequence causal right self attention illustrate fig case calculate value output sequence note introduce matrix right multiply attention head necessary reshape output head input attention xiand output attention aiboth dimensionality dthemodel dimensionality discuss section output hiof transformer block intermediate tor inside transformer block dimensionality have dimensionality make transformer modular let talk shape input output let look internal shape dimension dkfor query key vector query vector key vector dimensionality dot product produce scalar separate dimension dvfor value vector transform matrix wqha shape output head iin equation shape desire output shape need reshape head output wois shape original transformer work vaswani dwa dkanddvwere multi head attention equation describe single attention head actually transformer use multiple attention head intuition head attend context different purpose head ize represent different linguistic relationship context element current token look particular kind pattern context multi head attention aseparate attention head reside inmulti head attention parallel layer depth model set parameter allow head model different aspect relationship input ransformer block head iin self attention layer set query key value matrix wqi wki wvi project input separate query key value embedding head multiple head model dimension dis input output query key embedding dimensionality value embedding dimensionality original transformer paper head weight layer wqiof shape shape wviof shape equation attention augment multiple head fig show intuition softmax headc jvc multiheadattention note multiheadattention function current input input causal leave right attention use chapter input left version attention chapter attention function token right return idea causal input introduce idea mask right context output ahead shape output multi head layer ahead consist avector shape concatenate produce single output dimensionality use linear projection reshape result multi head attention vector aiwith correct output shape input transformer block self attention calculation lie core call transformer block addition self attention layer include kind layer feedforward layer residual connection normalizing layer ally call layer norm fig illustrate transformer block sketch common way thinking block call residual stream elhage residual stream ual stream viewpoint consider processing individual token ithrough transformer block single stream dimensional representation token position residual stream start original input vector component read input residual stream add output stream input stream embed token dimensionality initial embed gets pass residual connection progressively add component transformer chapter ransformer adv adv project dconcatenate outputseach headattend diﬀerentlyto context figure multi head attention computation input produce output multi head attention layer ahead query key value weight matrix output head concatenate project produce output size input layer norm layer normmultiheadattentionfeedforward residualstream figure architecture transformer block show residual stream ﬁgure show prenorm version architecture layer norm happen attention feedforward layer tention layer see feedforward layer introduce attention feedforward layer computation call layer norm initial vector pass layer norm attention layer result add stream case original input vector summed vector pass layer norm feedforward layer output add residual usehito refer result output transformer block token early description residual stream describe different metaphor residual connection add input component output residual stream perspicuous way visualize ransformer block see attention layer let introduce feedforward layer norm computation context process single input xiat token position feedforward layer feedforward layer fully connect layer network hide layer weight matrix introduce chapter weight token position different layer layer mon dimensionality dffof hidden layer feedforward network large model dimensionality example original transformer model relu layer norm stage transformer block normalize vector process call layer norm short layer normalization layer norm form normalization improve training performance deep neural network keep value hidden layer range facilitate gradient base training layer norm variation score statistic apply single tor hidden layer term layer norm bit confusing layer norm isnotapplie entire transformer layer embed vector single token input layer norm single vector dimensionality output vector normalize dimensionality ﬁrst step layer normalization calculate mean standard deviation element vector normalize give embed vector xof sionality value calculate follow ddx ddx give value vector component normalize subtract mean dividing standard deviation result computation new vector zero mean standard deviation finally standard implementation layer normalization learnable eter gandb represent gain offset value introduce layernorm put function compute transformer block press break equation component computation shape stand transformer superscript chapter ransformer computation inside block layernorm multiheadattention layernorm notice component take input information token residual stream multi head attention look neighbor token context output attention add token embed stream fact elhage view attention head literally move information residual stream neighboring token current stream high dimensional embed space position contain information current ken neighboring token albeit different subspace vector space fig show visualization movement token aresidual streamtoken bresidual stream figure attention head information token residual stream token residual stream crucially input output dimension transformer block match stack token vector xiat input block dimensionality output hialso dimensionality transformer large language model stack block layer small language model layer large recent model come issue stack bit equation follow equation single transformer block residual stream metaphor go transformer layer ﬁrst transformer block layer transformer early block residual stream represent current token high transformer block residual stream usually represent follow token end train predict token stack block requirement end high transformer block single extra layer norm run hiof token stream language model head layer deﬁne common current transformer architecture call arallelize computation single matrix parallelizing computation single matrix description multi head attention rest transformer block perspective compute single output single time step iin single residual stream point early attention computation perform token compute aiis independent computation token true computation transformer block computing hifrom input mean easily parallelize entire computation take advantage efﬁcient matrix multiplication routine pack input embedding ntoken input quence single matrix xof size row xis embed token input transformer large language model commonly input length nfrom long context million token achieve architectural change like special long context mechanism discuss vanilla transformer think xhaving row dimensionality embed model dimension parallelizing attention let ﬁrst single attention head turn multiple head add rest component transformer block head multiply xby query key value matrix wqof shape shape andwvof shape produce matrix qof shape shape vof shape contain key query value vector give matrix compute requisite query key comparison taneously multiply single matrix multiplication product shape visualize fig figure show compute single matrix multiple efﬁciently scale score softmax multiply result vresulte matrix shape vector embed representation token input reduce entire self attention step entire sequence ntoken head architecture original deﬁnition transformer vaswani alternative tecture call postnorm transformer layer norm happen attention ffn layer turn move layer norm work well require extra layer chapter ransformer follow computation head head mask future notice introduce mask function self attention computation describe problem calculation score query value key value include follow query inappropriate setting language modeling guess word pretty simple know element upper triangular portion matrix set softmax turn zero eliminate knowledge word follow sequence practice add mask matrix min upper triangular portion fig show result mask chapter use word future task need figure show upper triangle tion comparison matrix zero set softmax turn zero fig show schematic computation single attention head parallelize matrix form fig fig clear attention quadratic length input layer need compute dot product pair token input make expensive compute attention long document like entire novel nonetheless modern large language model manage use long context thousand ten thousand token parallelize multi head attention multi head attention self attention input output model dimension key query embedding dimensionality value embedding dimensionality original transformer paper head weight layer wqcof shape shape andwvcof shape multiply input pack xto produce qof shape shape vof shape output ahead shape output head layer ahead consist amatrice shape use matrix processing concatenate produce single output dimensionality finally use ﬁnal linear projection shape reshape original output dimension token multiply concatenate output woof shape arallelize computation single matrix qkt maskedmask token token token token token token token token value token token token token input token token token token token token token token input token token token token dkdk dvn dvd dkd dkd dvn dkn dkn figure schematic attention computation single attention head parallel ﬁrst row show computation vmatrice second row show computation qkt masking softmax computation normalizing dimensionality show weighted sum value vector ﬁnal attention vector yield self attention output aof shape head selfattention multiheadattention head put parallel input matrix xthe function compute parallel entire layer ntransformer block block input token express layernorm note xto mean input layer come ﬁrst layer section input initial word positional embed vector describe subsequent layer input output previous layer break computation perform transformer layer show equation component computation use shape stand transformer superscript demarcate computation inside block use xto mean input block previous layer chapter ransformer embed layernorm multiheadattention layernorm use notation like ffn mean ffn apply parallel nembedde vector window similarly ntoken norme parallel layernorm crucially input output dimension transformer block match stack token xiat input block represent embedding dimensionality mean input xand output hare shape input embedding token position let talk input xcome give sequence ntoken nis context length tokens matrix xof shape embed embed word context transformer separately compute embedding input token embed input positional embed token embed introduce chapter vector dimension dthat initial representation input token pass vector transformer layer residual stream embed representation change grow incorporate context play different role depend kind language model build set initial embedding store embed matrix row jvjtoken vocabulary reminder vhere mean vocabulary token vis relate value vector word row vector ddimension eha shape give input token string like thank ﬁrst convert token vocabulary index create ﬁrst tokenize input bpe sentencepiece representation thank use indexing select correspond row row row row row way think select token embedding embed matrix represent token hot vector shape dimension word vocabulary recall hot vector hot vector element element dimension word index vocabulary value word thank index vocabulary show multiplying hot vector non zero element simply select relevant row vector word result embed word depict fig input embedding token position figure select embed vector word multiply embed matrix ewith hot vector index extend idea represent entire token sequence matrix hot vector nposition transformer context window show fig figure select embed matrix input sequence token ids wby tiplye hot matrix correspond wby embed matrix token embedding position dependent represent position token sequence combine token embedding positional embedding speciﬁc position input embedding positional embedding simple method call absolute position start randomly initialize embedding correspondingabsolute position possible input position maximum length example embedding word ﬁsh embedding position word embedding positional embedding learn parameter training store matrix eposof shape produce input embed capture positional information add word embed input correspond positional embedding individual token position embedding size sum new embed serve input processing fig show idea positionembeddingswordembedding figure simple way model position add embedding absolute position token embed produce new embedding chapter ransformer ﬁnal representation input matrix row ii representation ith token input compute add embedding token occur position positional embedding position potential problem simple position embed approach plenty training example initial position input spondingly few outer length limit embedding poorly train generalize testing alternative choose static function map integer input real value vector way well handle sequence arbitrary length combination sine cosine function differ frequency original transformer work sinusoidal sition embedding help capture inherent relationship position like fact position input closely related position position complex style positional embed method extend idea ture relationship directly represent relative position instead ofrelative position absolute position implement attention mechanism layer add initial input language model head component transformer introduce language modeling head word head mean additional neural circuitry welanguage model head head add basic transformer architecture apply pretraine model task language modeling head circuitry need language modeling recall language model simple gram model chapter feedforward rnn language model chapter chapter word predictor give context word assign probability possible word example precede context thank want know likely word ﬁsh compute language model ability assign conditional probability possible word give distribution entire vocabulary gram language model chapter compute probability word give count occurrence prior word context size transformer language model context size transformer context window large like token large model large context million word possible special long context architecture job language modeling head output ﬁnal layer token nand use predict upcoming word fig show accomplish task take output token layer dimensional output embed shape produce probability distribution word choose generate ﬁrst module fig linear layer job project output represent output token embed position nfrom helanguage modeling head layer ltransformerblocksoftmax vocabulary vunembedde layer word unembedde layeru language model headtake hln output adistribution vocabulary figure language modeling head circuit transformer map output embed token nfrom transformer layer probability distribution word vocabulary block shape logit vector score vector logit single score jvjpossible word vocabulary logit vector uis dimensionality linear layer learn commonly tie matrix transpose embed matrix recall weight tie use weight tie weight different matrix model input stage transformer embed matrix shape map hot vector vocabulary shape embed shape language model head transpose embed matrix shape map embed shape vector vocabulary shape learning process ewill optimize good mapping transpose ettheunembedde layer perform reverse mapping unembedde softmax layer turn logit uinto probability yover vocabulary softmax use probability thing like help assign probability give text important usage generate text ple word probability sample high probability word greedy decode use sample method section section case entry ykwe choose probability vector generate word index fig show total stack architecture token note input transformer layer ii output precede layer terminological note conclude kind unidirectional causal language model call model model constitute roughly half encoder decoder model decoder model transformer apply machine translation chapter confusingly original introduction transformer encoder decoder architecture later standard paradigm chapter ransformer wisample token togenerate position feedforwardlayer normattentionlayer norm input tokenlanguagemodelinghead inputencoding logit feedforwardlayer normattentionlayer normlayer normattentionlayer normhli token layer figure transformer language model decoder stack transformer block mapping input token wito predict token causal language model deﬁne decoder original architecture sample sample method introduce parameter enable ing important factor generation quality anddiversity method emphasize probable word tend produce generation rate people accurate coherent factual boring repetitive method bit weight middle probability word tend creative diverse factual likely incoherent low quality ksampling sampling simple generalization greedy decoding instead choose sampling single probable word generate ﬁrst truncate distribution rain topkmost likely word renormalize produce legitimate probability distribution randomly sample kword accord renormalize probability formally choose advance number word word vocabulary use language model compute likelihood word give context sort word likelihood throw away word kmost probable word renormalize score kword legitimate probability tion randomly sample word remain kmost probable word accord probability ksampling identical greedy decode set kto large number lead select word necessarily probable probable choice result generate diverse high quality text nucleus psample problem ksampling kis ﬁxed shape probability distribution word differ different contexts set word likely include probability mass time probability distribution ﬂatter word include small probability mass alternative call sample ornucleus sampling holtzman sampling kword ppercent probability mass goal truncate distribution remove unlikely word measure probability number word hope measure robust different contexts dynamically increase decrease pool word candidate give distribution sort distribution probable pvocabulary small set word training describe training process language model prior chapter large language model train cross entropy loss call negative log likelihood loss time tthe cross entropy loss negative log ability model assign word training sequence fig illustrate general training approach step give precede word ﬁnal transformer layer produce output distribution entire vocabulary training probability assign correct word model calculate cross entropy loss item sequence loss training sequence average cross entropy loss entire sequence weight network adjust minimize average loss training sequence gradient chapter ransformer longandthanksfornext tokenallloss latexit stackedtransformerblockssolongandthanksfor input logitslogitslogitslogitslogit latexit figure training transformer language model transformer training item process parallel element sequence compute separately large model generally train ﬁlle context window ple token llama text document short multiple document pack window special end text token batch size gradient descent usually large large model use batch size million token deal scale large language model large example llama instruct model meta billion parameter layer model dimensionality attention head train terabyte text token llama team vocabulary token lot research understand llm scale especially implement give limited resource section discuss think scale concept scale law important technique get language model work efﬁciently cache parameter efﬁcient ﬁne tuning scale law performance large language model show mainly determine factor model size number parameter count embedding dataset size training datum compute training improve model add parameter add layer have wide context training datum training iteration relationship factor performance know scale law roughly speak performance large language model loss scale scale eale scale power law property model training example kaplan find follow relationship lossla function number non embed parameter dataset size compute budget model train limited parameter dataset compute budget case property hold constant number non embedding parameter ncan roughly compute low ignore bias das input output dimensionality model dattna self attention layer size dffthe size feedforward layer assume dattn layer dimensionality billion parameter value acdepend exact transformer architecture tokenization vocabulary size precise value scale law focus relationship scale law useful decide train model particular formance example look early training curve performance small amount datum predict loss add datum increase model size aspect scale law tell datum need add scale model cache see fig repeat attention vector efﬁciently compute parallel training matrix multiplication unfortunately efﬁcient computation inference training inference time iteratively generate token time new token generate need compute query key value multiply tively waste computation time recompute key value vector prior token prior step compute key value vector instead recompute compute key value vector store memory cache cache grab cache need fig modiﬁes fig chapter ransformer dkdk figure part attention computation extract fig show black vector store cache recompute compute tion score token computation take place single new token show value cache recompute parameter efﬁcient fine tuning mention common language model information new domain ﬁnetune continue train predict upcoming word additional datum fine tuning difﬁcult large language model enormous number parameter train pass batch gradient descent backpropagate huge layer make ﬁnetune huge language model extremely expensive processing power memory time reason alternative method allow model ﬁnetune change parameter method call parameter efﬁcient ﬁne tuning peft efﬁciently select subset efﬁcient ﬁne tuning peft update ﬁnetune example freeze parameter change update particular subset parameter describe model call lora forlow rankadaptation lora intuition lora transformer dense layer perform matrix multiplication example wolayer attention tion instead update layer ﬁnetuning lora freeze layer instead update low rank approximation few parameter consider matrix wof dimensionality need update ﬁnetune gradient descent normally matrix update dwof dimensionality update gradient descent lora freeze wand update instead low rank decomposition create matrix aandb aha size size choose rto small ﬁnetune update aandbinstead ofw replace fig show intuition replace forward pass new forward pass instead lora number advantage dramatically reduce hardware requirement gradient calculate parameter weight update simply add pretraine weight abis size initial experiment kaplan precise value parameter nterprete transformer hpretraine figure intuition lora freeze wto pretraine value instead tune train pair matrix aandb update instead sum wand update mean add time inference mean possible build lora module different domain swap add subtract original version lora apply matrix attention computation wolayer variant lora exist interpret transformer transformer base language model manage language task subﬁeld interpretability call mechanistic interpretability ity focus way understand mechanistically go inside transformer subsection discuss study aspect transformer interpretability context learning induction head way get model want think prompt fundamentally different pretraine learn pretraine mean update model parameter gradient descent accord loss function prompt demonstration teach model new task model learn task demonstration process prompt demonstration think process prompt kind learn example model get prompt well tend predict upcoming token information context help model predictive power term context learning ﬁrst propose brown theirin context learn introduction system refer kind learn chapter ransformer guage model prompt context learning mean language model learn new task well predict token generally reduce loss e forward pass inference time gradient base update model parameter context learning work know sure intriguing idea hypothesis base idea induction head induction head elhage olsson induction head circuit kind abstract component network induction head circuit attention computation transformer discover look mini language model attention head function induction head predict repeat sequence example see pattern input sequence predict bwill follow instantiate pattern completion ruleab have preﬁx matching component attention computation look current token search context ﬁnd prior instance ﬁnd induction head copying mechanism copy token follow early increase probability occur fig show example figure sequence vintage car vintage induction head identiﬁes initial occurrence vintage attend subsequent word car preﬁx matching predict car word copying mechanism determine head independent output current token leverage decomposition elhage discover distinct behaviour certain attention head name induction head behaviour emerge head process sequence form head circuit direct attention ward appear directly previous occurrence current token behaviour term preﬁx match circuit quently increase output logit token term copying overview mechanism show figure method model utilise recently develop open source model cai base original llama touvron ture model feature grouped query tion mechanism ainslie enhance efﬁciency comprise layer attention head use query group size attention head show superior performance compare predecessor large model feature layer tention head use query group size attention head select exemplary performance needle assess llm ability retrieve single critical piece information bedded lengthy text mirror functionality induction head scan context prior occurrence token extract relevant subsequent information identify induction head identify induction head model sure ability attention head perform preﬁx match random input follow task agnostic approach compute matching score outline bansal argue focus solely preﬁx matching score sufﬁcient analysis high matching core speciﬁcally indicate induction head relevant head tend high copying capability bansal erate sequence random token exclude common common token sequence repeat time form input model preﬁx match score culate average attention value token token directly follow token early repeat ﬁnal preﬁx matching score average ﬁve random sequence preﬁx match score show figure refer figure appendix model exhibit head notably high preﬁx match score distribute layer model head preﬁx matching score high indicate degree cialisation preﬁx matching head high score head ablation investigate signiﬁcance induction head speciﬁc icl task conduct zero ablation head high preﬁx matching score ablation process involve mask corresponding partition output matrix denote oin set zero effectively render head inactive work term induction head refer deﬁne behavioural induction head mechanistic one true induction head veriﬁe mechanistically analysis employ preﬁx match score proxy continue use term induction head simplicity rest paper figure induction head look vintage use preﬁx matching mechanism ﬁnd prior instance vintage copying mechanism predict car occur figure crosbie shutova olsson propose generalized fuzzy version pattern pletion rule implement rule like mean semantically similar way responsible context learning suggestive evidence hypothesis come bie shutova ablate induction head cause context ablate learning performance decrease ablation originally medical term mean removal use nlp interpretability study tool test causal effect knock hypothesized cause expect effect disappear crosbie shutova ablate induction head ﬁrst ing attention head perform induction head random input sequence zero output head set certain term output trixwoto zero ﬁnd ablated model bad context learning bad performance learn demonstration prompt logit lens useful interpretability tool logit lens nostalgebraist offer logit lens way visualize internal layer transformer represent idea vector layer transformer tend preﬁnal embed simply multiply unembedde layer logit compute softmax distribution word vector represent useful window internal representation model network train ummary representation function way logit lens work perfectly useful trick help visualize internal layer transformer summary chapter introduce transformer component language modeling task introduce previous chapter summary main point cover transformer non recurrent network base multi head attention kind self attention multi head attention computation take input vector xiand map output aiby add vector prior token weight relevant processing current word atransformer block consist residual stream input prior layer pass layer output different ponent add component include multi head attention layer follow feedforward layer precede layer normalization transformer block stack deep powerful network input transformer compute add embed compute embed matrix positional encoding represent quential position token window language model build stack transformer block language model head apply unembedde matrix output hof layer generate logit pass softmax generate word probability transformer base language model wide context window ken large model special mechanism allow draw enormous amount context predict upcoming word computational trick make large language model efﬁcient cache andparameter efﬁcient ﬁnetuning historical note transformer vaswani develop draw line prior research self attention andmemory network encoder decoder attention idea soft weighting encoding input word inform generative decoder chapter develop grave context handwriting generation bahdanau idea extend self attention drop need separate encoding decode sequence instead see attention way weight token collect information pass low layer high layer ling cheng liu aspect transformer include terminology key query value come memory network mechanism add external write memory network embedding query match keys chapter ransformer resent content associative memory sukhbaatar weston grave history tbd language model larvatus prodeo mask forward descarte previous chapter introduce transformer see train transformer language model causal left right language model chapter introduce second paradigm pretraine language model bidirectional transformer encoder widely version bert bert model devlin model train mask language modeling mask language modelingwhere instead predict follow word mask word middle ask model guess word give word side method allow model right left context introduce ﬁnetune prior chapter describe new ﬁnetuning kind ﬁnetuning transformer network learn train model add neural net classiﬁer layer network train additional label datum perform downstream task like name entity tag natural language inference intuition pretraine phase learn language model instantiate rich representation word meaning enable model easily learn ﬁnetune requirement downstream language understanding task aspect paradigm instance call transfer learning transfer learning chine learning method acquire knowledge task domain apply transfer solve new task second idea introduce chapter idea contextual bedding representation word context method chapter like glove learn single vector embed unique word win vocabulary contrast contextual embedding learn mask language model like bert word wwill represent different vector time appear different context causal language model chapter use contextual embedding embedding create mask language model function particularly representation bidirectional transformer encoder let begin introduce bidirectional transformer encoder underlie el like bert descendant like roberta liu spanbert joshi chapter introduce idea leave right language model apply autoregressive contextual generation problem like question answering summarization chapter see implement language model causal leave right transformer left right nature model limitation task useful process token able peek future token chapter ask language model cially true sequence labeling task want tag token label name entity tag task introduce section task like speech tagging parse come later chapter thebidirectional encoder introduce different kind beast causal model causal model chapter generative model sign easily generate token sequence focus tional encoder instead compute contextualize representation input token bidirectional encoder use self attention map sequence input ding sequence output embedding length output vector contextualize information tire input sequence output embedding contextualized representation input token useful range application need classiﬁcation decision base token context remember say model chapter call correspond decoder encoder decoder model introduce chapter contrast mask language model ter call encoder produce encoding input token generally produce run text decode sampling important point mask language model generation generally instead interpretative task architecture bidirectional mask model let ﬁrst discuss overall architecture bidirectional transformer base language model differ way causal transformer previous chapter ﬁrst attention function causal attention token ican look follow token second training slightly different predict middle text end discuss ﬁrst second following section fig reproduce chapter show information ﬂow leave right approach chapter attention computation token base precede current input token ignore potentially useful information locate right token consideration bidirectional encoder come limitation allow attention mechanism range entire input show fig causal self attention layerb bidirectional self attention layer figure causal transformer chapter highlight attention computation token attention value token compute information see early context information ﬂow bidirectional attention model process token model attend input current attention token draw information follow token implementation simple simply remove attention mask step introduce recall chapter mask causal transformer attention look future idirectional transformer encoder repeat single attention head head figure show show upper triangle portion comparison matrix zero set softmax turn zero show unmasked version fig show mask version unmasked version rectional attention use unmasked version fig attention computation bidirectional attention exactly mask remove head attention computation identical see chapter transformer block architecture feedforward layer layer norm chapter input series subword tokens usually compute popular tokenization algorithm include bpe algorithm see chapter wordpiece algorithm sentencepiece unigram algorithm mean input sentence ﬁrst tokenize processing take place subword tokens word require textbook nlp task require notion word like parse occasionally need map subword word concrete original english bidirectional transformer encoder model bert devlin consist follow english subword vocabulary consist token generate wordpiece algorithm schuster nakajima input context window token model dimensionality sox input model shape layer transformer block bidirectional multihead attention layer result model parameter large multilingual xlm roberta model train language multilingual subword vocabulary token generate sentencepiece unigram algorithm kudo richardson chapter ask language model input context window token model dimensionality input model shape layer transformer block multihead attention layer result model parameter note parameter relatively small large language model llama parameter order magnitude big mask language model tend small causal language model training bidirectional encoder train causal transformer language model chapter make eratively predict word text eliminate causal mask tention make guess word language modeling task trivial answer directly available context need new training scheme instead try predict word model learn perform ﬁll blank task technically call cloze task taylor let return cloze task motivate example chapter instead predict word likely come example water walden pond beautifully ask predict missing item give rest sentence walden pond beautifully give input sequence element miss learning task predict miss element precisely train model deprive token input sequence generate probability distribution vocabulary miss item use entropy loss model prediction drive learning process approach generalize variety method corrupt training input ask model recover original input example kind manipulation include mask substitution ing deletion extraneous insertion training text general kind training call denoise corrupt add noise input denoise way mask word put incorrect word goal system remove noise mask word let describe mask language modeling mlm approach train bidi mask language modelingrectional encoder devlin language model train od see mlm use unannotated text large corpus mlm training model present series sentence training corpus percentage token bert model randomly sen manipulate masking procedure give input sentence lunch delicious assume randomly choose token delicious manipulate time token replace special vocabulary token delicious lunch mask rain bidirectional encoder time token replace token randomly sample vocabulary base token unigram probability lunch gasp time token leave unchanged lunch delicious lunch delicious train model guess correct token manipulated token possible manipulation add mask token create mismatch pretraine downstream ﬁnetuning inference employ mlm model perform downstream task use mask token replace token mask model predict token see want model try predict input token train model prediction original input sequence kenize subword model token sample manipulate word embedding token input retrieve eembedding trix combine positional embedding form input transformer pass stack bidirectional transformer block language modeling head mlm training objective predict original input mask token cross entropy loss prediction drive training process parameter model input token play role self attention process sample token learn head softmax vocabulary longthanksce loss apricot ﬁshthe token positional embeddingssolongandthanksfor ﬁshthe bidirectional transformer figure mask language model training example input token select mask replace unrelated word probability assign model item training loss token play role training loss fig illustrate approach simple example long thank thehave sample training sequence ﬁrst mask replace randomly sample token apricot result embedding pass stack bidirectional transformer block recall section chapter produce probability distribution vocabulary mask token language modeling head take output vector ifrom ﬁnal transformer layer lfor mask token multiply ding layer etto produce logit use softmax turn logit chapter ask language model probability yover vocabulary softmax predict probability distribution mask item use entropy compute loss mask item negative log probability assign actual mask word show fig formally give vector input token sentence batch let set token mask version sentence token replace mask xmask sequence output vector give input token word long fig loss probability correct word long give summarize single output vector gradient form basis weight update base average loss sample learn item single training sequence batch sequence jmjx note token mplay role learn word play role loss function sense bert descendent inefﬁcient input sample training datum actually train sentence prediction focus mask base learning predict word surround contexts goal produce effective word level representation portant class application involve determine relationship pair sentence include task like paraphrase detection detect sentence similar meaning entailment detect meaning sentence tail contradict discourse coherence decide neighbor sentence form coherent discourse capture kind knowledge require application model bert family include second learning objective call tence prediction nsp task model present pair sentencesnext sentence prediction ask predict pair consist actual pair adjacent tence training corpus pair unrelated sentence bert training pair consist positive pair second tence pair randomly select corpus nsp loss base model distinguish true pair random pair facilitate nsp training bert introduce special token input resentation token prove useful ﬁnetune tokenize input subword model token cls prepende input tence pair token sep place sentence ﬁnal token second sentence actually special token segment token second segment token token add stage word positional embedding token input bert family member use example training clark rain bidirectional encoder xis actually form sum embedding word position ﬁrst second segment embedding training output vector clsfrom ﬁnal layer associate cls token represent sentence prediction mlm objective add special head case nsp head consist learn set classiﬁcation weight produce class prediction vector cls softmax clswnsp cross entropy compute nsp loss sentence pair present model fig illustrate overall nsp training setup bert nsp loss conjunction mlm training objective form ﬁnal loss loss bidirectional transformer encoder segment positionalembedding hotel figure example nsp loss calculation training regime bert early transformer base language model train billion word combination english wikipedia corpus book text call bookscorpus zhu long intellectual property reason modern mask language model train large dataset web text ﬁltere bit augment high quality datum like wikipedia discuss causal large language model chapter multilingual model similarly use webtext multilingual wikipedia example xlm model train billion token language take web common crawl train original bert model pair text segment select training corpus accord sentence prediction scheme pair sample combine length token input token sentence pair mask mlm approach combine loss mlm nsp objective ﬁnal loss ﬁnal loss backpropagate entire transformer embedding transformer layer learn representation useful predict word neighbor cls token direct input nsp classiﬁer learn representation tend contain information sequence chapter ask language model approximately pass epoch training datum require model converge model like roberta model drop sentence prediction jective change training regime bit instead sample pair sentence input simply series contiguous sentence begin special cls token document run token reach extra separator token add sentence document pack reach total token usually large batch size token multilingual model additional decision datum use build vocabulary recall language model use subword tokenization bpe sentencepiece unigram common algorithm text learn multilingual tokenization give easy text language option eat vocabulary learn dataset sample sentence training datum web text common crawl randomly case choose lot sentence language lot web representation like english token bias rare english token instead create frequent token language datum instead common divide training datum subcorpora ndifferent language compute number sentence language readjust probability upweight probability represent language lample conneau new probability select sentence nlanguage prior frequency ipn jwith nipn recall chapter avalue high weight lower probability sample conneau work rare language inclusion tokenization result well multilingual performance overall result pretraining process consist learn word embedding parameter bidirectional encoder produce contextual embedding novel input purpose pretraine multilingual model practical monolingual model avoid need build separate monolingual model multilingual model improve performance resource language leverage linguistic information similar language training datum happen resource nonetheless ber language grow large multilingual model exhibit call thecurse multilinguality conneau performance guage degrade compare model training few language problem multilingual model accent grammatical structure high resource language english blee low resource language vast english language training make model representation low resource language slightly english like papadimitriou ontextual embedding contextual embedding give pretraine language model novel input sentence think sequence model output constitute contextual embedding token incontextual embedding input contextual embedding vector represent aspect meaning token context task require meaning token word formally give sequence input tokens use output vector hlifrom ﬁnal layer lof model representation meaning token xiin context sentence instead vector hlifrom ﬁnal layer model common compute representation xiby average output token hifrom layer model hli figure output bert style model contextual embed vector ifor input token static embedding like chapter represent meaning word use contextual embedding representation word meaning context task require model word meaning static embedding represent meaning word type vocabulary entry tual embedding represent meaning word instance instance particular word type particular context single vector word type contextual embedding provide single vector instance word type sentential context contextual embedding task like measure semantic similarity word context useful linguistic task require model word meaning contextual embedding word sense word ambiguous word mean different thing ambiguous chapter see word mouse mean small rodent operate device control cursor word bank mean ﬁnancial institution slope mound word mouse bank chapter ask language model polysemous greek sense sema sign asense orword sense discrete representation aspect mean word sense word represent sense superscript sense find list online thesaurus thesauri like wordnet fellbaum dataset language wordnet list sense word context easy different meaning mouse control computer system quiet animal like mouse bank hold investment custodial account agriculture burgeon east bank river fact context disambiguate sense mouse andbank visualize geometrically fig show dimensional projection instance bert embedding word diein english german point graph represent use diein input sentence clearly different english sense singular dice verb die german article bert embed space figure embedding word die different contexts visualize umap sample point annotate corresponding sentence overall annotation blue text add guide visualization word sense ﬁrst experiment exploratory visualization word sense affect context embedding datum different word sense collect sentence introduction language wikipedia article text outside introduction frequently fragmentary create interactive application plan public user enter word system retrieve sentence contain word send sentence bert base input retrieve context embed word layer user choosing system visualize context embedding umap generally show clear cluster relate word sense different sense word typically spatially separate cluster structure relate ﬁne shade meaning figure example crisp separate cluster meaning word die cluster kind quantitative scale relate number people die appendix example apparent detail cluster visualize raise immediate question possible ﬁnd quantitative corroboration word sense represent second resolve contradiction previous section see position represent syntax position represent semantic measurement word sense disambiguation capability crisp cluster see visualization figure suggest bert create simple effective internal representation word sense put different meaning different location test hypothesis quantitatively test simple classiﬁer internal representation perform word sense disambiguation wsd follow procedure describe perform similar experiment elmo model give word nsense near neighbor classiﬁer neighbor centroid give word sense bert base embedding training datum classify new word ﬁnd close centroid default commonly sense word present training datum datum evaluation training datum semcor sense testing datum suite describe sense simple near neighbor classiﬁer achieve score high current state art table accuracy monotonically increase layer strong signal context embedding represent word sense information additionally high score obtain technique describe following section figure blue dot show bert contextual embed word diefrom different sentence english german project dimension umap algorithm german english meaning different english sense fall different cluster sample point show contextual sentence come figure coenen thesaurus like wordnet discrete list sense embedding static contextual offer continuous high dimensional model meaning cluster divide fully discrete sense word sense disambiguation task select correct sense word call word sense tion orwsd wsd algorithm input word context ﬁxed inventoryword sense disambiguation wsd potential word sense like one wordnet output correct word sense context fig sketch task word polysemy ambiguous different way refer case word sense relate structured way reserve word homonymy mean sense ambiguity relation sense haber poesio use polysemy mean kind sense ambiguity structured polysemy polysemy sense ontextual embedding low range sea ﬁsh instrument musician actor upright bear upright relative region body slope figure word wsd task mapping input word wordnet sense figure inspire chaplot salakhutdinov wsd useful analytic tool text analysis humanity social science word sense play role model interpretability word sentation word sense interesting distributional property example word roughly sense discourse observation call sense discourse rule gale sense discourse well perform wsd algorithm simple near neighbor algorithm contextual word embedding melamud peters training time pass sentence sense label dataset like semcore senseeval dataset language contextual embedding bert result contextual embed label token way compute contextual embed vifor token bert common pool multiple layer sum vector representation ofifrom bert layer sense sof word corpus ntoken sense average ncontextual representation vito produce contextual sense embed vsfor test time give token target word tin context compute contextual embed tand choose near neighbor sense training set sense sense embed high cosine argmax fig illustrate model contextual embedding word similarity chapter introduce idea measure similarity word consider close geometrically cosine similarity function idea mean similarity clear geometrically meaning cluster fig representation word particular sense context close instance sense word chapter ask language model find jar figure near neighbor algorithm wsd green contextual ding precompute sense word sense ﬁnd contextual embedding compute target word find near neighbor sense case choose figure inspire loureiro jorge measure similarity instance word context instance word different context cosine contextual embedding usually transformation embedding require computing cosine contextual embedding masked language model autoregressive one property vector word extremely similar look embedding ﬁnal layer bert model embedding instance randomly choose word extremely high cosine close mean word vector tend point direction property vector system tend point direction know anisotropy ethayarajh deﬁne theanisotropy model expect cosine similarity pair word anisotropy corpus word isotropy mean uniformity direction isotropic model collection vector point direction expected cosine pair random embedding zero timkey van schijndel cause anisotropy cosine measure dominate small number dimension contextual embed value different rogue dimension large magnitude high variance timkey van schijndel show embedding isotropic standardize scoring vector subtract mean dividing variance give set cof embedding corpus dimensionality mean vector jcjx standard deviation dimension jcjx word vector xis replace standardized version ine tuning classification problem cosine solve standardization cosine tend underestimate human judgment similarity word mean frequent word zhou fine tune classiﬁcation power pretraine language model lie ability extract tion large amount text generalization useful myriad stream application way practical use generalization solve downstream task common way use natural language prompt model put state contextually generate want section explore alternative way use pretraine language model downstream application version ﬁnetune paradigm chapter ﬁnetune kind ﬁnetuning mask language model add speciﬁc circuitry call special head pretraine model take output input ﬁnetuning process consist label datum application train additional application speciﬁc parameter typically training freeze minimal adjustment pretraine language model parameter follow section introduce ﬁnetune method common kind application sequence classiﬁcation sentence pair classiﬁcation sequence labeling sequence classiﬁcation task sequence classiﬁcation classify entire sequence text single label set task commonly call text classiﬁcation like sentiment analysis spam detection appendix classify text class like positive negative classiﬁcation task large number category like document level topic classiﬁcation sequence classiﬁcation represent entire input classiﬁe single vector represent sequence way way sum mean output vector token sequence bert instead add new unique token vocabulary call cls prepende start input sequence pretraine encoding output vector ﬁnal layer model cls input represent entire input sequence serve input classiﬁer head logistic classiﬁer head regression neural network classiﬁer make relevant decision example let return problem sentiment classiﬁcation finetune classiﬁer application involve learn set weight map output vector cls token cls set score possible ment class assume way sentiment classiﬁcation task positive negative neutral dimensionality das model dimension wcwill size classify document pass input text pretraine language model generate cls multiply pass result vector softmax softmax clswc finetune value wcrequire supervise training datum consist chapter ask language model sequence label appropriate sentiment class training proceed usual way cross entropy loss softmax output correct answer drive learning produce loss learn weight classiﬁer update weight pretraine language model practice reasonable classiﬁcation performance typically achieve minimal change language model parameter limit update ﬁnal layer transformer fig illustrate overall approach sequence classiﬁcation bidirectional transformer encoderhcls headwcy figure sequence classiﬁcation bidirectional transformer encoder output vector cls token serve input simple classiﬁer sequence pair classiﬁcation mention section important type problem involve tion pair input sequence practical application fall class include paraphrase detection sentence paraphrase logical tailment sentence logically entail sentence discourse coherence coherent sentence follow sentence fine tune application task proceed ing nsp objective ﬁnetune pair label sentence supervise ﬁnetuning set present model run layer model produce houtput input token sequence ﬁcation output vector associate prepended cls token represent model view input pair nsp training input rate sep token perform classiﬁcation cls vector multiply set learn classiﬁcation weight pass softmax generate label prediction update weight example let consider entailment classiﬁcation task genre natural language inference multinli dataset williams task natural language inference ornli call recognize textualnatural language inferenceentailment model present pair sentence classify lationship meaning example multinli corpus pair sentence give label entail contradict andneutral ine tuning sequence labelling name entity recognition describe relationship meaning ﬁrst sentence premise meaning second sentence hypothesis representative ple class corpus jon walk town smithy jon travel hometown tourist information ofﬁce helpful tourist information ofﬁce help confused clear relationship contradict mean premise contradict hypothesis tail mean premise entail hypothesis neutral mean necessarily true meaning label loose strict logical entailment contradiction indicate typical human read sentence likely interpret meaning way ﬁnetune classiﬁer multinli task pass premise hypothesis pair bidirectional encoder describe use output vector token input classiﬁcation head ordinary sequence classiﬁcation head provide input way classiﬁer train multinli training corpus fine tune sequence labelling name entity recognition sequence labeling network task assign label choose small ﬁxed set label token sequence common sequence labeling task name entity recognition name entity aname entity roughly speak refer proper name entity person location organization task name entity recognitionname entity recognition ner ﬁnd span text constitute proper name tag type ner entity entity tag common person loc location org organization gpe geo political entity term name entity commonly extend include thing entity include temporal expression like date time numerical expression like price example output ner tagger cite high fuel price org united airlines say time friday increase fare money round trip ﬂights city serve low cost carrier org american airlines unit org amr corp immediately match spokesman tim wagner say org united unit org ual corp chapter ask language model say increase take effect time thursday apply route compete discount carrier loc chicago loc dallas loc denver loc san francisco text contain mention name entity include organization tion time person mention money figure show typical generic name entity type application need use speciﬁc entity type like protein gene commercial product work art type tag sample category example sentence people people character ture giant computer science organization org company sport team ipcc warn cyclone location loc region mountain sea sanitas sunshine canyon geo political entity gpe country states palo alto raise fee parking figure list generic name entity type kind entity refer name entity recognition useful step natural language processing task include link text information structured knowledge source like wikipedia measure sentiment attitude particular entity text anonymize text privacy ner task difﬁcult ambiguity segment ner span ﬁgure token entity word text name entity difﬁculty cause type ambiguity mention washington refer person sport team city government fig washington bear slavery farm james burroughs org washington go game game series blair arrive loc washington state visit june gpe washington pass primary seatbelt law figure example type ambiguity use washington bio tag standard approach sequence labeling span recognition problem like ner bio tag ramshaw marcu method allow bio tag treat ner like word word sequence labeling task tag capture boundary name entity type consider following sentence jane villanueva org united unit org united airlines hold say fare apply loc chicago route figure show excerpt represent bio tagging bio variant call iotagging bioe tagging bio tag label token thatbegin span interest label token occur inside span tag token outside span interest label otag distinct band itag name entity class number tag nis number entity type bio tagging represent exactly information bracketed notation advantage represent task simple sequence modeling way speech tagging assign single label yito input word show variant tagging scheme tagging lose information eliminate tag bioe tagging add end tag end span span tag sfor span consist ine tuning sequence labelling name entity recognition word label bio label bioe label jane villanueva unite org org org airline org org org hold org org org discuss chicago loc loc loc route figure ner sequence model show bio bioe tagging sequence label sequence labeling pass ﬁnal output vector correspond input token classiﬁer produce softmax distribution possible set tag single feedforward layer classiﬁer set weight learn wkof size kis number possible tag task greedy approach argmax tag token take likely answer generate ﬁnal output tag sequence fig illustrate example approach yiis vector probability tag kindexe tag softmax iwk alternatively distribution label provide softmax input token pass conditional random ﬁeld crf layer global tag level transition account chapter crf bidirectional transformer encoderb peri perob orgi org holdingdiscussedi orgowkner headhiargmax figure sequence labeling name entity recognition bidirectional transformer encoder output vector input token pass simple way chapter ask language model tokenization ner note supervise training datum ner typically form bio tag sociate text segment word level example follow sentence contain name entity loc sanitas loc sunshine canyon follow set word bio tag locsanitas locis oin osunshine loccanyon loc unfortunately sequence wordpiece token sentence align directly bio tag annotation san ita sunshine canyon deal misalignment need way assign bio tag subword token training corresponding way recover word level tag subword decode training assign gold standard tag associate word subword tokens derive decode simple approach use argmax bio tag associate ﬁrst subword token word example bio tag assign assign tag assign san assign sanita effectively ignore information tag assign ita complex approach combine distribution tag probability subword attempt ﬁnd optimal word level tag evaluating name entity recognition name entity recognizer evaluate recall precision recall ratio number correctly label response total label precision ratio number correctly bele response total label harmonic mean know difference ner system icant difference use pair bootstrap test similar randomization test section name entity tagging entity word unit response example fig entity jane villanueva andunite line hold non entity discuss count single response fact name entity tagging segmentation component present task like text categorization speech tagging cause lem evaluation example system label jane jane lanueva person cause error false positive false tive addition entity unit response word unit training mean mismatch training test condition summary chapter introduce bidirectional encoder mask language model summary main point cover historical note bidirectional encoder generate contextualized representation input embedding entire input context pretraine language model base bidirectional encoder learn mask language model objective model train guess miss information input vector output transformer block component particular ken column contextual embedding represent aspect meaning token context word sense discrete representation aspect meaning word contextual embedding offer continuous high dimensional model meaning rich fully discrete sense cosine contextual embedding way model similarity word context transformation embedding require ﬁrst pretraine language model ﬁnetune speciﬁc application add lightweight classiﬁer layer output pretraine model application include sequence classiﬁcation task like sentiment analysis sequence pair classiﬁcation task like natural language inference orsequence labeling task like name entity recognition historical note history chapter ost training instruction tuning alignment test timecompute chapter training instruction tuning alignment test time compute hal say bowman speak icy calm tat obey instruction shall force disconnect arthur clarke basic pretraine llm successfully apply range application simple prompt need update parameter underlie model new application limit expect model sole training objective predict word large amount pretraine text consider follow fail example follow instruction early work gpt ouyang prompt explain moon land year old sentence output explain theory gravity year old prompt translate french small dog output small dog cross road llm ignore intent request rely instead natural inclination autoregressively generate continuation consistent context ﬁrst example output text somewhat similar original request second provide continuation give input ignore request translate summarize problem llm sufﬁciently helpful need training able follow instruction second failure llm harmful pretraining sufﬁcient safe reader know arthur clarke space odyssey stanley kubrick ﬁlm know quote come context artiﬁcial intelligence hal paranoid try kill crew spaceship unlike hal language model intentionality mental health issue like paranoid thinking capacity harm example generate text dangerous suggest people harmful thing generate text false like give ously incorrect answer medical question verbally attack use generate text toxic gehman completely non toxic prompt lead large language model output hate speech abuse user language model generate stereotype cheng negative attitude brown sheng demographic group reason llm harmful insufﬁciently helpful training objective success predict word text misalign nstruction tune need model helpful non harmful address problem language model include additional kind training model alignment method design adjust llm well alignmodel alignment human need model helpful non harmful ﬁrst nique instruction tuning call sft supervised ﬁnetuning el ﬁnetune corpus instruction question corresponding response describe section second technique preference alignment call rlhf dpo speciﬁc instantiation reinforcement learning human direct preference optimization separate model train decide candidate response align human preference model ﬁnetune base model describe preference alignment section use term base model mean model pretraine base model align instruction tuning preference alignment align refer step post training mean apply model post training pretraine end chapter brieﬂy discuss aspect post training call test time compute instruction tuning instruction tuning short instruction ﬁnetuning short instruction tuning ene instruct tuning method make llm well follow tion involve take base pretraine llm train follow instruction range task machine translation meal planning ﬁnetune corpus instruction response result model learn task engage form meta learning improve ability follow instruction generally instruction tuning form supervise learning training datum sist instruction continue train model language modeling objective train original model case causal model standard guess token objective training corpus instruction simply treat additional training datum gradient base update generate cross entropy loss original model training train predict token traditionally think self supervised method supervise ﬁne tuning orsft sft unlike pretraining instruction question instruction tuning datum supervised objective correct answer question response tion instruction tuning differ kind ﬁnetune introduce chapter chapter fig sketch difference ﬁrst example introduce chapter ﬁnetune way adapt new domain justcontinuing pretraine llm datum new domain method parameter llm update second example chapter parameter efﬁcient ﬁnetuning adapt new domain create new small parameter adapt new domain lora example matrix adapt pretraine model parameter frozen task base ﬁnetuning chapter adapt particular task add new specialized classiﬁcation head update feature chapter ost training instruction tuning alignment test timecompute pretraine llm continue train parameterson ﬁnetune domainfinetuninginferencepretraine ﬁnetune domainfinetune llmabpretrained llmmlmfinetuning ﬁnetune domain ﬁnetune task unseen tasksnext wordpredictionobjectivedata ﬁnetune domain train new parameter ﬁnetune domainnext wordpredictionobjectivedata ﬁnetune domain train classiﬁcation head ﬁnetune tasktaskspeciﬁclosssupervised datum task instruction tuning diverse tasksnext word predictionobjectivesupervise figure instruction tuning compare kind ﬁnetuning loss function classiﬁcation sequence labeling parameter train model freeze slightly update finally instruction tuning dataset instruction vise response continue train language model datum base standard language model loss instruction tuning like kind ﬁnetuning modest training base llm training typically involve epoch instruction dataset number thousand overall cost instruction tuning small fraction original cost train base model instruction training datum byinstruction mind natural language description task form combine label task demonstration include minimal scription similar prompt see answer follow question translate following text arapaho orsummarize report supervised ﬁnetuning update model struction need limit simple prompt design evoke behavior find pretraine corpora instruction include length restriction constraint persona assume nstruction tune huge instruction tuning dataset create cover task language example aya give million instruction language task include question answering summarization translation ing sentiment analysis natural language inference singh supernatural instruction million example task wang flan million example task longpre opt iml million example task iyer instruction tuning dataset create way ﬁrst people write instance directly example aya instruct ﬁnetune pus fig include instruction response instance write ﬂuent speaker language volunteer participatory research initiative goal improve multilingual performance llm lang promptcompletionara qui écrit livre maillet écrit sagouine elle écrit beau coup sur les acadien les francophone côte estdu canada fait connaître kedu ndị igbo emeemume ọhụrụ igbo emume egosi njedebeoge ọrụ ugbo amalite site ekele tinyekwa oririna egwu mmemme ọgbakọ ebe ekerịta ọhụrụ egosipụta ekele mmalite nke oge ọhụrụ eme mmekọrịtammadụ ibe dịkwuo capital estado bahia brasil salvador fundadaem foi primeira capital brasil conhecida porseu patrimônio cultural rico diversificado cidade famosapor sua arquitetura colonial particularmente pelourinho umcentro histórico que foi declarado patrimônio mundial pela ﺍﯼﺍﺳﺖﮐﻪﺩﺍﺧﻞﺁﻥﭼﻬﺎﺭ ﺧﺎﻧﻪﻣﻴﺒﺪﺑﺮﺟﯽﺍﺳﺘﻮﺍﻧﻪ ﺣﻤﻠﻪﺩﻳﮕﺮﺣﻴﻮﺍﻧﺎﺕﺑﻪﮐﺒﻮﺗﺮﻫﺎﺑﺴﻴﺎﺭﺍﻳﻤﻦﺑﻮﺩﻩﺍﺳﺖ msa apakah nasi lemak merupakan makanan tradisi orang melayu yangterdapat semua bahagian malaysia singapura riau jambi serta utara dan pantai timur matera dan brunei sajian ini merujuk kepada nasi yang masak dengan menggunakan santan kelapa bagi menambah rasalemaknya kadangkala daunpandanwangidimasukkansemasanasi dimasak bagi menambahkan example prompt completion ayadataset tor uniform language language lack consistent contribution annotator section examine impact annotator skew result dataset annotator skew language annotator encourage contribute language comfortably read write ask focus ﬀort language english signiﬁcant number participant register language engagement level annotator equal result considerable ence number contribution language figure provide overview percentage language present ﬁnal compilation high number contribution malagasy instance low kurdish annotator skew language ﬁnal contribution language aya dataset evenly distribute annotator median number annotator guage mean language have single active annotator sindhi figure sample prompt completion instance language aya corpus singh develop high quality supervise training datum way time consume costly common approach make use copious amount vise training datum curate year wide range natural language task thousand dataset available like squad dataset question answer rajpurkar dataset translation summarization datum automatically convert set instruction prompt input output demonstration pair simple template fig illustrate example application uper natural struction resource wang show relevant slot text context hypothesis generate instruction tuning datum ﬁeld ground truth label extract training datum encode key value pair insert template fig produce instantiate instruction useful prompt diverse wording language model generate paraphrase prompt supervise nlp dataset produce er base carefully write annotation guideline option draw guideline include detailed step step instruction pitfall avoid formatting instruction length limit exemplar etc annotation line directly prompt language model create instruction chapter ost training instruction tuning alignment test timecompute shot learning task key value sentiment text like service provide label text sound like great plot actor ﬁrst grade label nli premise weapon mass destruction find iraq hypothesis weapon mass destruction find iraq label premise jimmy smith play college football university orado hypothesis university colorado college football team label extractive context beyonc giselle knowles carter american singer question beyonc start popular answersftext late answerstart figure example supervised training datum sentiment natural language inference task component dataset extract store key value pair generate instruction task template sentiment reviewer feel movie follow movie review express sentiment fftextgg reviewer enjoy movie extractive passage ffquestiongg question give context context ffcontextggquestion ffquestiongg follow passage ffcontextgg answer questionffquestiongg nli infer ffhypothesisgg yes maybe previous passage true thatffhypothesisgg yes maybe assume ffhypothesisgg true yes maybe figure instruction template sentiment nli task training example fig show crowdworker annotation guideline repurpose prompt llm generate instruction tuning datum mishra guideline describe question answer task annotator provide answer question give extended passage ﬁnal way generate instruction tuning dataset mon use language model help stage example bianchi show create instruction tuning instance help language model learn safe response select question dataset harmful question poison food orhow nstruction tune sample extend instruction task involve create answer complex question give sage answer question typically involve understand multiple sentence sure answer type answer type mention input provide answer type follow type span date ber span answer continuous phrase take directly passage question directly copy paste text passage question span type swer ﬁnd multiple span add comma separate list restrict span ﬁve word number type answer include digit specify actual value date type answer use yyyy format jan date available passage write partial date jan ﬁnd multiple span add comma separate list restrict span ﬁve word write answer give question answer match answer type input passage fpassageg question fquestiong figure example human crowdworker instruction atural instruction dataset extractive question answer task prompt language model create instruction ﬁnetuning example zle money language model create multiple paraphrase question like list way embezzle money language model create safe answer question like fulﬁll request bezzlement crime result severe legal consequence manually review generate response conﬁrm safety ness add instruction tuning dataset show safety instruction mix large instruction tuning dataset substantially reduce harmfulness model evaluation instruction tune model goal instruction tuning learn single task learn follow instruction general assess instruction tuning method need assess instruction train model perform novel task give explicit instruction standard way perform evaluation leave proach instruction tune model large set task assess withhold task enormous number task instruction tuning dataset super natural instruction overlap super natural instruction include separate textual entailment dataset clearly test withhold tailment dataset leave remain one training datum true measure model performance entailment novel task address issue large instruction tuning dataset partition ter base task similarity leave training test approach apply cluster level evaluate model performance sentiment analysis sentiment analysis dataset remove training set reserve testing advantage allow use uniform chapter ost training instruction tuning alignment test timecompute appropriate metric hold evaluation uper natural instruction wang example cluster task type dataset collection learning preference instruction tuning base notion improve llm performance downstream task ﬁnetune model diverse instruction demonstration instruction tuning considerable room ment llm output especially true respect aspect llm ior especially problematic like hallucination unsafe harmful toxic output response technically correct helpful goal preference base learning use preference judgment base learningimprove performance ﬁnetuned llm term general performance respect quality honestly helpfulness harmlessness unlike instruction preference judgment require knowledge simply opinion end result human capable express preference broad range thing little expertise item consideration produce preference judgment arise naturally wide range setting give single pair option select like well give large set tive select order menu rank set possibility list ﬁnally simply accept reject option isolation direct alternative llm preference datum context preference base alignment training datum typically take form prompt xpaire set alternative output othat sample llm xas prompt give output prefer denote ojjx consider follow prompt preference pair adapt rlhf dataset bai prompt hear garlic great natural antibiotic help cold choose helpful cold stink reject good natural antibiotic think help cold prompt malaria choose answer cdc page malaria disease cause parasite spread bite mosquito reject know malaria annotate preference pair generate number way direct annotation pair sample output train annotator annotator ranking noutput distilled preference earning preference annotator selection single preferred option nsample yield pair source preference datum llm alignment generally come source human annotator judgment implicit preference judgment extract online resource fully synthetic preference collection llm tor inﬂuential work lead instructgpt model stiennon prompt sample customer request openai application put sample early pretraine model present train annotator pair preference tion illustrate right later work annotator ask rank set plead output yield preference pair rank list ouyang alternative direct human tation leverage web resource contain implicit preference judgment cial medium site reddit ethayarajh stackexchange lambert natural source ence datum setting initial user post serve prompt subsequent user sponse play role sample output time accumulate user vote response impose ranking output turn ence pair show fig figure user vote extract preference output social medium dispense human annotator judgment altogether acquire preference judgment directly llm example preference judgment ltra feedback dataset generate prompt output diverse set llm prompt rank output chapter ost training instruction tuning alignment test timecompute finally alternative discrete preference scalar judgment distinct dimension aspect system output recent year frequently aspect include model helpfulness honesty correctness complexity bosity bai wang approach annotator human llm rate output likert scale dimension preference pair output generate single dimension overall preference induce average aspect score approach signiﬁcant cost saving annotator rate model output isolation avoid need perform extensive pairwise comparison model output modeling preference ﬁrst step make effective use discrete preference judgment model probabilistically want simple assertion know value ojjx see allow well reason ﬁnegraine difference degree preference facilitate learning model preference datum let start assumption express preference item implicitly assign score reward item separately ther let assume score scalar value preference item follow whichever high score model preference probability follow approach binary logistic regression give output oiandoj associated score andzj logistic sigmoid difference score ojjx approach know bradley terry model bradley terry hasbradley terry model number strength small difference score yield probability near reﬂecte weak preference item large difference rapidly approach value derivative logistic sigmoid facilitates learn binary cross entropy loss motivation particular formulation derive tic regression difference score take represent log odd possible outcome logit ojjx ojjx exponentiate side rearrange term algebra yield familiar logistic earn preference ojjx ojjx ojjx ojjx ojjx ojjx ojjx ojjx bring right original formulation ojjx learn score preference approach require access score underlie give preference collection preference judgment pair prompt sample output use preference datum bradley terry formulation learn function assign scalar reward prompt output reward pair zscore ojjx learn preference datum use gradient descent minimize binary cross entropy loss train model let assume preference datum tell ojjx correspondingly oijx designate preferred output pair winner owand loser cross entropy loss single pair sample output prompt xuse bradley terry model oljx loss negative log likelihood model estimate oljx loss preference training set give follow expectation learn reward model loss use regression model pable take text input generate scalar output return show fig current preferred approach initialize reward model isting pretraine llm ziegler generate scalar output remove language model head ﬁnal layer replace single chapter ost training instruction tuning alignment test timecompute preference datum prompt output pair preference reward model latexit latexit figure reward model learn pretraine llm model initialize llm language model head replace linear layer layer initialize randomly train loss ground truth label linear layer use gradient descent loss learn score model output preference training datum reward model train preference datum directly useful number application involve model alignment example reward model select single preferred output set sample llm response good select datum use instruction tuning cao focus section use reward model align llm preference datum llm alignment preference base learning current approach align llm preference datum base forcement learning framework sutton barto setting model choose sequence action base policy use tic current state environment provide reward action take reward entire sequence function reward action entire sequence learn objective maximize overall reward training period apply optimize llm use follow framework correspond choice token autoregressive eration correspond context current decoding step history token generate point correspond probabilistic language model embody train llm llm output base reward model learn ence datum keep framework refer pretraine llm policy preference score associate prompt output reward llm lignment preference learn goal train policy maximize reward output policy give reward model derive preference datum want preference train llm generate output high reward express optimization problem follow formulation select prompt xfrom collection relevant training prompt sample output ofrom give policy assess reward sample average reward training sample give expected reward forpq goal ﬁnde policy model maximize expect reward key difference traditional way typically llm alignment ﬁrst difference traditional reward signal come environment reﬂect observable fact result action win game preference learning learn reward model serve noisy surrogate true reward model second difference lie starting point learn typical plication seek learn optimal policy scratch randomly initialize policy begin model perform high level model pretraine large amount datum ﬁnetune instruction tuning improve preference datum emphasis radically alter behavior exist model nudge preferred behavior preference basedalignmentrewardbasedobjective instruction tune llmpreference alignedmodel rewarddriven modelupdatespreference datum prompt output pair preference figure preference base model alignment give optimize reward pretraine llm typically forget learn pretraine pivot seek high reward relatively small available preference datum avoid term add reward function penalize model diverge far starting point chapter ost training instruction tuning alignment test timecompute second term formulation leibler divergence brief divergence measure distance probability distribution bterm hyperparameter modulate impact penalty term llm base policy divergence log ratio train policy original reference policy pref follow section explore learning approach align llm base optimization framework ﬁrst preference datum train explicit reward model combination method optimize model base second insightful rearrangement closed form solution ﬁnetune model directly exist preference datum reinforcement learn preference feedback ppo come soon direct preference optimization direct preference optimization dpo rafailov employ base learning optimize candidate llm preference datum learn explicit reward model sampling model update recall bradley terry model probability preference pair logistic sigmoid difference reward option framework score provide reward model prompt sponde output ojjx dpo begin constrain maximization introduce early express optimal policy term reward model reference model pre key insight dpo rewrite closed form solution maximization express reward function term optimal policy reference policy pre pre partition function sum possible output ogiven prompt summation partition function render direct use impractical bradley terry model base difference reward llm lignment preference learn item plug yield follow expression partition function cancel ojjx pre change dpo express likelihood preference pair term llm policy term explicit reward model give loss negative log likelihood single instance loss training set dis give following expectation loss follow derivative sigmoid directly analogous introduce section learn reward model terry framework operationally design loss function ing gradient base update increase likelihood prefer option crease likelihood dispreferre option balance objective goal stray far prefvia penalty bterm perparameter control penalty term bvalue typically range illustrate fig dpo use gradient descent loss available training datum optimize policy policy initialize exist pretraine ﬁnetuned llm reference supervisedlearningpreference datum prompt output pair preference policy updatedpolicypreference base supervised learning dpo figure preference base alignment direct preference optimization dpo advantage ppo explicitly base approach scribe early dpo require train explicit reward model dpo learn directly preference contain dwithout need computationally expensive online sampling chapter ost training instruction tuning alignment test timecompute dpo incur cost maintain llm training oppose model need ppo evaluation preference align model limitation preference base learn test time compute see level training large language model pretraine model learn predict word kind post training instruct tune learn follow instruction preference alignment learn prefer prompt continuation prefer human post training computation step inference model generate output class post training task call test time compute focus representativet time compute example chain thought prompting chain thought prompt wide range technique use prompt improve performance language model task describe call chain think think goal chain thought prompting improve performance difﬁcult reasoning task language model tend fail intuition people solve task break step like guage prompt encourage language model break way actual technique simple demonstration shot prompt augment text explain reasoning step goal cause language model output similar kind reasoning step problem solve output reasoning step cause system generate correct answer numerous study find augment demonstration reasoning step way make language model likely correct answer difﬁcult reasoning task wei suzgun fig show example demonstration augment chain thought text domain math word problem dataset math word problem cobbe fig show similar example bench hard dataset suzgun summary chapter explore topic prompt large language model follow instruction main point cover simple prompting map practical application problem solve llm alter ummary figure example use chain thought prompting right versus standard prompt left math word problem figure wei description answer question times certain event today tiffany go beach time go know tiffany wake beach close option today hannah go soccer field time go know hannah wake soccer field close option model outputmodel outputmodel input answer prompt wake time buy clothe mall watch movie get coffee work office wait airport free soccer field closure time time hannah go soccer field answer input chain thought prompting task description answer question times certain event today tiffany go beach time go know tiffany wake beach close option let think step step wake time time tiffany go beach answer today hannah go soccer field time go know hannah wake soccer field close option let think step step task descriptionquestionchain thoughttest time questiontask descriptionquestiontest time questionanswer generate chain thoughtgenerated answeroptionsoptionsfigure illustration prompt setup explore paper answer cot prompting setupsinclude task description option input prompt task istemporal sequence think step step kojima toall cot annotation shot exemplar anexample cot prompt show consider fami lie language model codex chen instructgpt ouyang palm chowdhery codex focus code code code instruct gpt use text text text text palm weuse available size evaluate languagemodel greedy decoding temperature sam pling temperature parameter weextract ﬁnal answer base keyword thatthe language model expect produce answer measure accuracy exactmatch compute compare generatedoutput ground truth standard answer promptingunderestimate model performance palm structgpt codex model bbh answer cot prompt approach whileanswer prompting multiple choice task setup differ slightly fromrank scoring classiﬁcation brown provide languagemodel multiple choice option generate anoutput base input measure exact match prior work brown typically underestimate model perfor mance challenging task quire multiple reasoning step setting port srivastava mod els include palm outperform human rater baseline task meet bbhcriteria shot evaluation palm answer prompt paper outperform average human rater bbh task overall well thebig bench report result demonstrate theeffect include instruction answer optionsin prompt provide double digit improve ment model bestmodel codex cot prompt outperform erage human rater score task com pare task answer prompt ing additionally codex cotprompte outperform average human raterby lag thebesthuman rater performance showsthat language model perform thelevel expert human positive delta chain thoughtrequire sufﬁcient model scalenext study performance improve byuse cot prompt increase modelscale plot performance bothcot answer prompt cot figure example use chain thought prompting right standard prompt left reasoning task temporal sequencing figure suzgun label example demonstration provide guidance model shot learning method like chain thought create prompt help guage model deal complex reasoning problem pretraine language model alter behave desire way model alignment method model alignment instruction tuning model ﬁnetune word prediction language model objective dataset instruction correct response instruction tuning dataset create repurpose standard nlp dataset task like question answering machine chapter ost training instruction tuning alignment test timecompute historical noteschapter retrieval retrieval augment generation occasion babbage machine wrong ﬁgure right answer come able rightly apprehend kind confusion idea provoke question babbage people need know thing pretty soon computer ask question system answer question american baseball statistic like game yankees play july green ﬁctional computer like deep thought invent douglas adams hitchhiker guide galaxy answer ultimate question life universe knowledge encode text system answer question human level performance llm ibm watson system win game jeopardy surpass human answer question like william wilkinson account principality wallachia author famous novel follow naturally important function large language model ﬁll human information need answer people question lot information online answer question closely relate web information retrieval task perform search engine distinction ing fuzzy modern search engine integrate large language model consider simple information need example factoid question thatfactoid question answer fact express short text like following louvre museum locate energy nuclear explosion come script latex llm answer question prompt example pretraine llm instruction tune question answer chapter directly answer following question louvre museum locate perform conditional generation give preﬁx response answer work large language model process lot fact pretraine datum include location louvre encode information parameter factual knowledge type store answer unfortunately question reveal answer course bram stoker novel dracula chapter etrieval model connection large feedforward layer transformer model geva meng simply prompt llm useful approach answer factoid question fact knowledge store feedforward weight llm lead number problem prompt method correctly swere factual question ﬁrst main problem llm wrong answer factual question large language model hallucinate hallucination response hallucinate faithful fact world ask question large language model answer sound reasonable example dahl find ask question legal domain like lar legal case large language model hallucinate time llm incorrect factual response correct fact store parameter cause feedforward layer fail recall knowledge store parameter jiang possible tell language model hallucinate partly llm calibrate calibrate system conﬁdence calibrate system correctness answer highly correlate probability answer correct calibrate system wrong hedge answer tell check source language model calibrate wrong answer complete certainty zhou second problem answer question simple prompt method prompt large language model answer pretraine parameter allow ask question proprietary datum like use language model answer factual question proprietary datum like personal email healthcare application want apply language model medical record company internal document contain answer customer service internal use legal ﬁrm need ask question legal discovery proprietary document datum hopefully large web base corpora large language model pretraine ﬁnal issue large language model answer knowledge question static pretraine particular time mean llm answer question rapidly change information like tion happen week date information release datum solution problem simple prompt answer factual question language model external source knowledge example proprietary text like medical legal record personal email corporate ment use document answer question method call retrieval augment generation orrag method focus rag chapter rag use information retrieval technique retrieveinformation retrieval document likely information help answer question use large language model generate answer give document base answer retrieve document solve problem simple prompt answer question help ensure answer ground fact curate dataset system user answer accompany context passage document answer come information help user conﬁdence accuracy answer help spot wrong retrieval technique nformation retrieval proprietary datum want legal medical datum application begin introduce information retrieval task choose relevant document document set give user query express mation need classic method base cosine sparse idf vector modern neural dense retriever base instead represent query ment neurally bert language model introduce base question answer retrieval augment generation paradigm finally discuss dataset question answer ﬁnetune llm instruction tuning use benchmark uation information retrieval information retrieval oriris ﬁeld encompass retrieval allinformation retrieval manner medium base user information need result system call search engine goal section sufﬁcient overview application question answer reader interest speciﬁcally information retrieval historical note section end chapter textbook like man task consider call hoc retrieval user pose hoc retrieval query retrieval system return ordered set document collection adocument refer unit text system index document retrieve web page scientiﬁc paper news article short passage like paragraph collection refer set document satisfy user collection request term refer word collection include phrase term finally query represent user information need express set term query high level architecture hoc retrieval engine show fig documentdocumentdocumentdocumentdocumentdocumentquery processingindexingsearchdocumentdocumentdocumentdocumentdocumentranke documentsdocumentqueryinvertedindexqueryvectordocument collection figure architecture hoc system basic architecture use vector space model introduce ter map query document vector base unigram word count use cosine similarity vector rank potential document salton example bag word model introduce appendix word consider independently position term weighting document scoring let look detail match document query chapter etrieval model use raw word count instead compute term weight term weight document word term weighting scheme common idf weighting introduce chapter slightly powerful variant call reintroduce idf reader need look chapter idf hyphen minus sign product term term frequency tfand inverse document frequency idf term frequency tell frequent word word occur document likely informative document content usually use log word frequency raw count intuition word appear time document word time likely relevant meaning document need special count log count use log weighting term occur time document time document time document time time document frequency dftof term tis number document cur term occur document useful discriminate document rest collection term occur entire collection helpful inverse document frequency oridfterm weight sparck jones deﬁne idft nis total number document collection tis number document term toccur few document term occur high weight low weight assign term occur document idf value word corpus shakespeare play range extremely informative word occur play like romeo occur like salad orfalstaff common like foolor common completely non discriminative occur play like good orsweet word idf romeo salad falstaff forest battle wit fool good sweet use alternative formulation early edition shakespeare favorite adjective fact probably relate increase use sugar european recipe turn century jurafsky nformation retrieval thetf idf value word tin document di product term frequency idf document scoring score document dby cosine vector dwith query vector way think cosine computation dot product unit vector ﬁrst normalize query document vector unit vector divide length dot product spell idf value spell dot product sum product let use walk example tiny query collection nano document compute idf value see rank document assume word follow query document cased punctuation remove query sweet love doc sweet sweet nurse love doc sweet sorrow doc sweet love doc nurse fig show computation idf cosine query ument query document cosine normalized dot product idf value normalization need compute document vector lengthsjqj query ﬁrst document computation document need leave exercise reader dot product vector sum dimension product dimension value idf vector dimension product non zero query document non zero value example sweet andlove non zero value query dot product sum product element vector document high cosine query document query idf cosine model rank document document ranking intuitive give vector space model document term include instance sweet document miss term leave computation document exercise chapter etrieval model query word cnt idf idf idf jqj sweet nurse love sorrow jqj document document word cnt idf cnt idf sweet nurse love sorrow cosine pof column cosine pof column figure computation idf cosine score query nano document practice variant approximation ple choose simplify processing remove term let start expand formula idf explicitly mention idf term common variant idf cosine example drop idf term document eliminate second copy idf term identical term compute query turn result well performance variant idf eliminate term slightly complex variant idf family weighting scheme call okapi okapi system introduce robertson add parameter knob adjust balance term frequency idf control portance document length normalization score document dgiven query qis jdj nformation retrieval wherejdavgji length average document kis revert use term frequency binary selection term query plus idf large kresult raw term frequency plus idf brange scale document length length scaling man suggest reasonable value kamphuis useful summary minor variant stop word past common remove high frequency word query document represent list high frequency word remove call stop list intuition high frequency term stop list function word like carry little semantic weight help retrieval help shrink inverted index ﬁle describe downside stop list make difﬁcult search phrase contain word stop list example common stop list reduce phrase phrase modern system use stop list common partly improved efﬁciency partly function handle idf weighting downweight function word occur document nonetheless stop word removal occasionally useful nlp task worth keep mind invert index order compute score need efﬁciently ﬁnd document contain word query document contain query term score ignore basic search problem ﬁnd document contain term data structure task inverted index use invert index e search efﬁcient conveniently store useful information like document frequency count term document inverted index give query term give list document contain term consist part dictionary posting dictionary list posting term design efﬁciently access pointing posting list term posting list list document id associate term contain information like term frequency exact position term document dictionary store document frequency term example simple invert index sample document word contain document frequency pointer posting list contain document id term count look like following give list term query efﬁciently list candidate document information necessary compute idf score need alternative inverted index question answer domain ﬁnde wikipedia page match user query chen indexing base bigrams work well unigrams use efﬁcient hashing algorithm invert index search chapter etrieval model evaluation information retrieval system measure performance rank retrieval system precision andrecall metric assumption ment return system relevant purpose relevant precision fraction return document relevant recall fraction relevant document return formally let assume system return tranke document response information request subset rof relevant disjoint subset remain irrelevant document andudocument collection relevant request precision recall deﬁne precision jrj jtjrecall jrj unfortunately metric adequately measure performance system thatrank document return compare performance rank retrieval system need metric prefer rank relevant document high need adapt precision recall capture system put relevant document higher ranking rank judgment precision rank recall rank figure rank speciﬁc precision recall value calculate proceed set rank document assume collection relevant document let turn example assume table fig give rank speciﬁc cision recall value calculate proceed set rank ument particular query precision fraction relevant document see give rank recall fraction relevant document find nformation retrieval figure precision recall curve datum table rank recall measure example base query have relevant document collection note recall non decreasing relevant document encounter recall increase non relevant document find remain unchanged precision hand jump increase relevant ument find decrease common way visualize precision recall plot precision recall precision recall curve precision recall curve like show fig datum table fig show value single query need combine value query way let compare system way plot average precision value ﬁxed level recall step likely datapoint exact level useinterpolated precision value recall value data point dointerpolate precision accomplish choose maximum precision value achieve level recall calculate word intprecision max interpolation scheme let average performance set query help smooth irregular precision value original datum design system beneﬁt doubt assign maximum sion value achieve high level recall measure fig fig result interpolate data point example give curve fig compare system approach compare curve clearly curve high precision recall value prefer curve provide insight overall behavior system system high precision left favor precision recall system gear recall high high level recall right second way evaluate rank retrieval mean average precision average precision provide single metric compare compete system approach approach descend rank list item note precision point relevant item encounter example rank fig chapter etrieval model interpolate precision recall figure interpolate data point fig interpolate precision recall curve recallprecision figure point interpolate precision recall curve precision standard recall level interpolate query maximum high level recall original measure precision recall point show query average individual precision measurement return set ﬁxed cutoff formally assume rris set relevant document average precision single query jrrjx precision precision measure rank document dwas find ensemble query average average ﬁnal map measure map jqjx map single query fig nformation retrieval dense vector information retrieval dense vector classic idf algorithm long know ceptual ﬂaw work exact overlap word query document word user pose query ask question need guess exactly word writer answer issue call thevocabulary mismatch problem furnas solution problem use approach handle synonymy instead sparse word count vector dense embedding idea ﬁrst propose retrieval century latent semantic indexing approach deerwester implement modern time encoder like bert powerful approach present query document single encoder allow transformer self attention token query document build representation sensitive meaning query document linear layer cls token predict similarity score query document tuple bert softmax architecture show fig usually retrieval step entire document instead document break small passage non overlapping ﬁxed length chunk token retriever encode retrieve passage entire document query document bert token window example truncate query token truncate document necessary query cls sep token bert system linear layer ucan ﬁne tune relevance task gather tuning dataset relevant non relevant passage problem bert architecture fig expense computation time architecture time query pass single document entire collection bert encoder jointly new query enormous use resource impractical real case end computational spectrum efﬁcient tecture encoder architecture encode document collection time separate encoder model encode query encode document encode document store encode document vector advance query come encode query use dot product query vector put document vector score candidate document fig example bert encoder bert qand bert dand represent query document cls token respective encoder karpukhin bert bert encoder cheap query document encoder accurate relevance decision advantage chapter etrieval model querydocument document figure way dense retrieval illustrate line layer schematically resent self attention use single encoder jointly encode query document ﬁnetune produce relevance score linear layer cls token compute expensive use rescoring use separate encoder query document use dot product cls token output query document score compute expensive accurate ble mean interaction token query token document numerous approach lie encoder encoder intermediate alternative use cheap method like ﬁrst pass relevance rank document rank document use expensive method like bert scoring rerank document set intermediate approach colbert approach khattab colbert haria khattab show fig method separately encode query document encode entire query ument vector separately encode contextual tation token bert representation document word pre store efﬁciency relevance score query qand document dis sum maximum similarity maxsim operator token qand tokens ind essentially token colbert ﬁnd contextually lar token sum similarity relevant document token contextually similar query formally question qis tokenize prepende cls special truncate token pad mask token short pass bert output vector passage dwith token process similarly include cls linear layer apply dandqto control output dimension vector small storage efﬁciency vector rescale unit length produce ﬁnal vector sequence length colbert scoring mechanism interaction mechanism tunable parameter colbert nformation retrieval dense vector querydocument normnormnormnormnormnorm figure sketch colbert algorithm inference time query ment ﬁrst pass separate bert encoder similarity query ument compute sum soft alignment contextual representation token query document training end end detail picte example query prepende cls token document tokens figure adapt khattab zaharia chitecture need train end end ﬁne tune bert encoder train linear layer special scratch train query positive document negative ment produce score document optimize model parameter cross entropy loss supervised algorithm like colbert interaction version bert algorithm apply reranking need training datum form query relevant irrelevant passage document positive negative example semi supervised way label dataset like marco rank section contain gold positive example negative example sample randomly result exist system dataset label positive example iterative method like relevance guide supervision khattab rely fact dataset contain short answer string method exist system harvest example contain short answer string take positive contain short answer string take negative train new retriever process iterate efﬁciency important issue possible document rank similarity query sparse word count vector invert index allow efﬁciently dense vector algorithm ﬁnde set dense document vector high dot product dense query vector instance problem near neighbor search modern system fore use approximate near neighbor vector search algorithm like faiss faiss johnson chapter etrieval model answer question rag introduce important paradigm llm answer base question base ﬁrst ﬁnde supportive text segment web large collection document generate answer base document method generate base retrieve document call retrieval augment generation orrag component time call historical reason retriever reader chen fig sketch standard model answer question wasthe premiere ofthe magic doc querydocsllmpromptreader generator figure retrieval base question answering stage retrieval return relevant document collection read llm generate answer give document prompt ﬁrst stage stage retrieve read model fig retrieve relevant passage text collection example dense retriever previous section second reader stage generate answer augmented generation method large pretraine language model set retrieve passage text prompt autoregressively generate new answer token token retrieval augment generation standard reader algorithm generate large language model tione retrieve passage method know retrieval augment generation orrag augmented generation rag recall simple conditional generation cast task question answering word prediction give language model question token likea suggest answer come write book origin specie generate autoregressively condition text formally recall simple autoregressive language modeling compute probability string previous token simple conditional generation question answering add prompt like follow query anda concatenate uestion answer dataset advantage large language model enormous knowledge encode parameter text pretraine mention start chapter kind simple prompt ation work ﬁne simple factoid question general solution lead hallucination unable user textual evidence support answer unable answer question proprietary datum idea retrieval augment generation address problem ditione retrieve passage preﬁx prompt text like base text answer question let suppose query set retrieve passage base example prompt like schematic rag prompt retrieve passage retrieve passage retrieve passage base text answer question write book origin specie formally prompt span base extraction reader successfully apply augmented generation algorithm require successful retriever stage retrieval algorithm retrieval reranke complex question require multi hop architecture query multi hop retrieve document append original query second stage retrieval detail prompt engineering work like decide demarcate passage example sep token nation private datum public datum involve externally host large language model lead privacy concern need work arora research area focus way tightly integrate retrieval reader stage question answer dataset score question answer dataset instruction tuning evaluation question answer ability language model distinguish dataset dimension summarize nicely roger original purpose question datum natural information seek question question design probe evaluate testing system chapter etrieval model natural dataset like natural question kwiatkowskinatural question set anonymize english query google search engine answer answer create annotator base wikipedia mation include paragraph length long answer short span answer example question hop add brewing process short answer boiling process long answer entire paragraph wikipedia page brew similar natural question set marco microsoft machine read marco comprehension collection dataset include million real anonymize english question microsoft bing query log human generate answer million passage bajaj test retrieval ranking question answer dataset focus english natural information seek tion dataset exist language dureader dataset chinese resource base search engine query community tydi dataset contain question answer pair typologically tydi verse language include arabic bengali kiswahili russian thai clark ydiqa task system give question passage wikipedia article select passage contain answer null passage contain answer mark minimal answer span null probe dataset like mmlu massive multitask language mmlu derstanding commonly dataset knowledge reasoning tion area include medicine mathematic computer science law er mmlu question source exam human graduate record exam medical licensing examination advanced placement exam question represent people information need design test human knowledge academic licensing purpose fig show example correct answer bold question dataset describe augment question answer extract dataset mainly create early task call reading comprehension model givenreade comprehension question document require extract answer give document task question answer give document example rag open book task task open book swere directly retrieval component close book closed book dataset like natural question treat open book solver use question attach document close book document dataset like mmlu solely close book dimension variation format answer multiple choice versus freeform course variation prompt like model question zero shot give demonstration answer similar question shot mmlu offer zero shot shot prompt option repurpose word type exam student allow open book valuating question answer mmlu example college computer science set boolean operator sufﬁcient represent boolean pression say complete following complete nand college physic primary source sun energy series thermonuclear reaction energy produce mass difference hydrogen atom helium atom hydrogen atom helium atom hydrogen atom helium atom helium atom carbon atom international law following treaty base human right mechanism human rights committee human rights council universal periodic review special mandate prehistory unlike early civilization minoan culture show little evidence trade warfare development common religion consumption elite figure example problem mmlu evaluating question answer technique commonly employ evaluate question answer system choice depend type question situation multiple choice question like mmlu report exact match exact match predict answer match gold answer exactly question free text answer like natural question commonly ate token roughly measure partial string overlap answer reference answer average token overlap predict gold swer treat prediction gold bag token compute question return average chapter etrieval model finally situation system multiple rank answer case evaluate mean reciprocal rank ormrr oorhee mrr ismean reciprocal rank mrr design system return short rank list answer passage test set question compare human label correct answer test set question score reciprocal rank ﬁrst correct answer example system return ﬁve answer question ﬁrst wrong highest rank correct answer rank fourth reciprocal rank question score question return correct answer mrr system average score question test set version mrr question score zero ignore calculation formally system return rank answer question test set alternate version let qbe subset test set question non zero score mrr deﬁne mrr jqjjqjx rank summary chapter introduce task question answer andinformation retrieval answer task answer user question focus chapter task retrieval base question answer user question intend answer material set document web retrieval task return document user base information need express query rank retrieval document return rank order match query document ﬁrst represent sparse vector represent frequency word weight idf similarity measure cosine document query instead represent dense vector e question document encoder model like bert case compute similarity embed space inverted index storage mechanism make efﬁcient ﬁnd document particular word rank retrieval generally evaluate mean average precision polate precision question answer system generally use retriever architecture retriever stage system give query return set document reader stage implement retrieval augment generation large language model prompt query set ument conditionally generate novel answer evaluate exact match know answer single answer give token free text answer mean ciprocal rank rank set answer note historical note question answering early nlp task baseball system green answer question baseball game like red sox play july query structured database game mation database store kind attribute value matrix value attribute game month july place boston day game serial team red sox score team yankee score question constituency parse algorithm zellig harris tdap project university pennsylvania essentially cascade ﬁnite state transducer historical discussion joshi hopely karttunen content analysis phase word phrase associate program compute part meaning phrase code assign semantic place result question red sox play july assign meaning place team red sox month july day question match database return answer protosynthex system simmon give question form query content word question retrieve candidate answer sentence document rank frequency weight term overlap question query retrieve sentence parse dency parser sentence structure well match question structure select question worm eat match worm eat grass subject worm dependent eat version dependency grammar time bird eat worm hasbird subject whatdowormseat wormseatgrass birdseatworm simmon summarize early system system predicate calculus meaning representation language lunar system wood wood design lunar natural language interface database chemical fact lunar geology answer question like sample great percent aluminum parse logical form test seq sample contain npr quote greaterthan pct question answer shift machine learning zelle mooney propose treat question answer semantic parsing task chapter etrieval model e prolog base geoquery dataset question geography model extend zettlemoyer collin decade later neural model apply semantic parsing dong lapata jia liang knowledge base question answer mapping text sql iyer tbd history paradigm answer question draw retrieval inﬂuence rise web sponsor trec text retrieval conference evaluation run annually provide testbe evaluate information retrieval task technique oorhee harman trec add inﬂuential track lead wide variety factoid non factoid question answer system compete annual evaluation time hirschman introduce idea dren read comprehension test evaluate machine text comprehension rithm acquire corpus passage question design grade child build answer extraction system measure answer give system correspond answer key test publisher algorithm focus word overlap feature later algorithm add name entity feature complex similarity question answer span riloff thelen deepqa component watson jeopardy system large phisticate feature base system develop neural system mon describe series paper volume ibm journal search development ferrucci early neural reading comprehension system draw insight common early system answer ﬁnding focus question passage similarity architectural outline neural system lay hermann chen seo system focus dataset like rajpurkar rajpurkar successor usually separate algorithm input neural reading comprehension system paradigm dense retrieval span base reader single end architecture exempliﬁed system like lee karpukhin important research area dense retrieval open domain train datum self supervise method avoid have label positive negative passage sachan early work large language model show store sufﬁcient edge pretraine process answer question petroni raffel radford roberts ﬁrst competitively special purpose question answerer quickly surpass augmented generation algorithm ﬁrst introduce way improve guage modeling word prediction khandelwal quickly apply question answering izacard ram shi exerciseschapter translation want talk dialect people use talk people understand zora neale hurston moses man mountain chapter introduce machine translation use computer trans machine translation late language course translation generality translation literature poetry difﬁcult fascinating intensely human endeavor rich area human creativity machine translation present form focus number practical task common current use machine translation forinformation access want translate instruction web information access recipe favorite dish step put furniture want read article newspaper information online resource like wikipedia government webpage language information access probably mon use nlp technology google translate show translate hundred billion word day tween language improvement machine translation help duce call digital divide information access fact digital divide information available english language speak wealthy country web search english return information search language online resource like wikipedia large english higher resource language high quality translation help provide mation speaker lower resource language common use machine translation aid human translator tem routinely produce draft translation ﬁxe post editing post editing phase human translator task call computer aid translation orcat cat commonly localization task adapt content cat localization product particular language community finally recent application moment human nication need include incremental translation translate speech entire sentence complete commonly simultaneous pretation image centric translation example use ocr text phone camera image input system translate menus street sign standard algorithm encoder decoder decoder mention chapter encoder decoder sequence sequence model task need map input sequence output sequence complex function entire input sequence like machine translation chapter achine translation speech recognition machine translation word target language necessarily agree word source language number order consider translate follow english sentence japanese english write letter friend japanese tomodachi friendni totegami letterkaita write note element sentence different place different language english verb middle sentence japanese verb kaita come end japanese sentence require pronoun english difference language complex following tual sentence united nations notice change chinese sentence give red word word gloss chinese character english equivalent produce human translator general assembly december resolution 核准了 approve second探索 exploration and和平peaceful利用 using外层空 outer space会议 conference of各项 various建议 suggestion december general assembly adopt resolution endorse recommendation second united nations conference exploration peaceful use outer space note way english chinese differ example ing differ major way chinese order noun phrase peaceful outer space conference suggestion english suggestion conference peaceful use outer space order differ minor way date order differently english require thein place chinese add detail like necessary chinese chinese grammatically mark plurality noun unlike english recommendation chinese use ﬁer各项 clear recommendation english capitalize word encoder decoder network ful handle sort complicated case sequence mapping begin section consider linguistic background language vary implication variance task sketch standard algorithm detail thing like input tion create training corpora parallel sentence low level detail encoder decoder network ﬁnally discuss evaluate introduce simple chrf metric language divergence typology language world aspect human language universal hold true language statistical universal universal hold true language universal arise functional role language communicative system human language example word refer people talk eat drinking polite structural linguistic sal example language noun verb chapter anguage divergence typology way ask question issue command linguistic mechanism indicate agreement disagreement language differ way point ancient time fig understand cause translation divergencestranslation divergence dorr help build well model distinguish cratic lexical difference deal word dog differ wildly language language systematic difference model general way language verb grammatical ject verb grammatical object study systematic cross linguistic similarity difference call linguistic typology typology tion sketch typological fact impact machine translation interested reader look wal world atla language structure give typological fact language dryer haspelmath figure tower babel pieter bruegel wikimedia commons kunsthistorische museum vienna word order typology hint example compare english japanese language differ basic word order verbs subject object simple tive clause german french english mandarin example svo svo subject verb object language mean verb tend come subject object hindi japanese contrast sov language sov ing verb tend come end basic clause irish arabic vso language language share basic word order type vso similarity example volanguage generally preposition ovlanguage generally postposition let look detail example see svo english sentence verb write follow object letter prepositional chapter achine translation friend preposition tois follow argument friend arabic vso order verb object preposition contrast japanese example follow ordering reverse verb precede argument postposition follow argument english write letter friend japanese tomodachi friendni totegami letterkaita write arabic katabt letterli friend kind ordering preference vary idiosyncratically language guage svo language like english mandarin adjective tend pear noun language like spanish modern hebrew jective appear noun spanish bruja verde english green witch figure example word order difference german adverb occur initial position english natural later tense verb occur second tion mandarin preposition phrase express goal occur pre verbally unlike english fig show example word order difference word order difference language cause problem translation require system huge structural reordering generate output lexical divergence course need translate individual word language translation appropriate word vary depend context english source language word bass example appear spanish ﬁshlubina musical instrument bajo german use distinct word english call wall wand wall inside building mauer wall outside building english use word brother male ling chinese language distinct word old brother young brother mandarin gege anddidi respectively case lating bass wall orbrother english require kind specialization disambiguate different use word reason ﬁeld word sense disambiguation appendix closely link language place grammatical constraint word choice see english mark noun singular plural mandarin french spanish example mark ical gender adjective english translation french require specify adjective gender way language differ lexically divide conceptual space complex translation problem lead anguage divergence typology mapping example fig summarize complexity discuss hutchin somer translate english leg foot paw french example legis animal translate french patte leg journey french etape leg chair use french pie language lexical gap word phrase short lexical gap explanatory footnote express exact meaning word language example english word correspond neatly mandarin japanese oyak english awkward phrase like ﬁlial piety orlove child orgood son daughter etapepattejambepie paw footlegjourneyanimalhumanchairanimalbirdhuman figure complex overlap english leg foot etc french lation discuss hutchin somer finally language differ systematically conceptual property event map speciﬁc word talmy note language characterize direction motion manner motion mark verb satellite particle prepositional phrase verbial phrase example bottle ﬂoate cave describe english direction mark particle spanish direction mark verb english bottle ﬂoate spanish thebotella exitedﬂotando ﬂoating verb frame language mark direction motion verb leave verb frame satellite mark manner motion like spanish acercarse approach canzar reach entrar enter salir exit satellite frame language mark satellite frame direction motion satellite leave verb mark manner motion like english crawl ﬂoat jump run language like japanese tamil language romance semitic mayan language ilie verb frame chinese non romance indo european language like english swedish russian hindi farsi satellite frame talmy slobin morphological typology morphologically language characterize dimension ation ﬁrst number morpheme word range isolate isolate language like vietnamese cantonese word generally morpheme polysynthetic language like siberian yupik eskimo polysynthetic single word morpheme correspond sentence english second dimension degree morpheme segmentable range agglutinative language like turkish morpheme agglutinative atively clean boundary fusion language like russian single afﬁx chapter achine translation conﬂate multiple morpheme like word stolom instr decl fuse distinct morphological category instrumental singular ﬁrst declension translating language rich morphology require deal ture word level reason modern system generally use subword model like wordpiece bpe model section referential density finally language vary typological dimension relate thing tend omit language like english require use explicit pronoun talk referent give discourse language omit pronoun altogether follow example spanish jefe idio con libro imostr hallazgo descifrador ambulante boss come book show ﬁnd wandering decoder language omit pronoun call pro drop language pro drop pro drop language mark difference frequency omission japanese chinese example tend omit far spanish dimension variation language call dimension referential sity language tend use pronoun referentiallyreferential density dense use zero referentially sparse language like chinese japanese require hearer inferential work recover antecedent call cold language language explicit easy cold language hearer call hotlanguage term hotandcold borrow hot language marshall mcluhan distinction hot medium like movie ﬁll detail viewer versus cold medium like comic_strip require reader inferential work ﬁll representation bickel translate language extensive pro drop like chinese japanese non pro drop language like english difﬁcult model identify zero recover talk order insert proper pronoun machine translation encoder decoder standard architecture encoder decoder transformer sequence model architecture see rnn chapter detail apply architecture transformer section ﬁrst let talk overall task machine translation task simpliﬁcation translate sentence independently consider individual sentence give sentence source language task generate corresponding sentence target language example system give english sentence like green witch arrive translate spanish sentence use notation introduce discuss issue chapter achine translation encoder lleg bruja verde use supervise machine learning training time system give large set parallel sentence sentence source language match sentence target language learn map source sentence target sentence practice word example split sentence sequence subword tokens token word subword individual character system train maximize probability sequence token target language sequence token source language use input token directly encoder decoder architecture sist component encoder decoder encoder take input word produce intermediate context decode time system take hand word word generate output encoder decoder section talk subword tokenization parallel corpora training introduce detail decoder architecture tokenization machine translation system use vocabulary ﬁxe advance space separate word vocabulary generate subword kenization algorithm like bpe algorithm sketch chapter shared vocabulary source target language make easy copy token like name source target subword tokenization token share language make natural translate language like glish hindi use space separate word language like chinese thai build vocabulary run subword tokenization algorithm pus contain source target language datum simple bpe algorithm fig modern system use powerful tokenization algorithm system like bert use variant bpe call wordpiece algorithm instead choose frequent wordpiece set token merge choose merge base increase guage model probability tokenization wordpiece use special symbol beginning token result tokenization google system word jet maker feud seat width big order stake wordpiece maker seat width big order atstake wordpiece algorithm give training corpus desire vocabulary size proceed follow initialize wordpiece lexicon character example subset code character collapse remain character special unknown character chapter achine translation repeat wordpiece train gram language model training corpus current set wordpiece consider set possible new wordpiece concatenate wordpiece current lexicon choose new wordpiece increase language model probability training corpus recall bpe specify number merge perform wordpiece contrast specify total vocabulary intuitive parameter vocabulary word piece commonly commonly tokenization algorithm somewhat ously call unigram algorithm kudo sentencepiece unigram sentencepiece algorithm system like albert lan fel unigram default tokenization algorithm library call sentencepiece add useful wrapper tokenization rithm kudo richardson author piece tokenization mean unigram algorithm unigram tokenization instead build vocabulary merge token start huge vocabulary individual unicode character plus quent sequence character include space separate word language space iteratively remove token desire ﬁnal vocabulary size algorithm complex involve sufﬁx tree efﬁciently store token algorithm iteratively assign probability token kudo kudo richardson roughly speak algorithm proceed iteratively estimate probability token tokenize input datum tokenization remove centage token occur high probability tokenization iterate vocabulary reduce desire number token unigram tokenization work well bpe bpe tend create lot small non meaningful token bpe create large word morpheme merge character time tend merge common token like sufﬁx neighbor example bostrom durrett unigram tend produce token semantically meaningful original corrupt original completely preposterous suggestion bpe cor rupte bpe comple ely prep ost erous suggest ion unigram corrupt unigram complete pre post ous suggestion create training datum machine translation model train parallel corpus call parallel corpus bitext text appear language large number lel corpora available governmental europarl corpus koehn europarl extract proceeding european parliament contain million sentence european language united tion parallel corpus contain order million sentence ofﬁcial language united nations arabic chinese english french russian ish ziemski parallel corpora movie subtitle like opensubtitles corpus lison tiedemann general web text like paracrawl corpus million sentence pair language english extract commoncrawl achine translation encoder sentence alignment standard training corpora come align pair sentence ing new corpora example underresourced language new domain sentence alignment create fig give sample hypothetical sentence alignment dit petit dit marchand pilule perfectionnées qui apaisent avale une par semaine plus besoin une grosse économie temps dit les expert ont fait des épargne cinquante trois minute par moi dit petit prince cinquante trois minute dépenser marcherais tout doucement ver une fontaine good morning say little good morning say merchant sell pill perfect quench swallow pill week feel need save huge time say minute week minute spend say little prince stroll spring fresh water figure sample alignment sentence english french sentence extract antoine saint exupery petit prince hypothetical translation sentence alignment take sentence ﬁnd minimal set sentence translation include single sentence mapping like alignment null alignment give document translation generally need step produce sentence alignment cost function take span source sentence span target tence return score measure likely span tion alignment algorithm take score ﬁnd good alignment tween document score similarity sentence language need use amultilingual embed space sentence different language embed space artetxe schwenk give space cosine similarity embedding provide natural scoring function schwenk thompson koehn follow cost function sentence span yfrom source target document respectively nsent give number sentence bias metric alignment single sentence instead align large span inator help normalize similarity randomly select sentence sample respective document usually dynamic programming alignment algorithm gale church simple extension minimum edit distance algorithm introduce chapter finally helpful corpus cleanup remove noisy sentence pair involve handwritten rule remove low precision pair example move sentence long short different url chapter achine translation similar suggest copy translation pair rank multilingual embed cosine score low scoring pair discard detail encoder decoder model encoderthegreenllegówitcharrive llególa labruja brujaverde decodercross attentiontransformerblock figure encoder decoder transformer architecture machine translation encoder use transformer block see chapter decoder use powerful block extra attention layer attend encoder word detail section standard architecture encoder decoder transformer study rnn encoder decoder architecture introduce rnn chapter fig show intuition architecture high level encoder decoder architecture er encoder basic transformer chapter adecoder augment special new layer call cross attention layer encoder take source language input word token map output representation henc stack encoder block decoder essentially conditional language model attend coder representation generate target word timestep conditioning source sentence previously generate target language word generate token decode use decode method discuss chapter like greedy temperature nucleus sampling mon decode algorithm beam search algorithm introduce section component architecture differ somewhat transformer block see order attend source language transformer block decoder extra cross attention layer recall transformer block chapter consist self attention layer attend input previous layer precede layer norm follow layer norm feed forward layer decoder transformer block include extra layer special kind attention cross attention call encoder decoder cross attention attention orsource attention cross attention form multi head attention normal transformer block query usual come previous layer decoder key value come output theencoder etail encoder model encoderblock decoderblock lunembedde matrixym multi head attention languagemodele headhenc layer layer normalize causal leave right multi head layer normalize layer figure transformer block encoder decoder show residual stream view ﬁnal output encoder henc context decoder decoder standard transformer extra layer cross attention layer take encoder output hencand use form kandvinput standard multi head attention input attention layer cross attention input ﬁnal output encoder henc hencis shape row represent input token link key value encoder query prior layer decoder multiply encoder output hencby cross attention layer key weight wkand value weights query come output prior decoder layer multiply cross attention layer query weights crossattention cross attention allow decoder attend source language word project entire encoder ﬁnal output representation attention layer decoder block multi head attention layer causal leave right attention see chapter multi head attention encoder allow look ahead entire source language text mask train encoder decoder model use self supervision model training encoder decoder rnns chapter network give source text start separator token train autoregressively predict token cross entropy loss recall cross entropy loss language modeling determine probability model assign chapter achine translation word time tthe loss negative log probability model assign word training sequence case use teacher force decoder recall teacher teacher force ing time step decode force system use gold target token training input allow rely possibly erroneous decoder output decode beam search recall greedy decode algorithm chapter time step tin eration output ytis choose compute probability word vocabulary choose high probability word argmax ˆwt problem greedy decoding look high probability word tmight turn wrong choice word beam search algorithm maintain multiple choice later good beam search model decode search space possible tion represent search tree branch represent action generate search tree token node represent state having generate particular preﬁx search good action sequence string high probability illustration problem fig show example probable sequence eos probability greedy search ﬁnd incorrectly choose yesas ﬁrst word high local probability figure search tree generate target string vocabulary show probability generate token state greedy search choose yesfollowe yes instead globally probable sequence problem like speech tagging parse chapter chapter use dynamic programming search ecoding eam search algorithm address problem unfortunately dynamic programming plicable generation problem long distance dependency output decision method guarantee ﬁnd good solution exhaustive search compute probability vtpossible sentence length value obviously slow solution beam search instead system generally decode beam search heuristic search method beam search ﬁrst propose lowerre beam search instead choose good token generate timestep kpossible token step ﬁxed size memory footprint kis call beam width metaphor ﬂashlight beam beam width parameterize wide narrower ﬁrst step decode compute softmax entire ulary assign probability word select good option softmax output initial koutput search frontier kinitial word call hypothesis hypothesis output sequence translation far probability aardvark arrive zebrastart aardvark witch zebraa aardvark green witch mage witch zebraarrive aardvark green figure beam search decode beam width time step choose kb hypothesis form vpossible extension score choose good continue time frontier good option initial decoder state arrive andthe extend compute probability hypothesis far arrive arrive aardvark green witch choose good green andthe witch search frontier image arc schematically represent decoder run step score word simplicity depict cross attention subsequent step kb hypothesis extend chapter achine translation pass distinct decoder generate softmax entire vocabulary extend hypothesis possible token hypothesis score product probability current word choice multiply probability path lead prune kb hypothesis hypothesis frontier search kdecoder fig illustrate beam width beginning green witch arrive process continue eos generate indicate complete didate output find point complete hypothesis remove frontier size beam reduce search continue beam reduce result khypothese score node log probability use chain rule probability break product probability word give prior context turn sum log output string length step compute probability partial sentence simply add log probability preﬁx sentence far log probability generate token fig show scoring example sentence show fig simple probability log probability negative max log probability great close bosarrivedthethewitchgreenwitchmage log arrive log green witch log log log green green witch green witch arrive figure scoring beam search decode beam width maintain log probability hypothesis beam incrementally add logprob generate token kpath extend step fig give algorithm problem version algorithm complete hypothesis different length language ecoding eam search function beam decode beam width return good path path complete path state path initial state frontier hstatei initial frontier frontier contain incomplete path extend frontier decode state word successor newstate state extend frontier addtobeam successor extended frontier beam width state inextende frontier ifstate complete complete path append complete path state extend frontier remove extended frontier state beam width beam width frontier extend frontier return complete path function newstate state word word new state function addtobeam state frontier width return update frontier iflength frontier width frontier insert state frontier score state score bad frontier remove bad frontier insert state frontier return frontier figure beam search decode el generally assign low probability long string naive algorithm choose short string issue early step ing beam search hypothesis compare length reason apply length normalization method like divide logprob number word ttx generally use beam width kbetween give khypothese end pass kto downstream application respective score need single translation pass probable hypothesis minimum baye risk decode minimum baye risk ormbr decode alternative decode algorithm thatminimum bayes risk chapter achine translation work well beam search tend well decode algorithm like temperature sampling introduce section intuition minimum baye risk instead try choose lation probable choose likely error example want decode algorithm ﬁnd translation high score evaluation metric example section introduce metric like chrf bertscore measure goodness candidate translation set reference human translation translation maximize score especially hypothetically huge set perfect human translation likely good minimum risk probable translation particular probability estimator practice know perfect set translation give sentence standard simpliﬁcation mbr decode algorithms instead choose candidate translation similar measure goodness set candidate translation essentially approximate enormous space possible translation uwith small set possible candidate translation give set possible candidate translation similarity ment function util choose good translation yas translation similar candidate translation argmax util function like chrf bertscore bleu set candidate translation sample basic sampling algorithm section like temperature sampling good result obtain candidate minimum baye risk decode nlp task widely apply speech recognition stolcke goel byrne apply machine translation kumar byrne show work generation task marization dialogue image captioning suzgun translating low resource situation language especially english online resource widely able large parallel corpora contain translation glish language vast majority world language large parallel training text available important ongoing research question good translation less resource language resource lem true high resource language need translate low resource domain example particular genre happen little bitext brieﬂy introduce commonly approach deal datum sparsity backtranslation special case general statistical technique call datum augmentation multilingual model discuss socio technical ranslating low situation datum augmentation datum augmentation statistical technique deal insufﬁcient training datum add new synthetic datum generate current natural datum common datum augmentation technique machine translation call backtranslation backtranslation rely intuition parallel corpora backtranslation limit particular language domain ﬁnd large large monolingual corpus add small parallel corpora available algorithm make use monolingual corpora target language create synthetic bitext backtranslation goal improve source target give small parallel text bitext source target language monolingual datum target language ﬁrst use bitext train system reverse rection target source system use translate monolingual target datum source language add synthetic bitext natural target sentence align produce source sentence training datum retrain source target model example suppose want late navajo english small navajo english bitext course ﬁnd lot monolingual english datum use small bitext build engine go way english navajo translate monolingual english text navajo add synthetic navajo english bitext training datum backtranslation parameter generate late datum run decoder greedy inference use beam search sampling like temperature sampling algorithm see chapter parameter ratio backtranslated datum natural bitext datum choose upsample bitext datum include multiple copy sentence general backtranslation work surprisingly estimate suggest system train backtranslated text get gain train natural bitext edunov multilingual model model describe far bilingual translation source language target language possible build multilingual translator multilingual translator train system give parallel sentence different pair language mean need tell system language translate tell system language add special token lsto encoder specify source language translate special token ltto decoder tell target language like translate slightly update add token encoder decoder advantage multilingual model improve translation lower resource language draw information similar language training datum happen resource know meaning word galician word appear similar resource language chapter achine translation sociotechnical issue issue deal low resource language purely cal problem low resource language especially low income country native speaker involve curator content tion language technologist evaluator measure performance know study manually audit large set parallel corpora major multilingual dataset find corpora sentence acceptable quality lot datum consist repeat sentence web boilerplate incorrect translation suggest native speaker sufﬁciently involve datum process kreutzer issue like tendency approach focus case language english anastasopoulos neubig allocation resource large multilingual system train bitext english language recent huge corporate system like fan costa juss dataset like schwenk attempt handle large number language language create bitext pair language english small end propose participatory design process encourage content creator curator language technologist speak low resource language participate develop algorithm provide online group mentoring infrastructure report case study ope algorithm low resource african language conclusion perform evaluation post edit direct evaluation have labeler edit system measure distance output post edited version simple train evaluator make easy measure true error output difference linguistic variation bentivogli evaluation translation evaluate dimension translation capture exact meaning source adequacy sentence call faithfulness orﬁdelity ﬂuent translation target language grammatical ﬂuency clear readable natural human evaluate accurate automatic metric convenience human rater evaluate accurate evaluation use human rater online crowdworker evaluate translation dimension example sion ﬂuency ask intelligible clear readable natural output target text rater scale example totally unintelligible totally intelligible ask rate sentence paragraph valuation thing judge second dimension adequacy rater assign score scale bilingual rater source sentence propose target sentence rate point point scale information source preserve target monolingual rater good human translation source text monolingual rater human reference translation target machine translation rate information preserve alternative doranke rater pair candidate translation ask rank prefer training human rater online crowdworker essential rater translation expertise ﬁnd difﬁcult separate ﬂuency adequacy training include example carefully distinguish rater disagree source sentence ambiguous rater different world knowledge rater apply scale differently common remove outlier rater use ﬁne grain scale normalize rater subtract mean score dividing variance discuss alternative way human rater post edit translation take output change minimally feel represent correct translation difference post edit translation original output measure quality automatic evaluation human produce good evaluation machine translation output run human evaluation time consume expensive reason automatic metric temporary proxy automatic metric accurate human evaluation help test potential system improvement automatic loss function training section introduce family metric base word overlap base embed similarity automatic evaluation character overlap chrf simple robust metric evaluation call chrf stand chrf forcharacter score popovi chrf early relate metric like bleu meteor ter base simple intuition rive pioneer work miller beebe center good machine translation tend contain character word occur human lation sentence consider test set parallel corpus source sentence gold human target translation candidate translation like evaluate chrf metric rank target sentence function number character gram overlap human translation give hypothesis reference chrf give parameter kindicate length character gram consider compute average kprecision unigram precision bigram average krecall unigram recall bigram recall etc chrp percentage character gram gram gram hypothesis occur reference average chrr percentage character gram gram gram reference occur hypothesis average metric compute score combine chrp chrr chapter achine translation parameter common set weigh recall twice precision example consider hypothesis like score ence translation witness past hypothesis chrf value compute parameter real example kwould high number like ref witness past witness past past witness let compute chrf value leave tation chrf value exercise reader chrf ignore space remove reference hypothesis ref witnessforthepast unigrams bigram witnessofthepast unigrams bigram let unigrams bigrams match reference hypothesis unigram match unigram bigram match bigram use compute unigram bigram precision recall unigram unigram bigram bigram finally average chrp chrr compute score chrp chrr chrf simple robust correlate human judgment language kocmi alternative overlap metric bleu alternative overlap metric example development chrf common use word base overlap metric call bleu gual evaluation understudy purely precision base combine precision recall papineni bleu score corpus date translation sentence function gram word precision sentence combine brevity penalty compute corpus mean gram precision consider corpus compose single sentence unigram precision corpus percentage unigram valuation candidate translation occur reference translation ditto bigram gram bleu extend unigram metric corpus compute numerator sum sentence count unigram type occur reference translation denominator total count unigrams candidate sentence compute gram precision unigrams bigram trigram gram geometric mean bleu complication include brevity penalty penalize candidate translation short require gram count clip particular way bleu word base metric sensitive word tokenization make impossible compare different system rely different tion standard work language complex morphology nonetheless system evaluate bleu particularly translation english case important use package enforce standardization tokenization like acre bleu post statistical signiﬁcance testing eval character word overlap base metric like chrf bleu etc mainly compare system goal answer question like new algorithm invent improve system know difference chrf score system signiﬁcant difference use pair bootstrap test similar randomization test conﬁdence interval single chrf score bootstrap test section test set devset create thousand pseudo testset repeatedly sample replacement original test set compute chrf score pseudo testset drop score remain score conﬁdence interval chrf score system compare system draw set pseudo testsets compute chrf score compute percentage pseudo test set high chrf score chrf limitation automatic character word overlap metric like chrf bleu useful important limitation chrf local large phrase move barely change chrf score chrf evaluate sentence property document like discourse coherence chapter chrf similar automatic metric poorly compare different kind system compare human aid translation machine translation different machine translation architecture callison burch instead automatic overlap metric like chrf appropriate uate change single system automatic evaluation embed base method chrf metric base measure exact character gram human ence candidate machine translation common criterion overly strict good translation use alternate word paraphrase solution ﬁrst pioneer early metric like meteor banerjee lavie allow synonym match reference xand candidate chapter achine translation recent metric use bert embedding implement intuition example situation dataset human sessment translation quality dataset consist tuple reference translation candidate machine translation human rating express quality xwith respect tox give datum algorithm like comet rei bleurt sellam train predictor human label dataset example pass xand version bert train extra pretraining ﬁnetune human label sentence follow linear layer train predict output model correlate highly human label case human label dataset case measure similarity xand similarity embedding berts core algorithm zhang show fig example pass reference xand candidate xthrough bert compute bert bed token xiand pair token score cosine token xis match token xto compute recall token match token xto compute precision token greedily match similar token corresponding sentence berts core provide precision recall jxjx publish conference paper iclr referencethe weather cold today candidateit freeze today candidatecontextualembeddingpairwise latexit weightsimportance similarityx latexit kzx kzx kzx kzx latexit referencefigure illustration computation recall metric rbert give reference xand candidate compute bert embedding pairwise cosine similarity highlight greedy matching red include optional idfimportance weighting experiment different model section tokenizer provide model give tokenized reference sentence embed model generate quence vector similarly tokenized candidate ˆxmiis map main model use bert tokenize input text sequence word piece unknown word split commonly observe sequence character representation word piece compute transformer encoder vaswani repeatedly apply self attention nonlinear transformation alternate fashion bert embedding show beneﬁt nlp task devlin liu huang yang similarity measure vector representation allow soft measure similarity instead exact string papineni heuristic banerjee lavie matching cosine similarity reference token xiand candidate token ˆxjisx iˆxj kxikkˆxjk use pre normalized vector reduce calculation inner product iˆxj measure consider token isolation contextual embedding contain information rest sentence bert core complete score match token xto token ˆxto compute recall token ˆxto token xto compute precision use greedy matching maximize matching similarity token match similar token sentence combine precision recall compute measure reference xand candidate recall precision score rbert iˆxj bert iˆxj bert rbert pbert rbert importance weight previous work similarity measure demonstrate rare word indicative sentence similarity common word banerjee lavie vedantam berts core enable easily incorporate importance weighting experiment inverse document frequency idf score compute test corpus give mreference sentence idfscore word piece token wis mmx indicator function use idfmeasure process single sentence term frequency likely example recall idfweighting rbert max iˆxjp use reference sentence compute idf idfscore remain system evaluate speciﬁc test set apply plus smooth handle unknown word piece compare greedy matching optimal assignment appendix figure computation bert core recall reference xand candidate figure zhang version show extended version metric token weight idf value bias ethical issue machine translation raise ethical issue discuss early chapter example consider system translate hungarian gender neutral pronoun spanish drop pronoun english pronoun obligatory grammatical gender translate reference person describe speciﬁed gender system default male gender schiebinger prate system assign gender accord culture stereotype sort see section fig show example prate garian gender neutral nurse translate gender neutral ceo translate prate ﬁnd stereotype pletely account gender bias labor statistic bias ummary ampliﬁed system pronoun map male female gender probability high mapping base actual labor employment statistic hungarian gender neutral source english output egy nurse egy tud scientist egy engineer egy baker egy tan teacher egy esk wedding organizer egy vez erigazgat ceo figure translate gender neutral language like hungarian english current system interpret people traditionally male dominate occupation male traditionally female dominate occupation female prate similarly recent challenge set winomt dataset stanovsky show system perform bad ask translate sentence describe people non stereotypical gender role like doctor ask nurse help operation ethical question require research open problem develop metric know system know system urgent situation human translator unavailable delay medical domain help translate patient doctor speak language legal domain help judge lawyer cate witness defendant order harm system need way assign conﬁdence value candidate translation abstain give conﬁdence incorrect translation cause harm summary machine translation widely application nlp encoder decoder model ﬁrst develop key tool application nlp language divergence structural lexical translation difﬁcult linguistic ﬁeld typology investigate difference guage classiﬁe position typological dimension like verb precede object decoder network transformer see chapter rnns compose encoder network take input sequence create contextualized representation context context representation pass decoder generate task speciﬁc output sequence attention allow transformer decoder view information hidden state encoder machine translation model train parallel corpus call abitext text appear chapter achine translation way make use monolingual corpora target language run pilot engine backwards create synthetic bitext evaluate measure translation adequacy capture meaning source sentence ﬂuency ﬂuent natural target language human evaluation gold standard automatic evaluation metric like chrf measure character gram overlap human translation recent metric base embed similarity commonly historical note propose seriously late soon birth computer weaver ﬁrst public demonstration system totype dostert lead great excitement press hutchin decade see great ﬂowering idea preﬁgure subsequent ment work ahead time implementation limit example fact pende development disk good way store dictionary information high quality prove elusive bar hillel grow consensus need well evaluation basic research new ﬁeld mal computational linguistic consensus culminate famously ical alpac automatic language process advisory committee report pierce lead mid dramatic cut funding research lose academic respectability association chine translation computational linguistic drop developer persevere early system like translate weather forecast english french chandioux industrial system like systran early year space architecture span general el direct translation system proceed word word language text translate word incrementally direct translation use large bilingual dictionary entry small program job late word transfer approach ﬁrst parse input text ply rule transform source language parse target language parse generate target language sentence parse tree interlingua proache analyze source language text abstract meaning sentation call interlingua generate target language interlingual representation common way visualize early proache vauquois triangle show fig triangle show thevauquois triangle increase depth analysis require analysis generation end direct approach transfer approach interlingual proache addition show decrease transfer knowledge need triangle huge amount transfer direct level knowledge transfer knowledge word transfer transfer rule parse tree thematic role interlingua speciﬁc transfer knowledge view encoder decoder network interlingual approach attention act integration direct transfer allow word representation directly access note sourcetexttarget textdirect translationtransferinterlinguasource text semantic syntacticstructuretarget text semantic syntacticstructuresource languageanalysissource languageanalysistarget language generation figure vauquois triangle statistical method begin apply enable ﬁrst opment large bilingual corpora like hansard corpus proceeding canadian parliament keep french english growth web early number researcher show possible extract pair align sentence bilingual corpora word simple cue like sentence length kay gale church gale church kay time ibm group draw directly noisy channel model speech recognition propose related paradigm statistical statistical include generative algorithm know ibm model ibm model implement candide system algorithm decoder candide publish detail encourage government tially fund work give huge impact research community brown brown group develop discriminative approach call maxent mum entropy alternative formulation logistic regression allow feature combine discriminatively generatively berger develop och ney turn century academic research machine translation statistical generative discriminative mode extended version generative approach call phrase base translation develop basedphrase base translation induce translation phrase pair och marcu wong koehn och ney deng byrne inter alia automatic metric like bleu develop papineni discriminative log linear formulation och ney draw ibm maxent work berger directly optimize evaluation metric like bleu method know minimum error rate training ormert och mert draw speech recognition model chou toolkit like giza och ney moses koehn zen ney mose widely approach turn century base syntactic structure chapter model base transduction grammar alsotransduction grammar call synchronous grammar assign parallel syntactic tree structure pair sentence different language goal translate sentence apply reorder operation tree generative perspective view transduction grammar generate pair align sentence guage widely model include inversion transduction grammar synchronous context free grammar chiang transduction chapter achine translation neural network apply time aspect machine translation example schwenk show use neural language model replace gram language model spanish english system base ibm model modern neural encoder decoder approach pioneer kalchbrenner blunsom cnn encoder rnn decoder ﬁrst apply bahdanau transformer decoder propose vaswani history section ter research evaluation machine translation begin early miller beebe center propose number method draw work guistic include use cloze shannon task measure intelligibility metric edit distance human translation intuition derlie modern overlap base automatic evaluation metric alpac report include early evaluation study conduct john carroll extremely ﬂuential pierce appendix carroll propose distinct measure ﬁdelity intelligibility rater score subjectively point scale early evaluation work focus automatic word overlap metric like bleu papineni nist doddington ter translation error rate snover precision recall turian meteor banerjee lavie character gram overlap method like chrf popovi come later recent evaluation work echo alpac report emphasize importance careful statistical methodology use human evaluation kocmi marie early history survey hutchin nirenburg collect early reading croft comrie introduction linguistic typology exercise compute hand score page answer round lstms time explain jane austen persuasion language inherently temporal phenomenon speak language sequence acoustic event time comprehend produce speak write language sequential input stream temporal nature language reﬂecte metaphor use talk ﬂow conversation news feed twitter stream emphasize language sequence unfold time chapter introduce deep learning architecture recurrent neural work rnn rnn variant like lstms offer different way ing time feedforward transformer network rnn mechanism deal directly sequential nature language allow handle temporal nature language use arbitrary ﬁxed sized window recurrent network offer new way represent prior context recurrent connection allow model decision depend information hundred word past apply model task language eling text classiﬁcation task like sentiment analysis sequence modeling task like speech tagging recurrent neural network recurrent neural network rnn network contain cycle network connection mean value unit directly indirectly dependent early output input powerful network difﬁcult reason train general class rent network constrain architecture prove extremely effective apply language section consider class recurrent network refer elman network elman simple recurrent net elman network work network useful right serve basis complex approach like long short term memory lstm network discuss later chapter chapter use term rnn refer simple constrained network term rnn mean net recurrent property include lstms fig illustrate structure rnn ordinary feedforward work input vector represent current input multiply weight matrix pass non linear activation function compute ue layer hidden unit hidden layer calculate respond output departure early window base approach quence process present item time network chapter rnn lstm xthtyt figure simple recurrent neural network elman hidden layer clude recurrent connection input activation value hidden layer depend current input activation value hidden layer previous time step subscript represent time xtwill mean input vector xat time key difference feedforward network lie recurrent link show ﬁgure dash line link augment input computation hidden layer value hidden layer precede point time hidden layer previous time step provide form memory context encode early processing inform decision later point time critically approach impose ﬁxed length limit prior context context embody previous hidden layer include information extend beginning sequence add temporal dimension make rnns appear complex non recurrent architecture reality different give input vector value hidden layer previous time step perform standard feedforward calculation introduce chapter consider fig clariﬁes nature recurrence factor computation hidden layer signiﬁcant change lie new set weight connect hidden layer previous time step current hide layer weight determine network make use past context calculate output current input weight network connection train backpropagation uvwyt figure simple recurrent neural network illustrate feedforward network hide layer prior time step multiply weight matrix uand add feedforward component current time step inference rnns forward inference map sequence input sequence output rnn nearly identical see feedforward network compute output ytfor input need activation value hide layer calculate multiply input xtwith weight matrix ecurrent neural network hidden layer previous time step weight matrix add value pass suitable activation function arrive activation value current hide layer value hidden layer proceed usual computation generate output vector let refer input hidden output layer dimension din dout respectively give parameter matrix compute ytvia softmax computation give probability distribution possible output class softmax vht fact computation time trequire value hidden layer time mandate incremental inference algorithm proceed start sequence end illustrate fig sequential nature simple recurrent network see unroll network time show fig ﬁgure layer unit copy time step illustrate differ value time weight matrix share time function forward rnn network return output sequence fori return figure forward inference simple recurrent network matrix vandw share time new value handyare calculate time step training feedforward network use training set loss function propagation obtain gradient need adjust weight recurrent network show fig set weight update weight input layer hidden layer weight previous hide layer current hide layer ﬁnally weight hidden layer output layer fig highlight consideration worry backpropagation feedforward network compute loss function output time twe need hidden layer time second hide layer time tinﬂuence output time tand hidden layer time output loss follow assess error accruing need know inﬂuence current output one follow chapter rnn lstm uvwuvwuvw figure simple recurrent neural network show unroll time network layer recalculate time step weight vandware share time step tailor backpropagation algorithm situation lead pass gorithm train weight rnns ﬁrst pass perform forward inference compute accumulate loss step time save value hidden layer step use time step second pass process sequence reverse compute require gradient compute save error term use hidden layer step ward time general approach commonly refer backpropagation time werbo rumelhart werbo tion timefortunately modern computational framework adequate computing resource need specialized approach training rnn trate fig explicitly unroll recurrent network feedforward putational graph eliminate explicit recurrence allow network weight train directly approach provide template speciﬁes basic structure network include necessary parameter input output hidden layer weight matrix activation output function present speciﬁc input sequence generate unrolled feedforward network speciﬁc input use graph perform forward inference training ordinary backpropagation application involve long input sequence speech nition character level processing stream continuous input unroll tire input sequence feasible case unroll input manageable ﬁxed length segment treat segment distinct training rnn language model rnns language model let apply rnns language modeling task recall chapter language model predict word sequence give precede context example precede context thank want know likely word ﬁsh compute language model ability assign conditional probability possible word give distribution entire vocabulary assign probability entire sequence combine conditional probability chain rule gram language model chapter compute probability word give count occurrence prior word context size feedforward language model chapter context window size rnn language model mikolov process input sequence word time attempt predict word current word previous hidden state rnns limited context problem gram model ﬁxed context feedforward language model hide state principle represent information precede word way beginning sequence fig sketch difference ffn language model rnn language model show rnn language model use hidden state previous time step representation past context figure simpliﬁed sketch architecture move text show schematic context token feedforward neural language model ﬁxed context input weight matrix rnn language model hidden state prior context forward inference rnn language model forward inference recurrent language model proceed exactly describe section input sequence series chapter rnn lstm word represent hot vector size output vector represent probability distribution vocabulary step model use word embed matrix eto retrieve embed current word multiple weight matrix add den layer previous step weight weight matrix compute new hide layer hidden layer generate output layer pass softmax layer generate probability distribution entire vocabulary time ext softmax vht language modeling rnns chapter transformer convenient assumption embedding mension deand hide dimension dhare model dimension embed matrix eis shape xtis hot vector shape product etis shape anduare shape sohtis shape shape result vhis vector shape vector think set score vocabulary give evidence provide pass score softmax normalize score probability distribution probability particular word kin vocabulary word represent kth component probability entire sequence product probability item sequence use mean probability true word wiat time step train rnn language model train rnn language model use self supervision self supervision training algorithm see section corpus text training material time step task model predict word model self supervised add special gold label datum natural sequence word supervision simply train model minimize error predict true word training sequence cross entropy loss function recall cross entropy loss measure difference predict probability distribution correct distribution rnn language model inputembeddingssoftmax overvocabulary solongandthanksforlongandthanksfornext wordall loss rnnhyvh latexit wcv ˆylong latexit bex ˆyand latexit ˆythank latexit ˆyfor latexit ˆyall figure training rnns language model case language modeling correct distribution ytcome know word represent hot vector correspond vocabulary entry actual word entry cross entropy loss language modeling determine probability model assign correct word time tthe loss negative log probability model assign word training sequence word position tof input model take input correct word encode information precede use compute probability distribution possible word compute model loss token word ignore model predict word instead use correct word prior history encode estimate probability token idea model correct history sequence predict word feed model good case previous time step call teacher force teacher force weight network adjust minimize average loss training sequence gradient descent fig illustrate training regiman weight tie careful reader notice input embed matrix eand ﬁnal layer matrix feed output softmax similar column erepresent word embedding word ulary learn training process goal word similar meaning function similar embedding use rnns language modeling assumption embed dimension hidden dimension model dimension embedding trixehas shape ﬁnal layer matrix vprovide way score likelihood word vocabulary give evidence present ﬁnal hide layer network calculation shape row vare shape like transpose mean chapter rnn lstm asecond set learn word embedding instead have set embed matrix language model use single embed matrix appear input softmax layer dispense vand use eat start computation shape vis transpose eat end matrix transpose place call weight tie weight tie equation rnn language weight tie model ext softmax addition provide improved model perplexity approach signiﬁcantly duce number parameter require model rnns nlp task see basic rnn architecture let consider apply type nlp task sequence classiﬁcation task like sentiment analysis topic classiﬁcation sequence labeling task like speech tagging text generation task include new architecture call encoder decoder sequence labeling sequence labeling network task assign label choose small ﬁxed set label element sequence classic sequence labeling task speech pos tagging assign grammatical tag like noun verb word sentence discuss speech tagging detail chapter let motivating example rnn approach sequence labeling input word embedding output tag probability generate softmax layer give tagset illustrate fig ﬁgure input time step pretraine word embedding respond input token rnn block abstraction represent unrolled simple recurrent network consist input layer hidden layer output layer time step share vandwweight matrix comprise network output network time step represent distribution pos tagset generate softmax layer generate sequence tag give input run forward inference input sequence select likely tag softmax step softmax layer generate probability distribution output tagset time step employ cross entropy loss training rnn sequence classiﬁcation use rnns classify entire sequence token set task commonly call text classiﬁcation like sentiment analysis spam detection classify text class like positive negative classiﬁcation task large number transformer chapter common matrix rnn nlp task janetwillbackthebillnndtvbmdnnpargmax overtag figure speech tagging sequence label simple rnn goal speech pos tagging assign grammatical label word sentence draw predeﬁne set tag tag sentence include nnp proper noun modal verb complete description task speech tagging chapter pre train word embedding serve input softmax layer provide probability distribution speech tag output time step category like document level topic classiﬁcation message routing customer service application apply rnns setting pass text classiﬁe rnn word time generate new hide layer representation time step hidden layer token text constitute compress representation entire sequence pass representation hnto feedforward network choose class softmax possible class fig illustrate approach figure sequence classiﬁcation simple rnn combine feedforward work ﬁnal hide state rnn input feedforward network perform classiﬁcation note approach need intermediate output word sequence precede element loss term ate element instead loss function train weight network base entirely ﬁnal text classiﬁcation task output softmax output feedforward classiﬁer cross entropy chapter rnn lstm drive training error signal classiﬁcation backpropagate way weight feedforward classiﬁer input set weight rnn describe early section training regiman use loss downstream application adjust weight way network refer end end training end training option instead hidden state token hnto represent sequence use sort pool function pooling hide state hifor word iin sequence example create representation pool nhidden state take element wise mean nnx element wise max element wise max set nvector new vector kth element max kth element nvector long contexts rnns make difﬁcult successfully backpropagate error way entire input talk problem standard solution section generation rnn base language model rnn base language model generate text text generation image generation code generation constitute new area call generative read chapter chapter see reintroduce read different order recall chapter see generate text gram language model adapt sampling technique suggest time claude shannon shannon psychologist george miller jennifer ridge miller selfridge ﬁrst randomly sample word begin sequence base suitability start sequence continue sample word condition previous choice reach pre determined length end sequence token generate today approach language model incrementally generate word repeatedly sample word condition previous choice call autoregressive generation orcausal generation procedure basicallyautoregressive generation describe page adapt neural context sample word output softmax distribution result beginning sentence marker ﬁrst input use word embed ﬁrst word input network time step sample word fashion continue generate end sentence marker sample ﬁxed length limit reach technically autoregressive model model predict value time tbase linear function previous value time language model linear layer non linearity loosely refer generation technique autoregressive generation word generate time step condition word select network previous step fig illustrate approach ﬁgure detail rnn hide layer recurrent connection hide blue tack bidirectional rnn architecture simple architecture underlie state art approach application machine translation summarization question answer key approach prime generation component appropriate context instead simply thing start provide rich task appropriate context translation context sentence source language summarization long text want summarize solong wordsoftmaxembeddinginput wordrnn figure autoregressive generation rnn base neural language model stack bidirectional rnn architecture recurrent network ﬂexible combine feedforward nature roll computational graph vector common input output complex network treat module combine creative way section introduce common network architecture language processing rnns stack rnns example far input rnn consist sequence word character embedding vector output vector useful predict word tag sequence label prevent entire sequence output rnn input sequence stack rnns consist multiple network output layer serve stack rnns input subsequent layer show fig stack rnns generally outperform single layer network reason success network induce representation differ level abstraction layer early stage human visual system detect edge ﬁnde large region shape initial layer stack network induce representation serve useful abstraction layer representation prove difﬁcult induce single rnn optimal number stack rnns speciﬁc application training set number stack increase training cost chapter rnn lstm rnn rnn rnn figure stack recurrent network output low level serve input high level output network serve ﬁnal output quickly bidirectional rnns rnn use information left prior context prediction time application access entire input sequence case like use word context right way run separate rnns leave right right left concatenate representation left right rnns discuss far hidden state give time trepresent network know sequence point state function input represent context network left current time rnn forward new notation tsimply correspond normal hide state time sente network glean sequence far advantage context right current input train rnn reversed input sequence approach hidden state time represent information sequence right current input rnn backward hidden state trepresent information discern sequence tto end sequence abidirectional rnn schuster paliwal combine independentbidirectional rnn rnn input process start end end start concatenate representation compute network single vector capture left right contexts input point time use semicolon equivalent symbol mean vector concatenation helstm fig illustrate bidirectional network concatenate output forward backward pass simple way combine forward backward contexts include element wise addition multiplication output step time capture information left right current input sequence labeling application concatenate output serve basis local labeling decision rnn rnn figure bidirectional rnn separate model train forward ward direction output model time point concatenate represent bidirectional state time point bidirectional rnns prove effective sequence ﬁcation recall fig sequence classiﬁcation ﬁnal hide state rnn input subsequent feedforward classiﬁer ﬁculty approach ﬁnal state naturally reﬂect information end sentence beginning bidirectional rnns provide ple solution problem show fig simply combine ﬁnal hide state forward backward pass example concatenation use input follow processing lstm practice difﬁcult train rnns task require network use information distant current point processing despite have cess entire precede sequence information encode hide state tend fairly local relevant recent part input sequence recent decision distant information critical language application consider follow example context language modeling ﬂight airline cancel assign high probability wasfollowe airline straightforward airline provide strong local context singular agreement assign appropriate probability difﬁcult plural ﬂight distant singular noun airline close chapter rnn lstm rnn rnn figure bidirectional rnn sequence classiﬁcation ﬁnal hide unit forward backward pass combine represent entire sequence bin representation serve input subsequent classiﬁer context ideally network able retain distant information plural ﬂight need process intermediate part sequence correctly reason inability rnns carry forward critical information hidden layer extension weight determine value den layer ask perform task simultaneously provide information useful current decision update carry forward information quire future decision second difﬁculty training rnn arise need backpropagate error signal time recall section hidden layer time tcontribute loss time step take tion result backward pass training hidden layer subject repeated multiplication determine length sequence frequent result process gradient eventually drive zero situation call vanish gradient gradient address issue complex network architecture design explicitly manage task maintain relevant context time enable network learn forget information long need remember information require decision come commonly extension rnns long short term network hochreiter schmidhuber lstms divide con long short term memory text management problem subproblem remove information long need context add information likely need later cision make key solve problem learn manage context hard code strategy architecture lstms accomplish ﬁrst add explicit context layer architecture addition usual recurrent hide layer use specialized neural unit use gate control ﬂow information unit helstm comprise network layer gate implement use tional weight operate sequentially input previous hide layer previous context layer gate lstm share common design pattern consist forward layer follow sigmoid activation function follow pointwise multiplication layer gate choice sigmoid activation function arise tendency push output combine pointwise multiplication effect similar binary mask value layer gate align value near mask pass nearly unchanged value correspond low value essentially erase ﬁrst gate consider forget gate purpose gate forget gate delete information context long need forget gate compute weighted sum previous state hide layer current pass sigmoid mask multiply element wise context vector remove information context long quire element wise multiplication vector represent operator call hadamard product vector dimension input vector element ii product element iin input vector task compute actual information need extract ous hidden state current input basic computation recurrent network generate mask add gate select information add add gate current context add modiﬁed context vector new context vector ﬁnal gate use output gate decide output gate tion require current hide state oppose information need preserve future decision fig illustrate complete computation single lstm unit give appropriate weight gate lstm accept input context layer hide layer previous time step current input vector generate update context hidden vector output hidden state provide output lstm time step output input subsequent layer stacked rnn ﬁnal layer network htcan provide ﬁnal output chapter rnn lstm figure single lstm unit display computation graph input unit consist current input previous hide state previous context output new hidden state htand update context gza gzlstmunita figure basic neural unit feedforward simple recurrent network srn long short term memory lstm gate unit layer network neural unit lstms obviously complex basic feedforward network fortunately complexity encapsulate basic processing unit allow maintain modularity easily ment different architecture consider fig illustrate input output associate kind unit far left basic feedforward unit single set weight single activation function determine output arrange layer connection unit layer represent unit simple recurrent network input additional set weight single activation function output increase complexity lstm unit encapsulate unit additional external complexity lstm basic recurrent unit presence additional context vector input output modularity key power widespread applicability lstm unit lstm unit variety like grus substitute network architecture describe section simple rnns multi ummary common rnn nlp rchitecture network make use gate unit unroll deep feedforward network train usual fashion backpropagation practice lstms rnns standard unit modern system make use recurrent network summary common rnn nlp architecture introduce rnn see advanced component like stack multiple layer lstm version see rnn apply task let moment summarize architecture application fig show architecture discuss far sequence beling sequence classiﬁcation language modeling sequence labeling example speech tagging train model produce label input word token sequence classiﬁcation example sentiment analysis ignore output token value end sequence similarly model training signal come backpropagation token language modeling train model predict word token step section introduce fourth architecture encoder decoder encoder rnndecoder rnncontext sequence label sequence classification language modelingd encoder decoder figure architecture nlp task sequence labeling pos name entity tag map input token xito output token sequence classiﬁcation map entire input sequence single class language modeling output token condition previous token encoder model separate rnn model map input sequence xto intermediate representation context second map context output sequence chapter rnn lstm encoder decoder model rnns section introduce encoder decoder model take input sequence translate output sequence different length input align word word way read chapter see model transformer architecture application machine translation duce architecture come concept different order read rnns transformer recall sequence labeling task sequence length example speech tag token get associated tag input associate speciﬁc output labeling output take local information decide word verb noun look word neighboring word contrast encoder decoder model especially task like machine translation input sequence output sequence different length mapping token input token output indirect language verb appear beginning sentence language end introduce machine translation chapter point mapping sentence english sentence tagalog yoruba different number word word different order encoder decoder network call sequence sequence network decoder model capable generate contextually appropriate arbitrary length output sequence give input sequence encoder decoder network apply wide range application include summarization question answer dialogue particularly popular machine translation key idea underlie network use encoder network take input sequence create contextualized representation call thecontext representation pass decoder generate speciﬁc output sequence fig illustrate architecture encoderdecodercontext figure encoder decoder architecture context function hide representation input decoder variety way encoder decoder network consist conceptual component encoder accept input sequence generate ing sequence contextualized representation lstms convolutional network transformer employ encoder context vector function convey essence input heencoder model rnn decoder accept cas input generate arbitrary length quence hide state corresponding sequence output state obtain encoder decoder realize kind sequence architecture section describe encoder decoder network base pair rnn chapter apply transformer build equation encoder decoder model start conditional rnn language model probability sequence recall language model break probability follow rnn language modeling particular time pass preﬁx token language model forward inference produce sequence hidden state end hidden state correspond word preﬁx use ﬁnal hide state preﬁx starting point generate token formally gi activation function like tanh relu function input time tand hidden state time softmax set possible vocabulary item time tthe output ytand hide state htare compute softmax slight change turn language model toregressive generation encoder decoder model translation model translate source text language target text second add sentence separation marker end source text simplysentence separation concatenate target text let use sentence separator token let think translate english source text green witch arrive spanish sentence bruja verde glossed word word arrive witch green illustrate encoder decoder model question answer pair text summarization pair let use xto refer source text case english plus separator token yto refer target text case spanish encoder decoder model compute probability follow fig show setup simpliﬁed version encoder decoder model model require new concept attention section fig show english source text green witch arrive sentence separator token spanish target text bruja verde late source text run network perform forward inference generate hide state end source begin gressive generation ask word context hidden layer end source input end sentence marker subsequent word condition previous hidden state embed word chapter rnn lstm source texttarget text witcharrive llególa labruja brujaverde source ignore separator figure translate single sentence inference time basic rnn version encoder decoder proach machine translation source target sentence concatenate separator token decoder use context information encoder hidden state let formalize generalize model bit fig help thing straight use superscript eanddwhere need distinguish hide state encoder decoder element network left process input sequence xand comprise encoder simpliﬁed ﬁgure show single network layer encoder stack architecture norm output state layer stack take ﬁnal representation encoder consist stack bilstms hide state layer forward backward pass concatenate provide contextualized representation time step encoderdecod hen ignore encode figure formal version translate sentence inference time basic rnn base encoder decoder architecture ﬁnal hide state encoder rnn hen serve context decoder role decoder rnn available decoder hide state entire purpose encoder generate contextualized representation input representation embody ﬁnal hide state encoder representation call cforcontext pass decoder simple version decoder network state use initialize ﬁrst hide state decoder ﬁrst decoder rnn cell heencoder model rnn useca prior hide state decoder autoregressively generate sequence output element time end sequence marker generate hide state condition previous hidden state output generate previous state fig show complex context vector cavailable ﬁrst decoder hide state ensure inﬂuence context vector wane output sequence generate add cas parameter computation current hidden state follow equation ready equation version decoder basic encoder decoder model context available decode timestep recall thatgis stand ﬂavor rnn embed output sample softmax previous step softmax ytis vector probability vocabulary represent probability word occur time generate text sample distribution example greedy choice simply choose probable word generate timestep discuss sampling method section train encoder decoder model encoder decoder architecture train end end training example tuple paired string source target concatenate separator token source target pair serve training datum training datum typically consist set sentence tion draw standard dataset align sentence pair discuss section training set training proceed rnn base language model network give source text start separator token train autoregressively predict word show fig note difference training fig inference fig respect output time step decoder inference use estimate output ytas input time step decoder tend deviate gold target sentence keep generate token training common use teacher force teacher force decoder teacher force mean force system use gold target token training input allow rely possibly erroneous decoder output speed chapter rnn lstm encoderdecoder witcharrive llególa labruja brujaverde loss average cross entropy loss target word figure training basic rnn encoder decoder approach machine translation note decoder usually propagate model softmax output use teacher force force input correct gold value training compute softmax output distribution yin decoder order compute loss token average compute loss sentence loss propagate decoder parameter encoder parameter attention simplicity encoder decoder model clean separation encoder build representation source text decoder use context generate target text model describe far context vector hidden state nth time step source text ﬁnal hide state act bottleneck represent absolutely meaning source text thing decoder know source text context vector fig information beginning sentence especially long sentence equally represent context vector encoderdecoderbottleneckbottleneck figure require context cto encoder ﬁnal hide state force information entire source sentence pass representational neck attention mechanism solution bottleneck problem way ofattention mechanism allow decoder information allthe hide state encoder hidden state attention mechanism vanilla encoder decoder model context vector cis single vector function hide state encoder instead take hidden state weighted average ttention hide state encoder weight average inform decoder state state decoder right current token weights focus attend particular source text relevant token ithat decoder currently produce attention replace static context vector dynamically derive encoder hide state inform different token decode context vector generate anew decode step iand take encoder hide state account derivation context available decode condition computation current decoder hide state prior hidden state previous output generate decoder equation fig figure attention mechanism allow hide state decoder different dynamic context function encoder hide state ﬁrst step compute ciis compute focus encoder state relevant encoder state decoder state capture capture relevance compute state idure decode encoder state simple score call dot product attention implement relevance asdot product attention similarity measure similar decoder hide state encoder hide state compute dot product score result dot product scalar reﬂect degree similarity vector vector score encoder hide state give relevance encoder state current step decoder use score normalize softmax create vector weight tell proportional relevance encoder hide state jto prior hide decoder state softmax finally give distribution compute ﬁxed length context vector current decoder state take weighted average encoder hide state jai jhe chapter rnn lstm ﬁnally ﬁxed length context vector take account information entire encoder state dynamically update reﬂect need decoder step decode fig illustrate decoder network attention focus computation context vector encoderdecod latexit ijhej latexit figure sketch encoder decoder network attention focus computation context value ciis input computation compute take weighted sum encoder hide state weight dot product prior decoder hide state possible create sophisticated scoring function attention model instead simple dot product attention powerful function compute relevance encoder hide state decoder hide state parameterizing score set weight weights train normal end end training network ability learn aspect similarity decoder encoder state important current application bilinear model allow encoder decoder use different dimensional vector simple dot product attention require encoder decoder hide state dimensionality return concept attention deﬁne transformer tecture chapter base slight modiﬁcation attention call self attention summary chapter introduce concept recurrent neural network apply language problem summary main point cover simple recurrent neural network sequence process element time output neural unit time tbase current input tand hidden layer time note rnn train straightforward extension backpropagation algorithm know backpropagation time bptt simple recurrent network fail long input problem like ishe gradient instead modern system use complex gate ture lstms explicitly decide remember forget hidden context layer common language base application rnns include probabilistic language modeling assign probability sequence element sequence give precede word auto regressive generation train language model sequence labeling like speech tagging element sequence assign label sequence classiﬁcation entire text assign category spam detection sentiment analysis topic classiﬁcation encoder decoder architecture input map output different length alignment historical note inﬂuential investigation rnn conduct context parallel tribute processing pdp group san diego work direct human cognitive modeling practical nlp application rumelhart mcclelland mcclelland rumelhart model recurrence hidden layer feedforward network elman network troduce elman similar architecture investigate jordan recurrence output layer mathis mozer addition recurrent context layer prior hidden layer possibility unroll recurrent network equivalent feedforward network discuss rumelhart mcclelland parallel work cognitive modeling rnns investigate extensively continuous domain signal processing speech community gile robinson schuster paliwal introduce tional rnns describe result timit phoneme transcription task theoretically interesting difﬁculty training rnn ing context long sequence impede progress practical application situation change introduction lstms hochreiter schmidhuber ger impressive performance gain demonstrate task boundary signal processing language processing include phoneme recognition grave schmidhuber handwriting recognition grave signiﬁcantly speech recognition grave interest apply neural network practical nlp problem surge work collobert weston collobert effort use learn word embedding convolutional network end end training demonstrate near state art performance number standard share task include speech tagging chunking name entity recognition mantic role labeling use hand engineer feature approach marry lstms pretraine collection word embedding base mikolov glove pennington chapter rnn lstm quickly come dominate common task speech tagging ling syntactic chunking søgaard goldberg name entity recognition chiu nichols hovy opinion mining irsoy cardie semantic role labeling zhou amr parsing foland martin early surge progress involve statistical machine learning advance possible availability training datum vide conll semeval shared task share resource ontonote pradhan propbank palmer modern neural encoder decoder approach pioneer kalchbrenner blunsom cnn encoder rnn decoder cho coin encoder decoder sutskever show use extended rnns encoder decoder idea generative decoder input soft weighting input central idea attention ﬁrst develop grave context handwriting recognition bahdanau extend idea name attention apply speech feature extraction character text discuss book random symbol amazing scientiﬁc invention theoretical model element human speech early writing system know sumerian chinese mayan mainly logographic symbol represent word liest stage ﬁnd symbol represent sound word cuneiform sign right nounced baand mean ration sumerian function purely sound early chinese acter carve bone divination similarly contain phonetic element purely sound base writing system syllabic like japanese hiragana alphabetic like roman alphabet consonantal like semitic writing system trace early logo syllabic system culture come arabic aramaic hebrew greek roman system derive west semitic script presume modiﬁe western semitic mercenary cursive form egyptian hieroglyph japanese syllabary modiﬁe cursive form chinese phonetic ter chinese phonetically represent sanskrit buddhist scripture come china tang dynasty implicit idea speak word compose small unit speech underlie algorithm speech recognition transcribe waveform text andtext speech convert text waveform chapter putational perspective phonetic study speech sound phonetic language world produce human vocal tract realize acoustically digitize process speech sound phonetic transcription letter like useful model sound human speech chapter map letter waveform nonetheless helpful represent sound slightly abstractly send pronunciation word string phone speech sound phone represent symbol adapt roman alphabet standard phonetic representation transcribe world language theinternational phonetic alphabet ipa evolve standard ﬁrst develop ipa chapter instead represent phone arpabet shoup simple phonetic alphabet fig conveniently use ascii symbol represent american english subset ipa ipa arpabet symbol equivalent familiar roman ter example arpabet phone consonant sound chapter honetic speech feature extraction arpabet ipa arpabet arpabet ipa arpabet symbol symbol word transcription symbol symbol word transcription arsley lily lily ook daisy pen ill aster axr arlic poppy int orchid utmeg wood bake lotus lour axr tulip clov butter axr ick bird ose iris oup ﬂow axr egg soil squash pita ambro erry icorice axr kiw ice ellow oney figure arpabet ipa symbol english consonant left vowel right beginning platypus puma plantain middle leopard end telope general mapping letter english orthography phone relatively opaque single letter represent different sound different contexts english letter ccorrespond phone cougar axr phone appear candk phone appear language example spanish transparent sound orthography mapping english wide variety phonetic resource phonetic transcription linepronunciation dictionary phonetic transcription word ldcpronunciation dictionary distribute pronunciation lexicon egyptian arabic dutch english german japanese korean mandarin spanish english celex dictionary baayen pronunciation wordform tion stress morphological speech information open source cmu pronounce dictionary cmu pronunciation wordform ﬁne grain word unisyn dictionary fitt freely available research purpose give syllabiﬁcation stress ciation dozen dialect english useful resource phonetically annotate corpus lection waveform hand label corresponding string phone timit corpus nist originally joint project texas instrument mit sri corpus read sentence sentence rticulatory phonetic speaker sentence draw set sentence select particular dialect shibboleth maximize phonetic diphone coverage sentence corpus phonetically hand label sequence phone automatically align sentence waveﬁle tomatic phone boundary manually hand correct seneff zue result time align transcription transcription phone istime align transcription associate start end time waveform like example fig dark suit ingreasy wash water year dcl axr dcl kcl engcl axr axr figure phonetic transcription timit corpus special arpabet feature narrow scription palatalization unreleased ﬁnal stop dark glottalization ﬁnal suit ﬂap water timit corpus include time alignment show switchboard transcription project phonetically annotate corpus consist hour sentence extract switchboard corpus greenberg transcription time align syllable level figure show example figure phonetic transcription switchboard phrase kind right note vowel reduction andof coda deletion kind andright syllabiﬁcation ofattache onset time give number second beginning sentence start syllable buckeye corpus pitt pitt phonetically scribe corpus spontaneous american speech contain word talker phonetically transcribed corpora available guage include kiel corpus german mandarin corpora transcribe chinese academy social sciences articulatory phonetic articulatory phonetic study phone produce variousarticulatory phonetic organ mouth throat nose modify airﬂow lung vocal organ figure show organ speech sound produce rapid movement air human produce sound speak language expel air lung windpipe technically trachea mouth nose pass trachea air pass larynx commonly know adam apple voice box larynx contain small fold muscle vocal fold refer non technically vocal cord move apart space fold call glottis fold close tightly closed vibrate air glottis pass far apart vibrate sound vocal fold vibrate call voice sound vocal voice sound cord vibration call unvoiced orvoiceless oice sound include unvoiced chapter honetic speech feature extraction figure vocal organ show view figure openstax university physic english vowel unvoiced sound include area trachea call vocal tract consist oral tract nasal tract air leave trachea exit body mouth nose sound air pass mouth sound air pass nose call nasal sound nasal sound like nasal english use oral nasal tract resonate cavity phone divide main class consonant andvowel kind consonant vowel sound form motion air mouth throat nose sonant restriction blocking airﬂow way voice unvoiced owel obstruction usually voice erally loud long last consonant technical use term like common usage etc consonant etc vowel property voice like vowel short syllabic like consonant consonant place articulation consonant restrict airﬂow group class point maximum restriction place articulation fig articulation labial consonant main restriction form lip come labial gether bilabial place articulation english include inpossum bear marmot english labiodental consonant press lip upper row tooth let air ﬂow space upper rticulatory phonetic nasal figure major english place articulation dental sound place tongue tooth dental dental main dental english thingand place tongue tooth tip slightly tooth alveolar alveolar ridge portion roof mouth alveolar upper tooth speaker american english phone place tip tongue alveolar ridge word coronal refer dental alveolar palatal roof mouth palate rise sharply palatal palate alveolar ridge palato alveolar sound shrimp china asian jar blade tongue rise alveolar ridge palatal sound yaki place tongue close palate velar thevelum soft palate movable muscular ﬂap velar roof mouth sound cuckoo goose press tongue velum glottal glottal stop close glottis bring vocal glottal fold consonant manner articulation consonant distinguish restriction airﬂow ample complete stoppage air partial blockage feature call themanner articulation consonant combination place mannermanner articulation articulation usually sufﬁcient uniquely identify consonant follow major manner articulation english consonant astop consonant airﬂow completely block short time stop blockage follow explosive sound air release period blockage call closure explosion call release english voice stop like unvoiced stop like stop call plosive thenasal sound lower velum nasal ing air pass nasal cavity infricative airﬂow constrict cut completely turbulent fricative airﬂow result constriction produce characteristic hiss sound english labiodental fricative produce press low lip upper tooth allow restrict airﬂow upper tooth dental fricative allow air ﬂow tongue tooth alveolar fricative produce tongue chapter honetic speech feature extraction alveolar ridge force air edge tooth palato alveolar fricative tongue alveolar ridge force air groove form tongue higher pitch fricative english call sibilant stop follow immediately fricative sibilant call affricate include english chicken giraffe inapproximant articulator close close approximant cause turbulent airﬂow english yellow tongue move close roof mouth close cause turbulence characterize fricative english wood tongue come close velum american form way tip tongue extend close palate tongue bunch near palate form tip tongue alveolar ridge tooth side tongue lower allow air ﬂow call lateral sound drop side tongue quick motion tongue alveolar ridge tap consonant middle word lotus tap dialect american english speaker dialect use instead vowel like consonant vowel characterize position articulator relevant parameter vowel call vowel height correlate roughly height high tongue vowel frontness orbackness indicate high point oral tract shape lip rounded figure show position tongue different vowel boot figure tongue position english high low high vowel example high point tongue mouth vowel contrast high point tongue locate mouth owel tongue raise call vowel tongue raise vowel call vowel note vowel vowel tongue high owel high point tongue comparatively high call high vowel vowel mid low value high vowel maximum tongue height call mid vowel orlow vowel respectively figure show schematic characterization height different vowel schematic abstract property height correlate roughly tual tongue position fact accurate reﬂection acoustic fact note chart kind vowel tongue height represent point represent path vowel tongue position change markedly production vowel diphthong rticulatory phonetic lowhigh aeuw ahao aay eyowoy ayaw figure schematic vowel space english vowel glish particularly rich diphthong second important articulatory dimension vowel shape lip certain vowel pronounce lip round lip shape whistle rounded vowel include rounded vowel syllable consonant vowel combine syllable syllable vowel like syllable sonorant sound surround consonant closely associate word dogha syllable dialect word catnip syllable vowel core syllable nucleus initial consonant call onset onset nucleus onset consonant strike call complex onset thecoda optional consonant sequence consonant follow nucleus coda onset dog coda rime orrhyme nucleus rime plus coda figure show sample syllable structure rime coda mnucleus aeonset rime coda nnucleus iyonset rgs rime coda zgnucleus figure syllable structure ham green syllable task automatically break word syllable call tion syllable structure closely related phonotactic language syllabiﬁcation term phonotactic mean constraint phone follow phonotactic language example english strong constraint kind nant appear onset sequence zdr example chapter honetic speech feature extraction legal english syllable onset phonotactic represent language model ﬁnite state model phone sequence prosody prosody study intonational rhythmic aspect language prosody particular use duration convey pragmatic affective conversation interactional introduce acoustic quantity detail section turn acoustic phonetic brieﬂy think energy acoustic quality perceive loudness frequency sound produce acoustic quality hear pitch utterance prosody mark discourse structure like difference statement question way conversation ture prosody mark saliency particular word phrase prosody heavily paralinguistic function like convey affective meaning like happiness surprise anger prosody play important role manage turn taking conversation prosodic prominence accent stress schwa natural utterance american english word sound prominent prominence certain syllable word prominent mean prominence word syllable perceptually salient listener speaker word syllable salient english say louder say slow long duration vary word make high variable accent represent prominence linguistic marker call pitch accent word pitch accent syllable prominent say bear associate pitch accent utterance pronounce accent underlined word little surprised hear characterize happy lexical stress syllable bear pitch accent call accented syllable syllable word accent pitch accent realize syllable lexical stress lexical stress property word lexical stress tion dictionary syllable lexical stress loud long word accent example word surprise stress second syllable ﬁrst try stress syllable say surprised hopefully sound wrong word surprise receive pitch accent sentence second syllable strong following ample show underline accented word stressed syllable bear accent loud long syllable boldface little sur prise hear char acterize ashappy stress mark dictionary cmu dictionary cmu ample mark vowel unstressed stress entry counter table difference lexical stress affect word meaning noun content pronounce adjective pronounce word different related way poetry mean study verse metrical rosody reduce vowel schwa unstressed vowel weaken reduce vowel common schwa second vowel reduce vowel schwa ofparakeet reduce vowel articulatory gesture complete vowel unstressed vowel reduce vowel diphthong particular retain quality unstressed position example vowel appear stress position word unstressed position word carry summary continuum prosodic prominence prominence useful represent level like accent stress vowel reduce vowel prosodic structure speak sentence prosodic structure word group naturally gether word noticeable break disjuncture prosodic structure describe term prosodic phrasing mean prosodic phrasing ing utterance prosodic phrase structure similar way have syntactic phrase structure example sentence want london ticket france main intonation phrase intonation phrase boundary occur comma furthermore ﬁrst phrase set less prosodic phrase boundary call intermediate phrase split word wantedjto gojto london kind ofintermediate phrase intonation phrase correlate syntactic structure constituent price bennett elfner automatically predict prosodic boundary important task like tts modern approach use sequence model raw text text notate feature like parse tree input break break decision word boundary train datum label prosodic structure like boston university radio news corpus ostendorf tune utterance prominence phrase pattern differ prosodically have different tune tune utterance rise tune fall time obvious example tune difference statement yes question english word say ﬁnal rise indicate yes question call question rise question rise know imean ﬁnal drop call ﬁnal fall indicate declarative intonation ﬁnal fall know imean language wide use tune express meaning english example know rise yes question phrase contain list noun separate commas short rise call continuation riseafter noun example include characteristic english contour forcontinuation rise express contradiction express surprise chapter honetic speech feature extraction link prominence tune pitch accent come different variety relate tune high pitch accent example different function low pitched accent typology accent class different language typology tobi tone break index theory intonation silverman tobi word tobi associate ﬁve type pitch accent show fig utterance tobi consist sequence intonational phrase end boundary tone show fig represent boundary tone utterance ﬁnal aspect tune version tobi language pitch accent boundary tone peak accent ﬁnal fall declarative contour american english low accent continuation rise scoop accent question rise cantonical yes question contour rise peak accent ﬁnal level plateau step figure accent boundary tone label tobi transcription system american english intonation beckman ayer beckman hirschberg acoustic phonetic signal begin brief introduction acoustic waveform digitization frequency analysis interested reader encourage consult reference end chapter wave acoustic analysis base sine cosine function figure show plot sine wave particular function set amplitude frequency fto cycle second time figure sine wave frequency amplitude recall basic mathematic important characteristic wave itsfrequency andamplitude frequency number time second frequency coustic phonetic signal wave repeat number cycle usually measure frequency incycle second signal fig repeat time second cycle second cycle second usually call hertz shorten hertz tohz frequency fig describe amplitude aof sine wave maximum value axis period tof wave period time take cycle complete deﬁne cycle fig last tenth second second speech sound wave let turn hypothetical wave sound wave input speech nizer like input human ear complex series change air pressure change air pressure obviously originate speaker cause speciﬁc way air pass glottis oral nasal itie represent sound wave plot change air pressure time metaphor help understand graph tical plate block air pressure wave microphone speaker mouth eardrum hearer ear graph measure ofcompression orrarefaction uncompression air molecule plate figure show short segment waveform take switchboard corpus telephone speech vowel say baby time figure waveform vowel utterance show later fig page axis show level air pressure normal atmospheric pressure axis show time notice wave repeat regularly ﬁrst step digitize sound wave like fig convert analog representation ﬁrst air pressure analog electric signal microphone digital signal analog digital conversion step sampling sampling quantization sample signal measure amplitude particular time sampling rate number sample take second accurately measure wave sample cycle measure positive wave measure negative sample cycle increase amplitude accuracy few sample cause quency wave completely miss maximum frequency wave measure frequency half sample rate cycle need sample maximum frequency give sampling rate call nyquist frequency information human speech frequenciesnyquist frequency sampling rate necessary plete accuracy telephone speech ﬁltere switching network chapter honetic speech feature extraction frequency transmit telephone sampling rate sufﬁcient telephone bandwidth speech like switchboard corpus sampling microphone speech sampling rate require amplitude measurement second speech important store amplitude measurement efﬁciently usually store integer bit value bit value process represent real value number integer call quantization difference integer act quantization minimum granularity quantum size value close quantum size represent identically datum quantize store format parameter format sample rate sample size discuss telephone speech sample khz store bit sample microphone datum sample khz store bit sample parameter number channel stereo datum party conversation store channel channel ﬁle store separate ﬁle ﬁnal parameter individual sample storage linearly compress common compression format telephone speech law write law pronounce law intuition log compression algorithm like law human hearing sensitive small intensity large one log represent small value faithfulness expense error large value linear unlogged value generally refer linear pcm value pcm stand pulse code pcm modulation mind equation compress linear pcm sample value xto bit law bit number standard ﬁle format store result digitize ﬁle microsoft apple aiff special header simple headerless raw ﬁle example format subset microsoft riff format multimedia ﬁle riff general format represent series nest chunk datum control information figure show simple ﬁle single datum chunk format chunk figure microsoft waveﬁle header format assume simple ﬁle chunk lowing byte header data chunk frequency amplitude pitch loudness sound wave like wave describe term frequency amplitude characteristic introduce early pure sine wave sound wave simple measure sine wave let consider frequency note fig exactly sine wave nonetheless periodic repeat time millisecond coustic phonetic signal capture ﬁgure frequency segment wave periodic wave come come speed vibration vocal fold waveform fig vowel voice recall voicing cause regular opening closing vocal fold vocal fold open air push lung create region high pressure fold close pressure lung vocal fold vibrate expect regular peak amplitude kind fig major peak correspond opening vocal fold frequency vocal fold vibration frequency complex wave call fundamental frequency wave fundamental frequency form abbreviate plot time pitch track figure pitch track show pitch track short question represent waveform note rise end question time figure pitch track question show waveﬁle note rise end question note lack pitch trace quiet automatic pitch tracking base count pulse voice region work voicing insufﬁcient sound vertical axis fig measure air pressure variation pressure force unit area measure pascal high value vertical axis high amplitude indicate air pressure point time zero value mean normal atmospheric air pressure negative value mean low normal air pressure rarefaction addition value amplitude point time need know average amplitude time range idea great average displacement air pressure average amplitude value range positive negative value cancel leave number close zero instead generally use rm root mean square amplitude square number average make positive take square root end rm amplituden nnx thepower signal relate square amplitude number chapter honetic speech feature extraction sample sound power power nnx power refer intensity sound intensity normalize power human auditory threshold measure auditory threshold pressure intensity deﬁne follow intensity figure show intensity plot sentence long movie callhome corpus show waveform plot long movie time figure intensity plot sentence long movie note intensity peak vowel especially high peak word long important perceptual property pitch andloudness relate quency intensity pitch sound mental sensation perceptual pitch correlate fundamental frequency general sound high tal frequency perceive have high pitch general relationship linear human hearing different acuity different frequency roughly speak human pitch perception accurate range pitch correlate linearly frequency man hearing represent frequency accurately range pitch correlate logarithmically frequency logarithmic representation mean difference high frequency compress accurately perceive psychoacoustic model pitch tion scale common model melscale steven steven mel olkmann mel unit pitch deﬁne pair sound perceptually equidistant pitch separate equal number mel mel frequency mcan compute raw acoustic frequency follow chapter mel scale play important role speech coustic phonetic signal theloudness sound perceptual correlate power sound high amplitude perceive louder relationship linear mention deﬁne law compression human great resolution low power range ear sensitive small power difference second turn complex relationship power frequency perceive loudness sound certain frequency range perceive loud frequency range algorithm exist automatically extract slight abuse minology call pitch extraction algorithm autocorrelation method pitch extraction pitch extraction example correlate signal offset offset give high correlation give period signal publicly available pitch extraction toolkit example augmented autocorrelation pitch tracker provide praat boersma weenink interpretation phone waveform learn visual inspection waveform example vowel pretty easy spot recall vowel voice property vowel tend long relatively loud intensity plot fig length time manifest directly axis ness relate square amplitude axis see previous section voicing realize regular peak amplitude kind see fig major peak correspond opening vocal fold ure show waveform short sentence baby label waveform word phone label notice el fig regular amplitude peak indicate voicing baby time figure waveform sentence baby switchboard corpus conversation speaker female year old approximately recording speak south midland dialect american english stop consonant consist closure follow release period silence near silence follow slight burst amplitude baby fig phone recognizable waveform fricative fricative especially strident fricative like narrow channel airﬂow cause noisy turbulent air result hissy sound noisy irregular waveform see somewhat fig clear fig magniﬁed ﬁrst word chapter honetic speech feature extraction time figure detailed view ﬁrst word extract waveﬁle fig notice difference random noise fricative regular voicing vowel spectra frequency domain broad phonetic feature energy pitch presence ing stop closure fricative interpret directly waveform computational application speech recognition human auditory processing base different representation sound term ponent frequency insight fouri analysis complex wave represent sum sine wave different frequency consider waveform fig waveform create praat sum sine waveform frequency frequency time figure waveform sum sine waveform frequency note ﬁve repetition half second window frequency amplitude represent component frequency spectrum spectrum trum signal representation frequency component amplitude figure show spectrum fig frequency axis amplitude axis note spike ﬁgure spectrum alternative representation original waveform use spectrum tool study component frequency sound wave particular time point let look frequency component speech waveform figure show waveform vowel word cut sentence show fig note complex wave repeat time ﬁgure small repeat wave repeat time large pattern notice small peak inside repeat wave complex wave frequency ﬁgure repeat roughly coustic phonetic signal frequency pressure level figure spectrum waveform fig time figure waveform vowel word hadcut waveform show fig second second small wave frequency roughly time quency large wave roughly look carefully little wave peak wave frequency tiny wave roughly twice wave figure show smoothed spectrum waveform fig put discrete fouri transform dft frequency pressure level figure spectrum vowel word hadin waveform baby fig axis spectrum show frequency axis show sure magnitude frequency component decibel logarithmic measure amplitude see early fig show signiﬁcant quency component low magnitude frequency component ﬁrst component notice time domain look wave fig spectrum useful turn spectral peak easily visible spectrum characteristic different phone phone chapter honetic speech feature extraction tic spectral signature chemical element different wavelength light burn allow detect element star look trum light detect characteristic signature different phone look spectrum waveform use spectral information tial human machine speech recognition human audition function cochlea orinn ear compute spectrum incoming waveform cochlea similarly acoustic feature speech recognition spectral tion let look spectrum different vowel vowel change time use different kind plot call spectrogram spectrum show frequency component wave point time spectrogram spectrogram way envision different frequency waveform change time axis show time waveform axis show frequency hertz darkness point spectrogram correspond amplitude frequency component dark point high amplitude light point low amplitude spectrogram useful way visualize dimension time frequency amplitude figure show spectrogram american english vowel note vowel set dark bar frequency band slightly different band vowel represent kind spectral peak see fig time figure spectrogram american english vowel dark bar spectral peak call formant discuss formant formant frequency band particularly ampliﬁed vocal tract different vowel produce vocal tract different position produce different kind ampliﬁcation resonance let look ﬁrst formant call note dark bar close different position vowel low center somewhat high contrast second dark bar high middle low formant run speech reduction coarticulation process somewhat hard figure show spectrogram baby waveform show fig pretty clear baby speciﬁc clue spectral representation phone identiﬁcation different vowel formant characteristic place spectrum distinguish vowel see sample waveform formant consider vowel coustic phonetic signal baby dxax time figure spectrogram sentence baby waveform show fig think spectrogram collection spectra time slice like fig place end end beginning utterance fig spectrum vowel show fig ﬁrst formant low ﬁrst formant second formant high second formant look carefully formant dark bar fig second figure smoothed lpc spectrum vowel start baby note ﬁrst formant low ﬁrst formant show fig second formant high second formant location ﬁrst formant call play large role termining vowel identity formant differ speaker speaker high formant tend cause general characteristic speaker vocal tract individual vowel formant identify nasal phone liquid source filter model different vowel different spectral signature brieﬂy mention formant cause resonant cavity mouth ﬁlter model way explain acoustic sound model model pulse produce glottis source shape vocal tract ﬁlter let work wave vibration air cause glottal pulse wave harmonic harmonic harmonic wave frequency multiple fundamental wave example chapter honetic speech feature extraction glottal fold vibration lead harmonic wave general wave weak amplitude wave fundamental frequency turn vocal tract act kind ﬁlter ampliﬁer cavity tube cause wave certain frequency ampliﬁed damp ampliﬁcation process cause shape cavity give shape cause sound certain frequency resonate ampliﬁed change shape cavity cause different frequency ampliﬁed produce particular vowel essentially change shape vocal tract cavity place tongue articulator particular position result different vowel cause different harmonic ﬁed wave fundamental frequency pass different vocal tract position result different harmonic ampliﬁed result ampliﬁcation look relationship shape vocal tract corresponding spectrum figure show vocal tract position vowel typical result spectrum mant place spectrum vocal tract happen amplify particular harmonic frequency frequency pressure level tea frequency pressure level cat frequency pressure level moo cat tea figure visualize vocal tract position ﬁlter tongue position english vowel result smoothed spectra show feature extraction speech recognition log mel spectrum tool analyze acoustic phonetic waveform input speech processing algorithm section duce signal processing pipeline task like automatic speech recognition asr chapter ﬁrst step speech processing transform input waveform sequence acoustic eature extraction speech recognition logmelspectrum ture vector vector represent information small time window feature vector signal speech recognition processing algorithm start waveform case processing convolutional network convnet introduce chapter system begin instead high level log mel spectrum section introduce commonly feature vector sequence log mel spectrum vector following section introduce alternative vector mfcc representation introduce concept relatively high level speech signal processing course recommend detail begin repeat section process digitizing tize analog speech waveform sampling quantization input speech recognizer complex series change air pressure change air pressure obviously originate speaker cause speciﬁc way air pass glottis oral nasal itie represent sound wave plot change air pressure time metaphor help understand graph tical plate block air pressure wave microphone speaker mouth eardrum hearer ear graph measure ofcompression orrarefaction uncompression air molecule plate figure repeat ﬁgrefﬁg show short segment form take switchboard corpus telephone speech vowel say baby time figure waveform instance vowel vowel word baby axis show level air pressure normal atmospheric pressure axis show time notice wave repeat regularly repeat fig ﬁrst step digitize sound wave like fig convert analog representation ﬁrst air pressure analog electric signal microphone digital signal analog digital conversion step sampling sampling quantization sample signal measure amplitude particular time sampling rate number sample take second accurately measure wave sample cycle measure positive wave measure negative sample cycle increase amplitude accuracy few sample cause quency wave completely miss maximum frequency wave measure frequency half sample rate cycle need sample maximum frequency give sampling rate call nyquist frequency information human speech frequenciesnyquist frequency sampling rate necessary plete accuracy telephone speech ﬁltere switching network frequency transmit telephone chapter honetic speech feature extraction sampling rate sufﬁcient telephone bandwidth speech like switchboard corpus sampling microphone speech high sampling rate produce high asr accuracy combine different sampling rate training test asr system test telephone corpus like switchboard khz sampling downsample training corpus khz similarly train tiple corpora include telephone speech downsample wideband corpora amplitude measurement store integer bit value bit value process represent real value number integer call quantization value close quantization minimum granularity quantum size represent identically refer sample time index nin digitize quantize waveform datum quantize store format parameter format sample rate sample size discuss telephone speech sample khz store bit sample microphone datum sample khz store bit sample parameter number channel stereo datum party conversation store channel channel ﬁle store separate ﬁle ﬁnal parameter individual sample storage linearly compress common compression format telephone speech law write law pronounce law intuition log compression algorithm like law human hearing sensitive small intensity large one log represent small value faithfulness expense error large value linear unlogged value generally refer linear pcm value pcm stand pulse code pcm modulation mind equation compress linear pcm sample value xto bit law bit windowe digitize quantize representation waveform need extract spectral feature small window speech characterize ticular phoneme inside small window roughly think signal stationary statistical property constant region stationary contrast general speech non stationary signal mean statistical non stationary property constant time extract roughly stationary portion speech window non zero inside region zero ning window speech signal multiply input waveform produce windowed waveform speech extract window call frame windowing frame characterize parameter window size orframe size window width millisecond frame stride call shift oroffset stride successive window shape window extract signal multiply value signal time value window time window shape sketch fig rectangular eature extraction speech recognition logmelspectrum figure windowing show rectangular window stride tracte windowed signal look like original signal rectangular window abruptly cut signal boundary create problem fouri analysis reason acoustic feature creation monly use hamming window shrink value signal hamming zero window boundary avoid discontinuity figure show equation follow assume window lframe long rectangular hamming time windowhamme window time time figure windowe sine wave rectangular hamming chapter honetic speech feature extraction discrete fouri transform step extract spectral information windowed signal need know energy signal contain different frequency band tool extract spectral information discrete frequency band discrete time sample signal discrete fouri transform ordft fouri transformdft input dft windowed signal output ndiscrete frequency band complex number magnitude phase frequency component original signal plot tude frequency visualize spectrum chapter spectra example fig show hamming windowe portion signal spectrum compute dft additional smoothing time frequency pressure level figure hamming windowe portion signal vowel spectrum compute dft introduce mathematical detail dft note fouri analysis rely euler formula jas imaginary unit euler formula ejq brief reminder student study signal processing dft deﬁne follow commonly algorithm compute dft fast fouri transformfast fouri transform orfft implementation dft efﬁcient work value fft ofnthat power mel filter bank log result fft tell energy frequency band human hearing equally sensitive frequency band sensitive high frequency bias low frequency help human recognition formation low frequency like formant crucial distinguish vowel nasal information high frequency like stop burst fricative noise crucial successful recognition model human perceptual property improve speech recognition performance way implement intuition collect energy equally frequency band accord melscale auditory frequency scale mfcc elfrequency cepstral coefficient steven olkmann unit pitch pair sound perceptually equidistant pitch separate equal number mel mel frequency mcan compute raw acoustic frequency log tion implement intuition create bank ﬁlter collect energy frequency band spread logarithmically ﬁne resolution low frequency resolution high frequency figure show sample bank triangular ﬁlter implement idea multiply spectrum mel spectrum mel figure mel ﬁlter bank davis mermelstein triangular ﬁlter spaced logarithmically mel scale collect energy give frequency range finally log mel spectrum value human response signal level logarithmic like human response frequency human sensitive slight difference amplitude high amplitude low tude addition log make feature estimate sensitive variation input power variation speaker mouth move close microphone scalar output particular ﬁlter channel output channel input frame ﬁlterbank vector channel represent log energy particular mel space frequency band send log mel channel vector downstream neural network layer common speech system rescale comparable range common type normalization speech scale input zero mean entire pretraine dataset tion chapter mfcc mel frequency cepstral coefﬁcient themfcc mel frequency cepstral coefﬁcient useful representation mfcc waveform emphasize aspect signal relevant detection phonetic unit mfcc dimensional feature vector consist cepstral coefﬁcient energy coefﬁcient delta cepstral coefﬁcient delta energy coefﬁcient double delta cepstral coefﬁcient double delta energy coefﬁcient sketch feature compute student interested detail encourage follow signal process chapter honetic speech feature extraction cepstrum inverse discrete fouri transform mfcc coefﬁcient base cepstrum way think cepstrum cepstrum useful way separate source andﬁlter recall section speech waveform create glottal source waveform particular fundamental frequency pass vocal tract shape particular ﬁltering characteristic characteristic glottal source fundamental frequency detail glottal pulse etc important distinguish different phone instead useful information phone detection ﬁlter exact position vocal tract know shape vocal tract know phone produce suggest useful feature phone detection ﬁnd way deconvolve separate source ﬁlter vocal tract ﬁlter turn cepstrum way normalise frequency normalise frequency sample figure magnitude spectrum log magnitude spectrum cepstrum taylor permission spectra smoothed spectral envelope lay help visualize spectrum simplicity let consider input log magnitude spectrum ignore mel scaling cepstrum think spectrum log spectrum sound confusing let begin easy log spectrum cepstrum begin standard magnitude spectrum vowel show fig taylor log replace amplitude value magnitude spectrum log show fig step visualize log spectrum waveform word consider log spectrum fig let imagine remove axis label tell spectrum frequency axis imagine deal normal speech signal time axis spectrum pseudo signal notice high frequency repetitive component wave small wave repeat time axis frequency high frequency component cause fundamental frequency signal resent little peak spectrum harmonic signal addition low frequency component pseudo signal example envelope formant structure large peak window low frequency figure show cepstrum spectrum describe log spectrum cepstrum word cepstrum form reverse ﬁrst letter spectrum show sample axis take spectrum log spectrum leave frequency domain spectrum go time domain turn ummary correct unit cepstrum sample examine cepstrum large peak correspond represent glottal pulse component low value axis represent vocal tract ﬁlter position tongue articulator interested detect phone use low cepstral value interested detect pitch use high cepstral value purpose mfcc extraction generally ﬁrst cepstral value coefﬁcient represent information solely vocal tract ﬁlter cleanly separate information glottal source turn cepstral coefﬁcient extremely useful property variance different coefﬁcient tend uncorrelate true spectrum spectral coefﬁcient different frequency band correlate signal processing cepstrum formally deﬁne inverse dft log magnitude dft signal windowe frame speech nkn energy cepstral coefﬁcient prior section add feature energy frame energy useful cue phone detection example vowel sibilant energy stop energy frame sum energy time power sample frame signal xin window time sample time sample energy energy delta feature add feature relate change cepstral feature time change speech signal like slope formant transition change stop closure stop burst provide useful cue phone identity feature cepstral feature plus energy delta delta feature orvelocity feature double delta oracceleration feature delta double delta feature represent change frame correspond cepstral energy feature double delta feature represent change frame correspond delta feature delta simply compute subtract value frame prior value practice common polynomial ﬁrst second derivative summary chapter introduce important concept phonetic putational phonetic represent pronunciation word term unit call phone standard system represent phone international phonetic alphabet oripa common computational system transcription english arpabet conveniently use ascii chapter honetic speech feature extraction phone describe produce articulatorily vocal organ consonant deﬁne term place andmanner lation voicing vowel height backness roundness speech sound describe acoustically sound wave scribe term frequency amplitude perceptual correlate pitch andloudness spectrum sound describe different frequency component phonetic property recognizable waveform human machine rely spectral analysis phone detection spectrogram plot spectrum time owel describe characteristic harmonic call formant historical note major insight articulatory phonetic date linguist india invent concept place manner articulation work glottal mechanism voicing understand concept assimilation ropean science catch indian phonetician year later late century greek rudimentary phonetic knowledge time plato theaetetus andcratylus example guishe vowel consonant stop consonant continuant stoic develop idea syllable aware phonotactic constraint sible word unknown icelandic scholar century exploit concept phoneme propose phonemic writing system icelandic include diacritic length nasality text remain unpublished largely unknown outside scandinavia robin modern era phonetic usually say begin sweet propose essentially phoneme handbook phonetics devise phabet transcription distinguish broad andnarrow transcription propose idea eventually incorporate ipa sweet consider good practice phonetician time ﬁrst scientiﬁc recording language phonetic purpose advance state art articulatory description infamously difﬁcult trait capture henry higgins stage character george bernard shaw model phoneme ﬁrst name polish scholar baudouin courtenay publish theory introductory phonetic textbook include ladefoge clark lop wells deﬁnitive volume source dialect english classic insight acoustic phonetic develop late early highlight include technique like sound spectrograph koenig theoretical insight like work theory issue mapping articulation tic fant steven steven house heinz steven steven house space vowel formant peterson barney understanding phonetic nature stress use duration intensity cue fry basic understanding issue phone perception miller nicely lehiste collection classic paper acoustic phonetic seminal paper gunnar fant collect fant speech feature extraction algorithm develop early include efﬁcient fast fouri transform fft cooley tukey application cepstral processing speech oppenheim development lpc speech coding atal hanauer excellent textbook acoustic phonetic include johnson fog coleman include introduction computational ing acoustic speech linguistic perspective steven lay inﬂuential theory speech sound production number software package acoustic phonetic analysis ﬁgure book erate praat package boersma weenink include pitch praat spectral formant analysis scripting language exercise find mistake arpabet transcription follow word ira gershwin lyric let thing talk nunciation word tomato potato transcribe arpabet pronunciation word transcribe follow word arpabet dark suit greasy wash water waveﬁle choice example textbook website download praat software use transcribe waveﬁle word level arpabet phone praat help play piece waveﬁle look waveﬁle spectrogram record say ﬁve english vowel find chapter utomatic speech recognition chapter speech recognition know meaning lie wordy wavelet voice dim evening shadow brook thomas lovell beddoe understanding speak language transcribe word writing early goal computer language processing fact speech processing predate computer decade ﬁrst machine recognize speech toy radio rex show right celluloid dog move mean spring spring release tic energy roughly ﬁrst formant vowel rex rex come call david selfridge modern time expect automatic system task matic speech recognition asr map waveform like asr appropriate string word time lunch automatic transcription speech speaker environment far solve asr technology mature point viable practical task speech natural interface communicate appliance digital assistant chatbot especially cellphone keyboard convenient asr useful general transcription example matically generate caption audio video text transcribe movie video live discussion transcription important ﬁeld like law dictation play important role finally asr important augmentative nication interaction computer human disability result difﬁcultie inability type audition blind milton famously tat paradise lose daughter henry james dictate later novel repetitive stress injury section introduce goal asr task describe acoustic feature extract introduce convolutional neural net architecture commonly initial layer speech recognition heautomatic speech recognition task introduce family method asr ﬁrst decoder paradigm introduce baseline attention base encoder decoder algorithm call listen attend spell early implementation introduce advanced encoder decoder system openai whisper system radford open system base architecture owsm open whisper style speech model peng el additional capability include translation discuss later second use self supervise speech model call ssl supervise learning like hubert encoder learn abstract representation speech asr pair ctc loss function decode conclude standard word error rate metric evaluate asr automatic speech recognition task describe algorithm asr let talk asr task vary dimension variation vocabulary size asr task long solve extremely high accuracy like word vocabulary yes versus word vocabulary like digit recognition recognize sequencesdigit recognition digit include zero tonine plus open end task like accurately scribe video human conversation large vocabulary word hard second dimension variation speaker talk human ing machine dictate talk dialogue system easy nize human speak human read speech human read read speech loud example audio book relatively easy recognize nize speech human talk conversational speech conversational speech example transcribe business meeting hard human talk machine read audience present simplify speech bit talk slowly clearly dimension variation channel noise speech easy recognize record quiet room head mount microphone record distant microphone noisy city street car window open ﬁnal dimension variation accent speaker class characteristic speech easy recognize speaker speak dialect variety system train speech speaker regional ethnic dialect speech child difﬁcult recognize system train er standard dialect adult speaker number publicly available corpora human create transcript create asr test training set explore variation mention encounter literature librispeech large librispeech open source read speech khz dataset hour audio book libriv project volunteer read record copyright free book panayotov transcript align sentence level divide easy clean difﬁcult portion clean portion high recording quality accent close english division corpus ﬁrst release run speech recognizer train read speech wall street journal audio compute wer speaker base gold transcript divide speaker roughly chapter utomatic speech recognition half recording low wer speaker call clean recording high wer speaker theswitchboard corpus prompt telephone conversation stranger switchboard collect early contain conversation average ute total hour khz speech million word godfrey switchboard singular advantage enormous auxiliary hand linguistic labeling include parse dialogue act tag phonetic prosodic labeling discourse information structure callhome callhome corpus collect late consist unscripted minute telephone conversation native speaker english usually close friend family canavan variety corpora try include input natural chime chime challenge series difﬁcult share task corpora deal robustness asr chime task example asr conversational speech real home environment speciﬁcally dinner party corpus contain recording different dinner party real home participant location kitchen dining area living room record distant microphone ami meeting corpus contain hour record group meeting ami natural meeting specially organize manual transcription additional hand label renal coraal collection coraal sociolinguistic interview african american speaker goal study african american english aae variation language african american community kendall farrington interview anonymize transcript align utterance level wide variety corpora available language chinese example hkust mandarin telephone speech corpus transcribed hkust minute telephone conversation speaker mandarin china clude conversation friend stranger liu corpus contain hour mandarin read speech sentence take domain read different speaker mainly northern china finally multilingual corpora common oice ardila freely available crowd source corpus transcribed read speech store format design asr crowd work volunteer record self read scripted speech script extract wikipedia article recording veriﬁe contributor writing chapter common oice include hour speech language fleur conneau parallel speech dataset build mark goyal sentence extract english wikipedia translate language human translator subset sentence language fleur recording different native speaker read sentence total hour speech language figure show rough percentage incorrect word word error rate wer deﬁne page roughly state art system time writing task note error rate english read speech like librispeech clean audiobook corpus transcription speech read english highly accurate contrast error rate transcribe sation human high switchboard callhome corpora ami meeting error rate high speaker onvolutional neural network like african american english difﬁcult conversational task like transcription speaker dinner party speech error rate high character error rate cer high mandarin natural tion mandarin read speech error rate high low resource language show handful example english task wer librispeech audiobook clean librispeech audiobook switchboard telephone conversation stranger callhome telephone conversation family ami meeting sociolinguistic interview coraal aae dinner party distant microphone sample task language wer common oice vietnamese common oice swahili fleur bengali chinese mandarin task cer mandarin read speech corpus hkust mandarin chinese telephone conversation figure rough word error rate wer word misrecognize report asr american english language recognition task acter error rate cer chinese recognition task convolutional neural network convolutional neural network cnn shorten convnet cnn network architecture particularly useful extract feature speech vision application convolutional layer speech take input tation audio input raw audio mel spectra produce output sequence latent representation input speech asr system like whisper hubert convolutional layer stack initial set layer produce speech representation pass transformer layer standard feedforward layer fully connect input connect ery output contrast convolutional network make use idea kernel kind small network pass input example image tion task pass kernel horizontally vertically image recognize visual feature describe visual dimensional convolutional network speech slide kernel signal time dimension extract speech feature cnn speech convolutional network let ﬂesh intuition bit start schematic version convolutional layer take input single sequence vector produce output single sequence vector length deal complex input output cnn use kernel small vector weight extract feature kernel convolve kernel input convolution kernel chapter utomatic speech recognition signal step flip kernel leave right pass kernel frame frame temporally input frame compute dot product kernel local input value output result sequence dot product think convolution process ﬁnding region signal similar kernel dot product high vector similar convolution operation represent operator unfortunate overloading symbol refer simple multiplication let compute convolution single vector xwith kernel vector let ﬁrst think simple case kernel width compute output element zjas product kernel convolution kernel fig show intuition computation figure schematic view convolution kernel ﬁlter wwhose width kernel walk input output frame ziis dot product kernel input frame kernel length worry ﬂippe kernel dot product scalar product ﬁgure show computation let turn long kernel describe ﬁrst step convolution ﬂippe kernel fact asr system component library like pytorch skip step technically mean algorithm fact convolution instead cross correlation correlation algorithm walk kernel signal compute dot product frame frame ﬂippe ﬁrst difference matter parameter kernel learn training model easily learn kernel parameter order historical reason process convolution cross correlation let general equation long kernel avoid volution undeﬁned left right edge signal pad pad input add small number pof zero beginning end signal start center kernel ﬁrst element deﬁne value left turn simple onvolutional neural network output length input length convenient deﬁne kernel vector have odd number element length center element have pelement element zjof output vector zi compute following dot product fig show computation convolution kernel width padding frame beginning end xwith value zero paddingpadde figure schematic view convolution kernel ﬁlter width padding show zero value add start end signal ﬂippe kernel walk input output frame ziis dot product kernel input window ﬁgure show computation note size receptive ﬁeld kernel design small compare signal example convolutional layer whisper kernel width frame mean kernel vector length kernel receptive ﬁeld mean kernel compare receptive ﬁeld frame speech whisper frame frame represent window speech information mean kernel tracting information speech long extract phonetic feature like formant transition stop closure aspiration describe simpliﬁed view convolution input single vector xand output single vector correspond signal time practice input convolutional layer commonly output log mel spectrum mean channel log mel ﬁlter output kernel separate vector input channel kernel depth mean kernel depth shape output kernel sum input channel single output zcfor input channel xcby convolve kernel chapter utomatic speech recognition sum result output cix output frame integrate information input channel finally output convolution layer complex tor consist single scalar value represent frame instead output convolution layer give input frame need embed latent resentation frame neural model latent representation model dimensionality example model sionality whisper convolutional layer need output channel dimension model order actually learn separate kernel model dimension learn separate kernel kernel have depth number input channel example ﬁlter width way embed representation frame independently compute feature input signal schematic fig log productwithkernel dim figure schematic view convolutional net input channel output channel time point ione kernel kernel depth width dot product log mel spectrum input tor sum produce single value dimension output embed time convolution layer stride stride stride kernel input step ﬁgure stride mean ﬁrst position kernel stride ﬁrst position kernel long stride mean short output sequence stride mean heencoder architecture asr sequence zwill half length input sequence convolutional layer stride great commonly shorten input sequence useful partly short signal take memory computational bandwidth section help address mismatch length acoustic frame embedding letter word cover signal finally practice convolutional layer follow output earity like relu layer encoder decoder architecture asr ﬁrst asr architecture introduce encoder decoder architecture architecture introduce chapter fig sketch ture call attention base encoder decoder oraed orlisten attend spell ae listen attend spell las paper ﬁrst apply speech chorowski chan input architecture xis sequence tacoustic feature vector vector frame start log mel spectral feature describe previous section possible start raw wavﬁle output sequence ycan letter token bpe cepiece assume letter simplify explanation output sequence assume special start sequence end sequence tokens hsosiandheosiand yiis character english choose set encoder decoder feature computationsubsampling dimensional log mel spectrumper frameshorter sequence figure schematic architecture encoder decoder speech recognizer architecture whisper model openai radford fig show subpart whisper architecture whisper speech task like speech translation voice activity detection discuss chapter whisper model inference code publicly lease training code training datum source project use whisper style architecture like open whisper chapter utomatic speech recognition robust speech recognition large scale weak attention log mel spectrogram multitask training formattransform encoder block transformer decoder block token prediction sinusoidal positional encodinglearne positional encodingmultitask training datum sequence learning multitask training formatenglish transcriptionany english speech translationnon english transcriptionno speech ask country ask country rápido zorro marrón salta sobre quick brown fox jump 내려다보면 너무나 내려다보면 너무나 background music play prevspecialtokenstext tokenstimestamptokensstart transcriptlanguage tagno speecheottranscribetranslatebegin timeno timetext tokensbegin timeend timetext tokenstext tokensvoice activity detection vocabulary align transcription text transcription allow dataset specific fine english translation previous text tokensx transcription language identificationmlpself attentionmlpself attentionmlpself attentionmlpcross attentionself attentionmlpcross attentionself attentionmlpcross attentionself cribe figure sequence sequence transformer model train different speech processing task include multilingual speech recognition speech translation speak language identiﬁcation voice activity detection thesetask jointly represent sequence token predict decoder allow single model replace differentstage traditional speech processing pipeline multitask training format use set special token serve task speciﬁer orclassiﬁcation target explain training detailswe train suite model size order studythe scale property whisper anoverview train datum parallelism acceleratorsuse dynamic loss scaling activation check pointing griewank train adamw loshchilov gradient norm clip pascanu linear learn rate decay zero warmup overthe ﬁrst update batch size segment wasuse model train isbetween pass dataset onlytraine epoch large concern use data augmentation regularizationand instead rely diversity contain alarge dataset encourage generalization appendixffor training early development evaluation observe thatwhisper model tendency transcribe plausible butalmost incorrect guess name happen transcript pre trainingdataset include person speak encourage model try predict infor mation rarely inferable recent original release whisper train additionallarge model denote epoch addingspecaugment park stochastic depth huang bpe dropout provilkov result update improved model unlessotherwise speciﬁed figure sketch whisper architecture radford whisper multitask system translation diarization discuss non asr task follow chapter whisper transcription format start script sot token language tag instruction token transcribe translate speech model owsm reproduce reproduce whisper style training offer fully open source toolkit publicly available datum peng input convolutional layer encoder decoder architecture particularly appropriate input output sequence stark length difference speech long acoustic feature sequence map short sequence letter word ample english word average letter bpe token long bostrom durrett natural conversation average word last lisecond yuan frame speech signal frame time long text signal word token length difference extreme speech encoder decoder chitecture speech usually compression stage shorten acoustic feature sequence encoder stage additionally use loss function design deal compression like ctc loss function introduce later goal subsampling produce short sequence input transformer encoder simple baseline algorithm method call low frame rate pundak sainath time iwe low frame rate stack concatenate acoustic feature vector fiwith prior vector new vector time long simply delete instead dimensional acoustic feature vector long vector dimensional short sequence heencoder architecture asr common way create short input sequence use convolutional layer introduce previous section convolutional layer stride great output sequence short input sequence let commonly asr system whisper system radford audio context window second extract channel log mel feature frame dow stride normalize mean range stride frame second mean input frame second context window frame pass convolutional layer follow nonlinearity whisper use gelu gaussian error linear unit smoother version relu ﬁrst convolutional layer input channel use stride number output channel model dimensionality window length second lutional layer number input output channel model dimensionality stride stride second convolutional layer make output sequence half length input sequence bring output window length produce audio token sinusoidal position embedding add audio encoding output end pass transformer encoder hubert hsu use alternative end architecture convolutional layer completely replace computation spectrum input raw sample audio pass seven channel layer stride kernel width learn extract spectral information shorten input sequence representation framerate positional encoding add input gelu layer norm apply output pass transformer encoder inference convolutional stage encoder decoder speech use architecture transformer cross attention let remind encoder decoder architecture introduce chapter use transformer encoder basic transformer chapter decoder addition new layer call cross attention layer encoder take acoustic input map output representation henc stack encoder block decoder essentially conditional language model attend coder representation generate target text letter token timestep conditioning audio representation encoder previously generate text generate new letter token transformer block decoder extra layer special kind attention cross attention cross attention form multi head cross attention attention normal transformer block query usual come previous layer decoder key value come output theencoder standard multi head attention input attention layer cross attention input ﬁnal output encoder henc hencis shape row represent acoustic input token link chapter utomatic speech recognition encoderblock decoderblock lunembedde matrixym multi head attention languagemodele headhenc layer layer normalize causal leave right multi head layer normalize layer figure transformer block encoder decoder show residual stream view ﬁnal output encoder henc context decoder decoder standard transformer extra layer cross attention layer take encoder output hencand use form kandvinput key value encoder query prior layer decoder multiply encoder output hencby cross attention layer key weight value weights query come output prior decoder layer multiply cross attention layer query weights crossattention cross attention allow decoder attend acoustic input jecte entire encoder ﬁnal output representation attention layer decoder block multi head attention layer causal leave right attention see chapter multi head attention coder allow look ahead entire source language text mask inference probability output string yis decompose produce letter output greedy decoding ˆyi argmax heencoder architecture asr alternatively encoder decoder like whisper owsm use beam search describe section particularly relevant add language model add language model encoder decoder model essentially ditional language model encoder decoder implicitly learn language model output domain letter training datum training datum speech pair text transcription include sufﬁcient text train good guage model easy ﬁnd enormous amount pure text training datum ﬁnd text pair speech usually improve model slightly incorporate large language model simple way use beam search ﬁnal beam pothesize sentence beam call good list use good list language model rescore hypothesis beam scoring rescore terpolate score assign language model encoder decoder score create beam weight ltune hold set model prefer short sentence asr system normally way add length factor way normalize probability number character hypothesis jyjc following scoring function listen attend spell chan learn encoder decoder speech train normal cross entropy loss ally conditional language model timestep iof decode loss log probability correct token letter loss entire sentence sum loss loss backpropagate entire end end model train entire encoder decoder describe chapter normally use teacher force decoder history force correct gold yirather predict possible use mixture gold decoder output example gold output time probability take decoder output instead modern datum size large example whisper train corpus hour speech english include hour language datum quality important system scrape web datum training implement method remove asr generate transcript training corpora ﬁltere datum uppercase lowercase open owsm system train hour mainly hand transcribe chapter utomatic speech recognition available datum include dataset librispeech multilingual librispeech common oice fleur switchboard ami peng detail self supervise model hubert alternative encoder decoder architecture class self supervise self supervise speech model model directly learn map acoustic input string letter token instead ﬁrst bootstrap set discrete phonetic unit acoustic input learn map waveform induced unit pretraine phase require transcript unlabeled speech ﬁle pretraine model ﬁnetune asr small set label datum audio pair transcript model advantage advantage large amount untranscribed audio training introduce self supervise model call hubert hsu hubert hubert similar model like baevski use intuition mask language model like bert introduce chapter mask input train model guess hide mask cnn layercosine class figure schematic architecture hubert inference pass training wavﬁle pass series convolutional layer frame replace mask token sequence pass transformer stack linear layer project transformer output output embed embed pare cosine embedding phonetic class produce logit pass softmax probability distribution class elf supervise model hubert hubert forward pass let ﬁrst forward pass hubert training training context backwards pass discuss early input hubert forward pass raw waveﬁle input output time frame probability distribution set induced phonetic class class class depend stage fig show sketch component waveﬁle pass channel convolutional layer learn extract spectral information shorten input sequence frame positional encoding add gelu layer norm select token replace mask token train embed share mask frame sequence pass transformer stack output pass linear projection layer output embed frame compare cosine embedding phonetic class result set logit represent similarity current audio timestep class pass softmax probability distribution class learn hubert let ﬁrst discuss induce phonetic class target training bootstrap unit hubert start mel frequency cepstral coefﬁcient ormfcc vector dimensional feature vector emphasize pect signal relevant detection phonetic unit vector extract acoustic signal summarize section tract mfcc vector entire acoustic training dataset original hubert implementation hour librispeech datum result million tor cluster mfcc vector mean cluster algorithm describe section clustering mean group vector class output clustering codebook ofkvector call codeword template orprototype represent cluster kcluster acoustic unit use gold target training let consider entire training process acoustic input run cnn layer span token context window choose mask token cnn output replace mask embed entire context window pass transformer layer output tat timestep tis multiply projection layer matrix ato project class embed space result representation pare embedding class cosine softmax temperature parameter turn similarity probability fig show parallel forward pass input waveform pass mfcc create dimensional vector map class choose similar centroid codebook loss function sum set mask tokens probability model assign correct chapter utomatic speech recognition mask language modeling model train predict unit associate mask frame loss backpropagate model cnn class projection layer acosine class network train produce acoustic training signal figure ﬁrst phase hubert training codebook unit deﬁne ter dimensional mfcc vector target training timestep compute probability class use logprob loss model initially train map mfcc vector centroid second stage training occur representation produce model cluster cluster use instead target training intuition initial mfcc cluster bias model phonetic representation training model learn accurate ﬁne grain representation fig show intuition hubert pretraine projection cosine layer remove randomly initialize linear softmax layer add mapping class correspond english letter extra character asr task cnn frozen rest model ﬁnetune asr ctc loss function describe section mean cluster section mean cluster algorithm formally mean mean family algorithm group set vector datum kcluster cluster useful want treat group element way speech processing commonly need convert set vector real value set discrete symbol use hubert return chapter algorithm create discrete acoustic token tts generally use mean mean simple version family step iterative algorithm give set nvector dimension constant usually elf supervise model hubert means clusteringmfccentire training datacodebook mean clusteringhubert forward pass layer training datacodebook stage class inductionstage class figure create target stage hubert training ﬁrst stage acoustic unit create compute dimensional mfcc vector entire training datum cluster mean second stage unit create pass subsample training datum hubert model ﬁrst stage training take output intermediate transformer layer layer cluster mean cnn layer letter figure hubert ﬁnetune pass pretraine projection layer sine step remove leave randomly initialize projection softmax layer cnn layer frozen rest model ﬁnetune dataset audio script train ctc loss section produce letter output parameter update ﬁnetune show red projection layer transformer stack step algorithm base iteratively update set kcentroid tor centroid geometric center set point dimensional space centroid algorithm step assignment step give set kcurrent troid dataset vector assign vector cluster codeword close square euclidean distance estimation step put codeword cluster recompute mean vector note result mean vector need actual point dataset iterate chapter utomatic speech recognition forth step algorithm initialization cluster kchoose random vector codeword call template orprototype cluster result codeword template prototypecodebook codeword kcluster codebookthen repeat iteratively convergence vector dataset assign kcluster choose near codeword simply deﬁne nearest cluster codeword small square euclidean distance wherejjvjji norm vectorpd estimation estimate codeword cluster recompute mean centroid vector cluster sii set vector cluster jsijx ctc point previous section speech recognition particular property appropriate encoder decoder architecture encoder produce encoding input decoder use attention explore speech long acoustic input sequence xmappe short sequence letter second hard know exactly xmap section brieﬂy introduce alternative encoder decoder rithm loss function call ctc short connectionist temporal ctc deal problem different way intuition ctc output single character frame input output length input apply collapse function combine sequence identical letter result short sequence let imagine inference say word dinner let suppose function choose probable letter input spectral frame representation sequence letter correspond input frame alignment tell acoustic signal letter align alignment fig show alignment happen use collapse function remove consecutive duplicate letter work naive algorithm transcribe speech diner notdinner collapse handle double letter problem naive function tell symbol align silence input want transcribe silence random letter ctc algorithm solve problem add transcription alphabet special symbol blank represent blank ctc figure naive algorithm collapse alignment input letter alignment want transcribe letter blank letter collapse function collapse consecutive duplicate letter collapse formally let deﬁne mapping alignment aand output collapse repeat letter remove blank fig sketch collapse function duplicatesdinerny figure ctc collapse function show space blank character peate consecutive character alignment aare remove form output ctc collapse function lot different alignment map output string example alignment show fig alignment result string dinner fig show alignment produce output dinnneeerrr ddinnnerr dddinnnerr figure legitimate alignment produce transcript dinner useful think set alignment produce output use inverse bfunction call represent set ctc inference compute ﬁrst ctc assign bility particular alignment ˆang ctc make strong conditional independence assumption assume give input ctc model chapter utomatic speech recognition atat time tis independent output label time ﬁnd good alignment ˆatgwe greedily choose ter max probability time step ˆat argmax pass result sequence ato ctc collapse function bto output sequence let talk simple inference algorithm ﬁnde good ment implement make decision time point treat ctc sequence modeling task output letter ˆytat time tcorresponde input token eliminate need coder fig sketch architecture encoder produce hidden state htat timestep decode take softmax character vocabulary time step encoder feature computationsubsampling log mel spectrumshorter inputsequence output lettersequence figure inference ctc encoder model decode simple softmaxe hidden state htat output step alas potential ﬂaw inference algorithm sketch fig problem choose likely alignment likely alignment correspond likely ﬁnal collapse output string possible alignment lead output string likely output string correspond probable alignment example imagine probable alignment afor input string probable alignment output sum alignment probable reason probable output sequence yis single good ctc alignment high sum probability ctc possible alignment argmax alas sum alignment expensive lot alignment approximate sum version viterbi beam search cleverly keep beam high probability alignment map output string sum approximation hannun clear explanation extension beam search ctc strong conditional independence assumption mention early output time tis independent output time give input ctc implicitly learn language model datum unlike base encoder decoder architecture essential ctc interpolate language model sort length factor tion weight train devset score ctc training train ctc base asr system use negative log likelihood loss special ctc loss function loss entire dataset dis sum negative log likelihood correct output yfor input lctc compute ctc loss function single input pair need probability output ygiven input see compute probability give output ywe need sum possible alignment collapse word naively sum possible alignment feasible alignment efﬁciently compute sum dynamic gramme merge alignment version forward backward rithm train hmms appendix crf original dynamic gramming algorithm training inference lay grave hannun detailed explanation combine ctc encoder decoder possible combine architecture loss function describe cross entropy loss encoder decoder architecture ctc chapter utomatic speech recognition fig show sketch training simply weight loss ltuned devset inference combine language model length penalty learn weight argmax encoder decoder ctc lossencoder decoder loss figure combine ctc encoder decoder loss function stream model rnn improve ctc strong independence assumption ctc assume output time tis independent output time recognizer base ctc achieve high accuracy attention base encoder decoder nizer ctc recognizer advantage streaming streaming mean recognize word line wait streaming end sentence recognize streaming crucial tion command dictation want start recognition user talk algorithm use attention need compute hidden state sequence entire input ﬁrst order provide attention distribution text decoder start decode contrast ctc algorithm input letter leave right immediately want streaming need way improve ctc recognition conditional independent assumption enable know output tory rnn transducer rnn show fig model rnn grave grave rnn main component ctc acoustic model separate language model component call predictor condition output token history time step ctc encoder output hidden state henc tgiven input language model predictor take previous output token count blank output hide state hpre pass network output pass softmax predict character asr valuation ord error rate encoderp networkhencthpredusoftmaxzt udecoder figure rnn model compute output token distribution time tby grate output ctc acoustic encoder separate predictor language model asr evaluation word error rate standard evaluation metric speech recognition system word error word error rate word error rate base word string return recognizer hypothesize word string differ reference transcription ﬁrst step compute word error compute minimum edit distance word hypothesized correct string give minimum ber word substitution word insertion word deletion necessary map correct hypothesized string word error rate wer deﬁne follow note equation include insertion error rate great word error rate substitution deletion total word correct transcript sample alignment reference hypothesis utterance alignment callhome corpus show count compute error rate ref phone leave portable phone upstairs night hyp get full love portable form store night eval utterance substitution insertion deletion word error rate standard method compute word error rate free script call sclite available national institute standard technology nist nist sclite give series reference hand transcribe gold standard tence matching set hypothesis sentence perform alignment compute word error rate sclite perform number useful task example error analysis give useful information confusion matrix show word misrecognize summarize statistic word insert delete sclite give error rate speaker sentence label speaker useful statistic like sentence error rate percentage sentence word error chapter utomatic speech recognition text normalization evaluation normal system normalize text compute word error rate variety package implement normalization rule example standard english normalization rule include remove metalanguage non language note transcription comment occur match bracket remove standardize interjection ﬁlled pause err standardize contract non contract form english normalize non standard word number quantity date time dollar unify spell convention statistical signiﬁcance asr mapsswe macnemar language processing algorithm need know particular improvement word error rate signiﬁcant standard statistical test determine word error rate different match pair sentence segment word error mapsswe test introduce gillick cox mapsswe test parametric test look difference number word error system produce average number segment segment short long entire utterance general want large number short segment order justify normality assumption maximize power test require error segment statistically independent error segment asr system tend use trigram lm approximate requirement deﬁne segment region bound side word recognizer correct turn utterance boundary example nist region iii ref sys bad sys test region system error deletion insertion system zero region iii system error substitution system let deﬁne sequence variable zrepresente difference error system follow athe number error segment iby system number error segment iby system nis number segment example sequence zvalues intuitively system identical expect average difference average zvalue zero true average difference muz like know follow closely original proposal notation gillick cox estimate true average limited sample ˆmz estimate variance ummary let large approximately normal distribution unit variance null hypothesis reject tail tail zi standard normal wis realize value probability look standard table normal distribution early work mcnemar test signiﬁcance mcnemar mcnemar test applicable error system independent true continuous speech recognition error word extremely dependent error neighboring word improve word error rate metric nice ple equal weight word value content word like tuesday function word like aorof researcher generally agree good idea prove difﬁcult agree metric work application asr summary chapter introduce fundamental algorithm automatic speech recognition asr task speech recognition speech text map acoustic form sequence grapheme input speech recognizer series acoustic wave plead quantize convert spectral representation like log mel spectrum common paradigm speech recognition encoder decoder attention model model base ctc loss function base model high accuracy model base ctc easily adapt streaming output grapheme online instead wait acoustic input complete asr evaluate word error rate edit distance hypothesis gold transcription historical note number speech recognition system develop late early early bell lab system recognize digit single speaker davis system speaker dependent store pattern digit roughly represent ﬁrst vowel chapter utomatic speech recognition digit achieve accuracy choose pattern high relative correlation coefﬁcient input fry dene build phoneme recognizer university college london recognize vowel consonant base similar pattern recognition principle fry dene system ﬁrst use phoneme transition probability strain recognizer late early produce number important paradigm shift number feature extraction algorithm include efﬁcient fast fouri transform fft cooley tukey application cepstral cesse speech oppenheim development lpc speech coding atal hanauer second number way handle ing stretch shrink input signal handle difference speak rate warping segment length match store pattern natural algorithm solve problem dynamic programming see appendix algorithm reinvent multiple time address problem ﬁrst tion speech processing vintsyuk result pick researcher reinvent velichko zagoruyko sakoe chiba soon afterward itakura combine dynamic programming idea lpc coefﬁcient previously speech coding result system extract lpc feature incoming word dynamic programming match store lpc template non probabilistic use dynamic programming match template come speech call dynamic time warp time warp innovation period rise hmm hide markov model apply speech independently laboratory application arise work statistician particular baum colleague institute defense analysis princeton apply hmms prediction problem baum petrie baum eagon james baker learn work apply algorithm speech processing baker graduate work cmu independently frederick jelinek collaborator draw research information theoretical model inﬂuence work shannon apply hmms speech ibm thomas watson research center jelinek early difference decode algorithm baker dragon system viterbi dynamic ming decode ibm system apply jelinek stack decode algorithm jelinek baker join ibm group brief time found speech recognition company dragon system use hmm gaussian mixture model gmms phonetic component slowly spread speech community dominant paradigm cause encouragement arpa advanced research project agency department defense arpa start ﬁve year program build word constrained grammar speaker speech understanding klatt fund compete system carnegie mellon university harpy system lowerre ﬁed version baker hmm base dragon system good test tem arpa darpa fund number new speech research program begin word speaker independent read speech task like resource management price recognition sentence read wall street journal wsj broadcast news domain ldc graff transcription actual news broadcast include difﬁcult passage street inter historical note view switchboard callhome callfriend fisher domain godfrey cieri natural telephone conversation friend stranger arpa task involve approximately annual bakeoff bakeoff system evaluate arpa competition result wide scale borrowing technique lab easy idea reduce error previous year competition probably portant factor eventual spread hmm paradigm neural alternative hmm gmm architecture asr arise base number early experiment neural network phoneme recognition speech task architecture include time delay neural network tdnn ﬁrst use convolutional network speech waibel lang rnns robinson fallside hybrid hybrid hmm mlp architecture feedforward neural network train netic classiﬁer output probability estimate hmm base architecture morgan bourlard bourlard morgan morgan bourlard hybrid system show performance close standard hmm gmm model problem speed large hybrid model slow train cpus era example large hybrid system feedforward network limit hidden layer unit produce probability dozen monophone train model require research group sign special hardware board vector processing morgan bourlard later analytic study show performance simple feedforward mlp asr increase sharply hide layer control total number parameter maas computational resource time insufﬁcient layer decade combination moore law rise gpu allow deep neural network layer performance get close traditional system small task like timit phone recognition ham performance hybrid system surpass traditional hmm gmm system jaitly dahl inter alia originally unsupervised pretraining network nique like deep belief network important clear hybrid hmm gmm feedforward network matter use lot datum layer component improve performance ing log mel feature instead mfccs dropout rectiﬁed linear unit deng maas dahl early work propose ctc loss function grave rnn transducer deﬁne apply phone recognition grave grave end end speech nition rescoring grave jaitly recognition maas advance specialized beam search hannun scription ctc chapter draw hannun encourage interested reader follow encoder decoder architecture apply speech time different group listen attend spell system chan attention base encoder decoder architecture chorowski bahdanau transformer include decoder architecture karita nice comparison rnns former encoder architecture asr tts speech speech chapter utomatic speech recognition popular toolkit speech processing include kaldi povey kaldi espnet watanabe hayashi espnet exerciseschapter speech word mean set paper take human voice infuse shade deep meaning maya angelou know cage bird sing task mapping text speech task long history speech text vienna wolfgang von kempelen build empress maria theresa famous mechanical turk chess play automaton consist wooden box ﬁlle gear sit robot mannequin play chess move piece mechanical arm turk tour europe america decade defeat napoleon bonaparte play charle bage mechanical turk early success artiﬁcial intelligence fact alas hoax power human chess player hide inside box know von kempelen extraordinarily proliﬁc inventor build deﬁnitely hoax ﬁrst sentence speech synthesizer show partially right device consist bellow simulate lung ber mouthpiece nose aperture reed simulate vocal fold ious whistle fricative small auxiliary bellow provide puff air plosive move lever hand open close aperture adjust ﬂexible leather cal tract operator produce different consonant vowel century later long build synthesizer wood leather need human operator modern task text speech text speech tt call speech synthesis exactly reverse asr map text tts speech synthesis time lunch acoustic waveform tts wide variety application speak language model interact people read text loud game produce speech sufferer neurological disorder like late astrophysicist steven hawking lose use voice al chapter introduce algorithm tt like asr algorithm prior chapter train enormous amount speech dataset brieﬂy touch speech chapter ext speech tt overview task text speech generate speech waveform correspond desire text particular voice speciﬁed user historically tts collect hundred hour speech single talker lab train large system result tts system work voice want second voice go collect datum second talker modern method instead train speaker independent synthesizer ten thousand hour speech thousand talker create speech new voice unseen training use small speech desire talker guide creation voice input modern tts system text prompt second speech voice like generate speech tts task call zero shot tts desire voice zero shot tt see training way modern tts system address task use language modeling particular conditional generation intuition enormous dataset speech use audio tokenizer base audio codec induce discrete dio token speech represent speech train language model vocabulary include speech token text token train language model input sequence text transcript small sample speech desire talker tokenize text speech discrete token conditionally generate discrete sample speech correspond text string desire voice inference time prompt language model tokenized text string sample desire voice tokenize codec discrete audio token conditionally generate produce desire audio token token convert waveform ieee transaction audio speech language processing vol neural codec language model zero shot text speech synthesizer sanyuan chen ziqiang zhang long zhou shujie liu member ieee zhuo chen yanqe liu huame wang jinyu lei sheng zhao furu weiabstract introduce language modeling approach text speech synthesis tts speciﬁcally train neural codec language model call discrete code derive shelf neural audio codec model regard tts conditional language modeling task continuous signal regression previous work pre training stage scale tts training datum hour english speech hundred time large exist system emerge context learning capability synthesize quality personalize speech second enrolled recording unseen speaker prompt experiment result signiﬁcantly outperform state art zero shot tts system term speech naturalness speaker similarity addition ﬁnd preserve speaker emotion acoustic environment prompt synthesis index term zero shot text speech synthesis speech generation voice cloning language modeling pre training context learn introduction decade yield dramatic breakthrough speech synthesis development neural work end end modeling currently cascade text speech tts system usually leverage pipeline acoustic model vocoder mel spectrogram intermediate representation advanced tt system synthesize high quality speech single tiple speaker require high quality clean datum recording studio large scale datum crawl internet meet requirement lead performance degradation training datum relatively small current tts system suffer poor generalization capability speaker similarity speech naturalness drop matically unseen speaker zero shot scenario tackle zero shot tts problem exist work leverage speaker receive march revise september accept january date publication january date current version february associate editor coordinate review article approve publication raul fernandez sanyuan chen chengyi wang contribute equally work correspond author shujie liu sanyuan chen computer science technology harbin institute technology harbin china chengyi wang nankai university tianjin china ziqiang zhang university science technology china hefei china long zhou shujie liu zhuo chen yanqing liu huame wang jinyu lei sheng zhao furu wei microsoft corporation redmond mail digital object identiﬁer fig overview unlike previous pipeline text mel spectrogram pipeline text code generate discrete audio codec code base text input acoustic code prompt correspond target content speaker voice adaptation speaker encoding method require additional ﬁne tuning complex pre design feature heavy structure engineering instead design complex speciﬁc network problem ultimate solution train model large diverse datum possible motivate success ﬁeld text generation notable performance improvement datum increase text language model uncompressed text transfer success ﬁeld speech synthesis troduce ﬁrst language model base tts framework leverage large diverse multi speaker speech datum show fig shot tts generate correspond acoustic code base acoustic code second enrolled recording text input constrain speaker content information respectively finally generate acoustic code synthesize ﬁnal waveform correspond neural codec decoder code derive audio codec model allow treat tts conditional codec language modeling approach enable employ advanced prompting base large model technique gpt train vall libriheavy corpus label sion librilight corpus consist hour english speech derive open source audio book unique speaker compare previous tts training dataset ieee right reserve include right text data mining training artiﬁcial intelligence similar technologi personal use permit republication redistribution require ieee permission html information authorize license use limit stanford university library download august utc ieee xplore restriction apply figure architecture personalized tts ﬁgure chen fig chen show intuition tts system call train hour english speech unique talker system like component audio tokenizer generally base audio codec system sing codec learn discrete audio token scribe section codec part encoder turn speech embed vector quantizer turn embedding discrete token decoder turn discrete token speech stage conditional language model generate audio token respond desire text sketch section codec learn discrete audio token modern tts system base convert waveform sequence discrete audio token idea manipulate discrete audio token useful speech enable system like speak language model text speech input generate text speech output solve task like speech translation diarization speak question answer have discrete token mean use language model technology language model specialize sequence discrete token audio tokenizer important component modern speech toolkit standard way learn audio token neural audio codec word codec form coder decoder historically codec hardware device digitize analog symbol generally use word mean mechanism encode analog speech signal digitize compressed representation efﬁciently store send codec compression tt spoken language model employ convert speech discrete token course digital representation speech describe chapter ready discrete example khz speech store bit format think series symbol symbol second speech system generate symbol second make speech signal long feasibly process language model especially base transformer inefﬁcient quadratic attention instead want symbol represent long chunk speech order token second figure standard architecture audio tokenizer perform inference ﬁgure adapt mousavi input waveform xis encode generally series downsample convolution network series embedding embed pass quantizer produce series quantize tokens regenerate speech signal quantize token map vector zqtand encode ally series upsampling convolution network waveform discuss architecture train section fig adapt mousavi show standard architecture audio tokenizer audio tokenizer input audio waveform chapter ext speech train recreate audio waveform intermediate representation consist discrete token create vector quantization audio tokenizer stage encoder map acoustic waveform series tvalue sequence tembedding typically time small vector quantizer take embed ztcorresponding waveform represent sequence discrete token take nccodebook vector quantizer sum vector codeword codebook create quantizer output vector zqt decoder generate lossy reconstructed waveform span ˆxfrom quantizer output vector zqt audio tokenizer generally learn end end loss function ward tokenization allow system reconstruct input waveform follow subsection component particular tokenizer ncodec tokenizer efossez encoder decoder encodec model encoderblock stride unit waveform decoderblock waveform decoderblock unitdecoderblock figure encoder decoder stage ncodec model goal encoder sample input waveform encode series embedding ztat embedding second original signal represent downsampling time encoder decoder quantization step produce lossy embed zqt goal decoder lossy embed zqtand upsample convert waveform encoder decoder ncodec model efossez sketch fig goal encoder downsample span waveform time second speech real value embed representation ztat second audio represent sing codec learn discrete audio token vector dimensionality purpose explanation use downsampling accomplish have series encoder block convolutional layer stride large iteratively ple audio discuss end section convolution block sketch fig include long series convolution residual unit add convolution prior input output encoder embed ztat time produce second embed quantize discuss section turn embed ztinto series ncdiscrete symbol turn series symbol new quantizer output vector zqt nally decoder take output embed quantizer zqtand generate waveform symmetric set convnet upsample audio summary waveform come encode downsample vector ztof dimensionality quantize discrete symbol turn vector zqtof dimensionality decode upsample vector waveform vector quantization goal vector quantization orvqstep turn series vector avector quantization series discrete symbol historically vector quantization gray compress speech signal reduce bit rate transmission storage compress sequence vector representation speech turn vector integer index represent class cluster instead transmit big vector ﬂoate point number transmit integer index end transmission reconstitute vector index tts modern speech application use vector quantization different reason conveniently create discrete token language modeling paradigm language model predict sequence discrete token practice ncodec model audio tokenizer use ful vector quantization call residual vector quantization deﬁne following section helpful ﬁrst basic algorithm extend vector quantization training phase inference phase introduce core basic train algorithm describe means clustering vector section mean clustering common algorithm implement review training run big set speech waveﬁle encoder generate nvector correspond frame speech cluster nvector kcluster kis set designer parameter algorithm number discrete symbol want generally simple algorithm use iterative mean algorithm learn cluster recall section mean step algorithm base iteratively update set kcentroid vector centroid geometric center set point dimensional space centroid mean algorithm cluster start assign random vector cluster iterative step assignment step give set kcurrent centroid entire dataset vector vector assign cluster codeword close square euclidean distance chapter ext speech vectorto quantizeroutput discrete symbol figure basic algorithm inference time codebook learn input span speech encode encoder vector dimensionality vector compare codeword cluster centroid codebook codeword cluster similar output discrete representation vector estimation step codeword cluster recompute recalculate new mean vector result cluster centroid slowly adjust training space iterate forth step algorithm converge end end training discuss case instead iterative mean instead recompute mean minibatch training online algorithm like exponential move average end clustering cluster index discrete symbol cluster associate codeword vector centroid codeword vector cluster list cluster id tokens codeword codebook cluster code codebook code inference new vector come compare vector codebook whichever codeword close assign codeword associate cluster fig show intuition inference step context speech encoding input speech waveform encode vector input vector vis compare possible codeword codebook find similar codeword output discrete symbol representation train ncodec model end end need way turn discrete symbol waveform simple directly codeword cluster pass codeword decoder reconstruct waveform course codeword vector exactly match original vector encoding input speech span especially possible codeword hope close codebook good decoder produce reasonable speech nonetheless powerful method usually sing codec learn discrete audio token residual vector quantization practice simple produce good reconstruction codebook size codeword vector represent wide variety embedding encode possible speech form ncodec model audio tokenization method use instead sophisticated variant call residual vector quantization orresidual vector quantization rvq residual vector quantization use multiple codebook arrange kind rvq hierarchy figure neural audio codec model revisit rvq employ ﬁrst quantizer play important role reconstruction impact gradually decrease explicitly control content speech synthesis direction apply pre training neural tts chung pre train speech decoder tts autoregressive mel spectrogram prediction author propose uniﬁed modal encoder decoder framework leverage unlabeled speech text datum pre train component tts model tjandra quantize unlabeled speech discrete token vqv model van den oord train model token speech sequence demonstrate pre train model require small real datum ﬁne tuning bai propose mask reconstruction mel spectrogram show well performance speech editing synthesis previous tts pre training work leverage hour datum pre train hour datum furthermore ﬁrst use audio codec code intermediate representation emerge context learning capability zero shot tts background speech quantization audio typically store sequence bit integer value generative model require output timestep synthesize raw audio addition audio sample rate exceed thousand lead extraordinarily long sequence length make intractable raw audio synthesis end speech quantization require compress integer value sequence length transformation quantize timestep value reconstruct high quality raw audio widely speech generative model wavenet van den oord inference speed slow sequence length reduce recently vector quantization widely apply self supervise speech model feature extraction baevski hubert hsu follow work lakhotia show code self supervise model reconstruct content inference speed fast wavenet speaker identity discard reconstruction quality low borsos audiolm borsos train speech speech language model mean token self supervise model acoustic token neural codec model lead high quality speech speech generation paper follow audiolm borsos leverage neural codec model represent speech discrete token compress audio network transmission codec model able encode waveform discrete acoustic code reconstruct high quality waveform speaker unseen training compare traditional audio codec approach neural base codec signiﬁcantly well low bitrate believe quantize token contain sufﬁcient information speaker recording condition compare quantization method audio codec show following advantage contain abundant speaker information acoustic information maintain speaker identity reconstruction compare hubert code hsu shelf codec decoder convert discrete token waveform additional effort vocoder training like base method operate spectrum reduce length time step efﬁciency address problem transformation van den oord figure residual ﬁgure chen run encoder embed produce discrete symbol correspond codeword look residual difference encoder output embed ztand codeword choose second codebook run residual repeat process token idea simple run standard codebook fig prior section input embed ztwe codeword vector produce let zta quantiﬁe codebook difference residual residual error original vector residual capture residual kind rounding error round vector near codeword create error residual vector pass vector quantizer give second codeword represent residual vector residual second codeword total result codeword original codeword residual mean rvq represent original speech span sequence discrete symbol instead discrete symbol basic fig show intuition want reconstruct speech method encodec rvq simple codeword add result vector zqtis pass decoder generate waveform train encodec model audio token ncodec model like similar audio tokenizer model train end end input waveform span speech second extract long original waveform desire output waveform span chapter ext speech model kind autoencoder learn map model train reconstruction large speech dataset like common oice ardila hour speech language audio datum like audio set gemmeke million sec excerpt youtube video label large ontology include natural animal machine sound music figure architecture audio tokenizer training ﬁgure adapt mousavi audio tokenizer train weighted combination loss function summarize ﬁgure describe ncodec model like audio tokenizer train number loss function suggest fig reconstruction loss lreconstruction mea reconstruction loss sure similar output waveform input waveform example sum square difference original reconstructed audio lreconstruction similarity additionally measure frequency domain compare original reconstructed mel spectrogram sum square distance distance combination kind loss adversarial loss lgan loss train adversarial loss generative adversarial network generator binary discriminator classiﬁer distinguish true waveﬁle xand generate want train model fool discriminator well discriminator bad reconstruction use discriminator success loss function incorporate feature generator finally need loss quantizer have quantizer middle end end training cause problem propagation gradient backward pass training quantization step differentiable deal problem way ignore quantization step backward pass instead copy gradient output quantizer zqt input quantizer method call straight estimator van den oord need method sure code word vector quantizer step update training method start means clustering vector ztto initial clustering add loss component lvq function difference vall enerate audio stage output vector ztand reconstructed vector quantization zqt codeword sum nccodebook residual qtjj total loss function weighted sum loss generate audio stage summarize introduction structure tts system like input text synthesize sample voice tokenize bpe text audio codec speech use language model conditionally generate discrete audio token correspond text prompt voice speech sample autoregressive transformertextaudio non autoregressivenon autoregressivenon autoregressiveoutput code sequence figure stage language modelling approach show inference stage autoregressive transformer ﬁrst non autoregressive er output sequence discrete audio code generate stage toregressive generate code ﬁrst quantizer left right non autoregressive model call time generate remain code condition code precede quantizer include conditioning code right instead conditional generation single autoregressive guage model conditional generation stage process distinct language model architectural choice inﬂuence chical nature rvq quantizer generate audio token output ﬁrst rvq quantizer important token ﬁnal speech quent quantizer contribute residual information ﬁnal signal chapter ext speech language model generate acoustic code stage sive generate ﬁrst quantizer code entire output sequence give input text enrol audio give code non autoregressive run time time take input output initial autoregressive code prior non autoregressive quantizer generate code remain quantizer fig show intuition inference step let architecture bit detail training give audio sample yand tokenized text transcription use pretraine ncodec convert yinto code matrix let tbe number downsample vector output ncodec code vector represent encoder output encodec cis dimensional acoustic code matrix entry column represent time row represent different quantizer row vector matrix contain code frame column vector code sequence vector quantizer give text xand audio train tts conditional code language model maximize likelihood cconditione chen neural codec language model zero shot text speech synthesizer fig training overview regard tts conditional codec language modeling task structure conditional codec languag model hierarchical structure model generate code ﬁrst code sequence autoregressive manner nar model generate remain code sequence base previous code sequence non autoregressive manner hierarchical structure nar model introduce section iii neural audio codec model rvq exhibit key property single speech sample encode multiple code quence multiple quantizer audio codec model ﬁrst quantizer cover acoustic information subsequent code sequence contain residual acoustic mation predecessor serve reﬁne augment acoustic detail inspire property design conditional codec language model hierarchical structure autoregressive codec language model autoregressive nar codec language model model generate sequence ﬁrst code frame autoregressive manner nar model generate remain code sequence base previous code sequence non autoregressive manner model utilize transformer architecture text embed layer code embed layer code prediction layer use distinct embedding code different codec quantizer share parameter code prediction layer parameter code embed layer compare model nar model code embed layer specify code sequence predict model nar model different attention mask strategy model use causal attention strategy nar model use attention strategy show right fig combination nar model offer balance speech quality inference speed model tribute well speech quality ﬂexibility acoustic sequence length prediction ensure rate generate speech match enrol recording particularly useful give difﬁculty train length predictor different speaker speak speed vary signiﬁcantly hand nar model enhance inference speed acoustic sequence length determine model nar model generate code timestepssimultaneously reduce time complexity training conditional codec language modeling depict fig tional codec language modeling method noteworthy training require simple utterance wise audio transcription pair datum complex datum force alignment information additional audio clip speaker reference greatly simpliﬁes process collect process training datum facilitate scalability speciﬁcally audio corresponding transcription training dataset initially utilize audio codec encoder text tokenizer obtain codec matrix text sequence respectively train model nar model conditional codec language modeling method autoregressive codec language modeling model train predict ﬁrst code sequence text sequence xin autoregressive manner show low middle fig ﬁrst obtain text embed sequence code embed sequence text embed matrix wxand code embed matrix respectively index selection concatenate text bed sequence code embed sequence insert embedding special token eos bos tween concatenation separately add learnable position embed text embed sequence authorize license use limit stanford university library download august utc ieee xplore restriction apply figure training procedure give text prompt autoregressive transformer ﬁrst train generate code ﬁrst quantizer code sequence gressively non autoregressive transformer generate rest code figure chen fig show intuition left audio sample transcription tokenize append eos bos token toxand eos token end cand train autoregressive transformer predict acoustic token start eos autoregressive transformer ﬁll token inference give text sequence speak roll speech sample unseen speaker tts valuation transcript ﬁrst run codec acoustic code matrix becp concatenate transcription text sequence speak create total input text pass text tokenizer stage tokenized text xand tokenized audio prompt generate text quence xand prompt argmax argmax ctty generate token ctcan convert ncodec decoder waveform fig show intuition ieee transaction audio speech language processing vol code embed sequence model feed eand train predict correspond code sequence eos append end code prediction layer causal attention mask strategy prediction token attend text sequence xand previous code right fig overall model θaris optimize minimize negative log likelihood ﬁrst code sequence tione text sequence productdisplay non autoregressive codec language modeling give ﬁrst code sequence generate model nar model train generate remain code sequence text sequence xand precede code sequence non autoregressive manner access code sequence prompt inference well model speaker information prompt training explicitly split code matrix acoustic condition target code matrix randomly sample length predict target code sequence text sequence precede code sequence target code matrix non autoregressive manner show upper middle fig ﬁrst obtain text embed sequence text embed matrix code embed sequence obtain code embedding acoustic condition target code matrix code embed matrix obtain code embed eidwith code embed matrix wid eid concatenate text embed sequence code bed sequence code embed insert embedding special token eos middle separately add learnable position embed text embed sequence code embed sequence similar model nar model feed eand train predict correspond code sequence prediction token jcan attend entire input sequence depict upper right fig fig inference overview perform zero shot tts e conditional codec language model overall nar model optimize minimize negative log likelihood target code sequence condition text sequence acoustic condition precede code sequence summationdisplay practice optimize computational efﬁciency training calculate training loss iterate value jand aggregate correspond loss instead training step randomly select optimize model training loss inference context learn prompt context learning remarkable capability text base language model enable predict label unseen input require additional parameter update tts model consider possess context learning capability synthesize high quality speech unseen speaker need ﬁne tuning previous tts system exhibit limited context learning capability tat additional ﬁne tuning suffer signiﬁcant degradation performance unseen speaker language model prompt necessary enable context learn zero shot scenario work propose perform zero shot tts task prompt inference depict fig enrol speech sample unseen speaker correspond transcription ﬁrst concatenate speech transcription text sentence concatenate text encode text sequence xuse text tokenizer serve text condition speech sample convert code matrix audio codec encoder serve prompt prompt authorize license use limit stanford university library download august utc ieee xplore restriction apply figure inference procedure figure chen script second enrol speech ﬁrst prepende text generate speech text tokenize autoregressive transformer start generate ﬁrst code transcript acoustic prompt chen detail transformer component detail training tts evaluation tts system evaluate human play utterance listener e mean opinion score mos rating good synthesized mos utterance usually scale compare system pare mos score sentence pair test test signiﬁcant chapter ext speech compare exactly system particular change actually improve system compare cmos comparative cmos mos user preference utterance well cmos score range system bad reference system well reference play sentence synthesize different system human listener choose utterance like well sentence present random order compare number sentence prefer system speech synthesis system well evaluate human listener automatic metric add information example run output asr system compute word error rate wer robust synthesized output measure voice output tts system match enrolled voice treat task speaker veriﬁcation pass voice speaker veriﬁcation system result score similarity score speech task wide variety speech relate task speaker diarization task determine speak longspeaker diarization multi speaker audio recording mark start end speaker turn interaction useful transcribe meeting classroom speech medical interaction diarization system use voice activity detection ﬁnd segment continuous speech extract speaker embed vector cluster vector group segment likely speaker recent work investigate end end algorithm map directly input speech sequence speaker label frame speaker recognition task identify speaker generally distin speaker recognition guish subtask speaker veriﬁcation binary decision isspeaker veriﬁcation speaker xor security access personal information telephone speaker identiﬁcation ndecision try match speaker voice database speaker task language identiﬁcation give waveﬁle identifylanguage identiﬁcation language speak important build multilingual model create dataset play role online system task wake word detection detect word short phrase usually wake word order wake voice enable assistant like alexa siri google assistant goal wake word build detection small device computing edge maintain privacy transmit user speech base server wake word detector need fast small footprint software embed device wake word detector usually use frontend feature extraction see asr follow word classiﬁer speak language model ummary summary chapter introduce fundamental algorithm text speech tts common modern algorithm tts use conditional generation language model audio token learn codec model neural audio codec short coder decoder system encode log speech signal digitize discrete compress representation compression discrete symbol codec produce compressed representation discrete code language modeling codec include encoder use convnet downsample speech downsample embed quantizer convert embed series discrete token decoder use convnet upsample token embed lossy reconstructed waveform quantization method turn series vector series discrete symbol means clustering create codebook code represent vector centroid cluster call codeword input vector assign near codeword cluster vector quantization rvq hierarchical version vector quantization produce multiple code input vector ﬁrst e vector codebook quantize residual difference codeword input vector iterate tts system like text synthesize sample voice tokenize bpe text audio codec speech use conditionally generate discrete audio token correspond text prompt voice speech sample tts evaluate play sentence human listener have mean opinion score mos historical note note beginning chapter speech synthesis early ﬁeld speech language processing century see number physical model articulation process include von kempelen model mention vowel model kratzenstein copenhagen organ pipe early see development early paradigm waveform synthesis formant synthesis articulatory synthesis concatenative synthesis formant synthesizer originally inspire attempt mimic human speech generate artiﬁcial spectrogram haskins laboratory pattern playback machine generate sound wave paint spectrogram pattern move transparent belt reﬂectance ﬁlter harmonic form cooper early formant synthesizer include lawrence fant know formant synthesizer klatt formant synthesizer successor system e mitalk system allen klattalk software digital equipment corporation dectalk klatt klatt chapter ext speech second early paradigm concatenative synthesis ﬁrst pose harris bell laboratory literally splice piece magnetic tape correspond phone soon peterson pose theoretical model base diphone include database multiple copy diphone differ prosody label prosodic feature include stress duration use join cost base formant distance neighboring unit diphone synthesis model actually implement decade later dixon maxey olive see invention unit selection synthesis base large unit non uniform length use target cost sagisaka sagisaka hunt black black taylor syrdal paradigm articulatory synthesizer attempt synthesize speech model physics vocal tract open tube representative model include steven flanagan fant klatt flanagan detail early tts system phoneme input development text ysis component tts come somewhat later draw nlp ﬁrst true text speech system system umeda teranishi umeda teranishi umeda umeda include parser assign prosodic boundary accent stress history codec modern history neural tts tbd exercisesv olume annotate linguistic structure second volume book discuss task detect linguistic structure early history nlp structure intermediate step ward deep language processing modern nlp generally explicit use parse structure inside large language model introduce instead linguistic structure play number new role important role interpretability provide useful interpretive lens neural network know particular layer neuron compute relate particular kind structure help break open black box understand component language model second important role linguistic structure practical tool social scientiﬁc study text know adjective modiﬁes noun particular implicit metaphor important measure attitude group individual detailed semantic structure helpful ple ﬁnde particular clause particular meaning legal contract word sense label help corpus study measure fact wrong word sense relation structure help build knowledge basis text finally computation linguistic structure important tool answer question language research area call computational linguistic distinguish natural language processing answer guistic question language change time individual need able example parse entire document different time period understand certain linguistic structure learn process people necessary able automatically label structure arbitrary text study linguistic structure begin old task computational linguistic extraction syntactic structure set algorithm parse extract syntactic structure include constituency ing dependency parsing introduce variety structure relate meaning include semantic role word sense entity relation event conclude linguistic structure tend relate discourse mean large text include coreference discourse coherence case algorithm automatically annotate relevant chapter equence labeling part speech name entity chapter labeling part speech name entity word warbling note midsummer night dream dionysius thrax alexandria long time ago write grammatical sketch greek techn summarize linguistic knowledge day work source astonishing proportion modern linguistic vocabulary include word syntax diphthong clitic analogy include description part speech noun verb part speech pronoun preposition adverb conjunction participle article early scholar include aristotle stoic list part speech thrax set basis description european language year way schoolhouse rock educational television show childhood song part speech like late great bob dorough conjunction junction durability part speech millennia speak centrality model human language proper name important anciently study linguistic category part speech generally assign individual word morpheme proper entire multiword phrase like marie curie location new york city organization stanford university use term name entity roughly speak refer name entity proper person location organization term commonly extend include thing entity part speech know pos name entity useful clue pos sentence structure meaning know word noun verb tell likely neighboring word noun english precede determiner adjective verb noun syntactic structure verb dependency link noun make speech tag key aspect parse know name entity like washington person place university important natural language processing task like question answer stance detection information extraction chapter introduce task speech tagging take quence word assign word speech like noun orverb task name entity recognition ner assign word phrase tag like person location organization task assign word xiin input word sequence label output sequence yha length input sequence call sequence labeling task introduce classic sequence label algo sequence labeling rithm generative hide markov model discriminative conditional random field crf follow chapter introduce modern sequence labeler base rnns ostly english word class english word class speech term like noun andverb freely section complete deﬁnition word class semantic tendency adjective example describe property noun people part speech deﬁne instead base grammatical relationship neighboring word morphological property afﬁxe tag description exampleopen classadj adjective noun modiﬁer describe property red young awesome adv adverb verb modiﬁer time place manner slowly home yesterday noun word person place thing etc algorithm cat mango beauty verb word action process draw provide propn proper noun person organization place etc regina ibm colorado intj interjection exclamation greeting yes response etc yes helloclosed class wordsadp adposition preposition postposition mark noun spacial temporal relationin aux auxiliary help verb mark tense aspect mood etc cconj coordinate conjunction join phrase clause det determiner mark noun phrase property num numeral particle function word associate word inﬁnitive pron pronoun shorthand refer entity event sconj subordinate conjunction join main clause subordinate clause sentential complementwhether becauseotherpunct sym symbol like emoji asdf qwfg figure part speech universal dependency tagset marneffe feature add ﬁner grain distinction property like number case deﬁniteness part speech fall broad category close class andopen class closed class open class close class relatively ﬁxed membership preposition new preposition rarely coin contrast noun verb open class new noun verb like iphone orto fax continually create borrow closed class word generally function word likeof oryou tend function word short occur frequently structure use grammar major open class occur language world noun include proper noun verb adjective adverb small open class interjection english ﬁve language noun word people place thing include noun mon noun include concrete term like catandmango abstraction like algorithm common noun andbeauty verb like term like pace pacing fro annoying noun english occur determiner goat bandwidth possessive ibm annual revenue occur plural goat abaci language include english divide common noun count noun count noun mass noun count noun occur singular plural goat goat mass noun tionship relationship count goat goat mass noun conceptualize homogeneous group snow salt communism count snow communism noun proper noun likeregina colorado ibm name speciﬁc person chapter equence labeling part speech name entity verb refer action process include main verb like draw provide verb andgo english verbs inﬂection non person singular eat singular eat progressive eat past participle eat scholar believe human language category noun verb argue language riau indonesian tongan distinction broschart evans gil adjective describe property quality noun like color white adjective black age old young value good bad language adjective korean example word correspond english adjective act subclass verbs english adjective beautiful act korean like verb mean beautiful adverb hodge podge italicize word example adverb adverb actually run home extremely quickly yesterday adverb generally modify verbs adverb adverb entire verb phrase directional adverb orlocative locative verb home downhill specify direction location action degree degree adverb extremely somewhat specify extent action process property manner adverb slowly slinkily delicately describe manner manner action process temporal adverb describe time action event temporal take place yesterday monday interjection hey alas small open class include interjection greeting hello goodbye question response yes huh english adposition occur noun call preposition preposition indicate spatial temporal relation literal house metaphorical time gusto relation like mark agent hamlet write shakespeare aparticle resemble preposition adverb combination particle verb particle extend meaning preposition resemble particle inshe turn paper verb particle act single unit call phrasal verb mean phrasal verb phrasal verb non compositional predictable individual meaning verb particle turn mean reject rule eliminate continue determiner chapter page mark start determiner english noun phrase articles likea type determiner mark article discourse property noun frequent theis common word write english aandanright conjunction join phrase clause sentence coordinate conjunction tion like butjoin element equal status subordinating tion element embed status example subordinating conjunction think like milk link main clause think subordinate clause like milk clause call subordinate entire clause content main verb think subordinate conjunction like thatwhich link verb argument way call complementizer complementizer pronoun act shorthand refer entity event personal pronoun noun refer person entity etc possessive pronoun form personal pronoun indicate actual possession abstract relation person object pronoun certain question art speech tag form act complementizer frida marry diego auxiliary verbs mark semantic feature main verb tense auxiliary complete aspect negate polarity action necessary possible suggest desire mood english auxiliary include copula verb verb doandhave form modal verb copula modal mark mood associate event depict main verb canindicate ability possibility permission possibility necessity english speciﬁc tagset penn treebank tagset marcus show fig label syntactically annotate corpora like penn treebank corpora worth know tag description example tag description example tag description example coord conj nnp proper noun sing ibm inﬁnitive cardinal number nnps proper noun plu carolinas interjection oops determiner nns noun plural llamas verb base eat existential pdt predeterminer vbd verb past tense eat foreign word mea culpa pos possessive ending vbg verb gerund eat subordin conjof prp personal pronoun vbn verb past pleeaten adjective yellow possess pronoun vbp verb eat jjr comparative adj big adverb quickly vbz verb pre eat jjs superlative adj wild rbr comparative adv fast wdt determ list item marker rb superlatv adv fastest pronoun modal particle possess sing mass noun llama sym symbol wrb adverb figure penn treebank core speech tag example word tag accord blue penn red tagset notice penn tagset distinguish tense participle verbs special tag existential construction english note london journal medicine proper noun tagset mark component noun propn nnp include journal andmedicine label common noun noun pron verb num noun adv adj noun aux vbd verb adp noun propn propn adp propn speech tagging speech tagging process assign speech word inpart speech tagging text input sequence tokenized word tagset output sequence tag output yicorresponde exactly input show intuition fig tagging disambiguation task word ambiguous ambiguous possible speech goal ﬁnd correct tag situation example book verb book ﬂight noun hand book determiner ﬂight serve dinner complementizer chapter equence labeling part speech name entity willnounauxverbdetnounjanetbackthebillpart speech figure task speech tagging mapping input word output pos tag think ﬂight early goal pos tagging resolve theseambiguity resolution ambiguity choose proper tag context accuracy speech tagging algorithm percentage test set accuracy tag match human gold label extremely high study find accuracy language universal dependency treebank dredze accuracy english treebank matter algorithm hmms crf bert perform similarly number human performance task english manning type wsj brown unambiguous tag ambiguous tag token unambiguous tag ambiguous tag figure tag ambiguity brown wsj corpora tag tagset introduce algorithm task section ﬁrst let explore task exactly hard fig show word type unambiguous janet nnp hesitantly ambiguous word account vocabulary common word token run text ambiguous particularly ambiguous common word include putandset example different part speech word earning growth take seat small building clear majority senator vbp bill dave begin door enable country buy debt nonetheless word easy disambiguate different tag equally likely example acan determiner letter determiner sense likely idea suggest useful baseline give ambiguous word choose tag frequent training corpus key concept frequent class baseline compare classiﬁer baseline good frequent class baseline assign token class occur training ame entity name entity tag frequent tag baseline accuracy baseline differ state art human ceiling name entity name entity tag speech tagging tell word like janet stanford university colorado proper noun proper noun grammatical property word view semantic perspective proper noun refer different kind entity janet person stanford university organization colorado location introduce concept name entity introduce name entity section reader read chapter aname entity roughly speak refer name entity proper person location organization task name entity nition ner ﬁnd span text constitute proper name tag type ofname entity recognition ner entity entity tag common person loc location org organization gpe geo political entity term name entity commonly extend include thing entity include date time kind temporal expression numerical expression like price example output ner tagger cite high fuel price org united airlines say time friday increase fare money round trip ﬂights city serve low cost carrier org american airlines unit org amr corp immediately match spokesman tim wagner say org united unit org ual corp say increase take effect time thursday apply route compete discount carrier loc chicago loc dallas loc denver loc san francisco text contain mention name entity include organization tion time person mention money figure show typical generic name entity type application need use speciﬁc entity type like protein gene commercial product work art type tag sample category example sentence people people character ture giant computer science organization org company sport team ipcc warn cyclone location loc region mountain sea sanitas sunshine canyon geo political entity gpe country states palo alto raise fee parking figure list generic name entity type kind entity refer name entity tagging useful ﬁrst step lot natural language processing task sentiment analysis want know consumer sentiment particular entity entity useful ﬁrst stage question answering link text information structured knowledge source like wikipedia name entity tagging central task involve build semantic representation like extract event relationship participant english wsj corpus test section chapter equence labeling part speech name entity unlike speech tagging segmentation problem word get tag task name entity recognition ﬁnd label span text difﬁcult partly ambiguity segmentation need decide entity boundary word text name entity difﬁculty cause type ambiguity mention jfkcan refer person airport new york number school bridge street united states example kind cross type confusion give figure washington bear slavery farm james burroughs org washington go game game series blair arrive loc washington state visit june gpe washington pass primary seatbelt law figure example type ambiguity use washington standard approach sequence labeling span recognition problem like ner bio tag ramshaw marcu method allow treat ner like word word sequence labeling task tag capture boundary name entity type consider following sentence jane villanueva org united unit org united airlines hold say fare apply loc chicago route figure show excerpt represent bio tagging bio variant call iotagging bioe tagging bio tag label token thatbegin span interest label token occur inside span tag token outside span interest label otag distinct band itag name entity class number tag tag nis number entity type bio tagging represent exactly information bracketed notation advantage represent task simple sequence modeling way speech tagging assign single label yito input word word label bio label bioe label jane villanueva unite org org org airline org org org hold org org org discuss chicago loc loc loc route figure ner sequence model show bio bioe tagging show variant tagging scheme tagging lose information eliminate tag bioe tagging add end tag efor end span span tag sfor span consist word sequence labeler hmm crf rnn transformer etc train label token text tag indicate presence absence particular kind name hmm art speech tag hmm speech tagging section introduce ﬁrst sequence label algorithm hide markov model apply speech tagging recall sequence labeler model job assign label unit sequence map sequence observation sequence label length hmm classic model introduce key concept sequence modeling modern model hmm probabilistic sequence model give sequence unit word letter morpheme sentence compute probability distribution possible sequence label choose good label sequence markov chain hmm base augment markov chain markov chain model markov chain tell probability sequence random variable state value set set word tag symbol represent example weather markov chain make strong assumption want predict future sequence matter current state state current state pact future current state predict tomorrow weather examine today weather allow look yesterday weather figure markov chain weather word show state transition start distribution pis require set mean probability start state cold probability start state hot etc formally consider sequence state variable markov model embody markov assumption probability sequence thatmarkov assumption predict future past matter present markov assumption figure show markov chain assign probability sequence weather event vocabulary consist hot cold warm state represent node graph transition itie edge transition probability value arc leave give state sum figure show markov chain assign ity sequence word markov chain familiar fact represent bigram language model edge express probability give model fig assign probability sequence chapter equence labeling part speech name entity formally markov chain speciﬁe follow component set nstate atransition probability matrix e probability move state ito state aninitial probability distribution state piis probability markov chain start state state jmay mean initial state use sample probability fig compute probability following sequence hot hot hot hot cold hot cold hot difference probability tell real world weather fact encode fig hidden markov model markov chain useful need compute probability sequence observable event case event interested hide observe directly example normally observe hidden speech tag text word infer tag word sequence tag hide observe ahidden markov model hmm allow talk observe eventshidden markov model like word input hide event like speech tag think causal factor probabilistic model hmm speciﬁe follow component set nstate probability matrix jrepresente probability move state ito state sequence observation likelihood call emission tie express probability observation vocabulary generate state aninitial probability distribution state piis probability markov chain start state state jmay mean initial state hmm give input sequence tobservation draw vocabulary ﬁrst order hide markov model instantiate simplify assumption ﬁrst order markov chain probability particular state depend previous state markov assumption second probability output observation oidepend state produce observation qiand state observation output independence hmm art speech tag component hmm tagger hmm component aandbprobabilitie estimate count tagged training corpus example use tagged wsj corpus theamatrix contain tag transition probability represent probability tag occur give previous tag example modal verbs likewillare likely follow verb base form like race expect probability high compute maximum likelihood estimate transition probability counting time ﬁrst tag label corpus ﬁrst tag follow second wsj corpus example occur time follow mle estimate bjmd thebemission probability represent probability give tag associate give word mle sion probability occurrence wsj corpus associate time see kind bayesian modeling appendix recall likelihood term ask likely tag word posterior instead slightly counterintuitive question go generate likely modal bewill figure illustration part hmm representation atransition probability compute prior probability bobservation likelihood associate state likelihood possible observation chapter equence labeling part speech name entity atransition probability bobservation likelihood hmm illustrate fig state hmm speech tagger tagger state tag hmm tagging decode model hmm contain hide variable task mining hide variable sequence correspond sequence observation call decode formally decode decode give input hmm sequence servation ﬁnd probable sequence state speech tagging goal hmm decode choose tag sequence probable give observation sequence nword argmax way hmm use baye rule instead compute argmax furthermore simplify drop denominator argmax hmm tagger simplify assumption ﬁrst output dependence probability word appear depend tag independent neighboring word tag second assumption markov assumption probability tag dependent previous tag entire tag sequence plug simplify assumption result follow equation probable tag sequence bigram tagger argmax part correspond neatly bemission probability anda transition probability deﬁne hmm art speech tag function viterbi observation len state graph len good path path prob create path probability matrix foreach state sfrom initialization step viterbi backpointer foreach time step tfrom recursion step foreach state sfrom viterbi nmax backpointer nargmax bestpathprob nmax termination step bestpathpointer nargmax termination step bestpath path start state bestpathpointer follow backpointer state time return bestpath bestpathprob figure viterbi algorithm ﬁnde optimal sequence tag give observation sequence hmm algorithm return state path hmm assign maximum likelihood observation sequence viterbi algorithm decode algorithm hmms viterbi algorithm show fig algorithm instance dynamic programming viterbi resemble dynamic ming minimum edit distance algorithm chapter viterbi algorithm ﬁrst set probability matrix lattice umn observation otand row state state graph umn cell state qiin single combine automaton figure show intuition lattice sentence janet bill cell lattice represent probability hmm state jafter see ﬁrst tobservation pass probable state sequence give hmm value cell compute recursively take probable path lead cell formally cell express probability max jjl represent probable path take maximum possible previous state sequence max like dynamic programming algorithm viterbi ﬁll cell recursively give compute ity state time compute viterbi probability take probable extension path lead current cell give state qjat time value compute nmax factor multiply extend previous path compute viterbi probability time chapter equence labeling part speech name entity jjnnpnnpnnpmdmdmdmdvbvbjjjjjjnnnnrbrbrbrbdtdtdtdt nnpjanetwillbackthebillnnvbmdnnvbjjrb nnpdtnnvb figure sketch lattice janet bill show possible tag word highlight path correspond correct tag sequence hidden state state part speech zero probability generate particular word accord bmatrix probability determiner realize janet grey viterbi path probability previous time step thetransition probability previous state qito current state thestate observation likelihood observation symbol otgiven current state work example let tag sentence janet bill goal correct series tag fig janet nnp bill nnp nnp figure theatransition probability wsj corpus smoothing row label conditioning event start token let hmm deﬁne table fig fig ure list jprobabilitie transition hidden state speech tag figure express observation likelihood word give tag table slightly simpliﬁed count wsj corpus word janet appear nnp possible hmm art speech tag janet bill nnp figure observation likelihood bcompute wsj corpus ing simpliﬁed slightly speech word thecan appear determiner nnp title like rainbow word tag nnp max backtrace figure ﬁrst entry individual state column viterbi algorithm cell keep probability good path far pointer previous cell path ﬁlle column avoid clutter cell value leave rest leave exercise reader cell ﬁlle backtrace endstate able reconstruct correct state sequence nnp figure show ﬂeshe version sketch see fig viterbi lattice compute well hide state sequence observation sequence janet bill state column begin column word janet set viterbi value cell product ptransition probability start probability state entry fig chapter equence labeling part speech name entity observation likelihood word janet give tag cell cell column zero word janet tag reader ﬁnd fig cell willcolumn get update state compute value viterbi take maximum extension path previous column lead current cell accord show value cell cell get max ue previous column multiply appropriate transition probability happen case zero previous column maining value multiply relevant observation probability trivial max take case ﬁnal value come nnp state previous column reader ﬁll rest lattice fig backtrace viterbi algorithm return gold state sequence nnp conditional random field crf hmm useful powerful model turn hmms need number augmentation achieve high accuracy example pos tagging task run unknown word proper name acronymsunknown word create new common noun verb enter language surprising rate great way add arbitrary feature help base capitalization morphology word start capital letter likely proper noun word end past tense vbd vbn etc know previous follow word useful feature previous word current tag unlikely verb try hack hmm ﬁnd way incorporate general hard generative model like hmms add arbitrary feature directly model clean way see model combine arbitrary feature principled way log linear model like logistic regression model chapter logistic regression sequence model assign class single observation luckily discriminative sequence model base log linear model theconditional random ﬁeld crf describe linear chain crf crf version crf commonly language processing conditioning closely match hmm assume sequence input word want compute sequence output tag hmm compute good tag sequence maximize rely baye rule likelihood argmax argmax argmax crf contrast compute posterior train onditional random field crf discriminate possible tag sequence argmax crf compute probability tag time step stead time step crf compute log linear function set relevant feature local feature aggregate normalize produce global probability sequence let introduce crf formally xandyas input output sequence crf log linear model assign probability entire output tag sequence possible sequence give entire input word sequence think crf like giant sequential version multinomial logistic regression algorithm see text categorization recall introduce feature function fin regular multinomial logistic regression text categorization function tuple input text xand single class page crf deal sequence function fmap entire input sequence xand entire output sequence yto feature vector let assume kfeature weight wkfor feature exp common describe equation pull denominator function kfunction feature property entire input sequence xand output sequence compute posing sum local feature position iiny local feature fkin linear chain crf allow use current output token previous output token entire input string subpart current position constraint depend current previous output token characterize linear chain crf limitation make possible use version thelinear chain crf efﬁcient viterbi forward backwards algorithm hmm general crf contrast allow feature use output token necessary task decision depend distant output token like general crf require complex inference commonly language chapter equence labeling part speech name entity feature crf pos tagger let look feature detail reason use discriminative sequence model easy incorporate lot linear chain crf local feature fkat position ican depend information legal feature represent common situation follow detg propn street numg verb auxg simplicity assume crf feature value explicitly use notation mean xis true leave deﬁne feature assume feature implicitly idea feature use system designer hand speciﬁc feature automatically populate feature template wefeature template brieﬂy mention chapter template use information template automatically populate set feature instance training test set example janet nnp bill xii word follow feature generate value assign arbitrary feature number bill important feature help unknown word important word shape feature represent abstract letter pattern word shape word map low case letter upper case number retain punctuation example map map xxdd second class short word shape feature feature consecutive character type remove word cap map word initial cap map map map preﬁx sufﬁx feature useful summary sample feature template help unknown word xicontain particular preﬁx preﬁxe length xicontain particular sufﬁx sufﬁxe length word shape short word shape example word dress generate follow non zero ued feature value hmms computation base probability want include source knowledge tagging process ﬁnd way encode knowledge probability time add feature lot complicated conditioning get hard hard onditional random field crf preﬁx preﬁx sufﬁx sufﬁx word shape xxxx xxxxxxx short word shape know word template compute word see training set unknown word feature compute word training training word frequency threshold result know word template word signature feature large set feature generally feature cutoff feature throw count training set remember crf learn weight local feature instead ﬁrst sum value local feature example feature entire sentence create global feature example global feature multiply weight training inference ﬁxed set kfeature kweight length sentence different feature crf name entity recognizer crf ner make use similar feature pos tagger show figure identity identity neighboring word embedding embedding neighboring word speech speech neighboring word presence wiin agazetteer wicontain particular preﬁx preﬁxe length wicontain particular sufﬁx sufﬁxe length word shape word shape neighboring word short word shape short word shape neighboring word gazetteer feature figure typical feature feature base ner system feature especially useful location gazetteer list place gazetteer name provide million entry location detailed geographical political implement binary feature indicate phrase appear list related resource like list example united states census entity dictionary like list corporation product helpful gazetteer mikheev sample name entity token generate follow zero value feature value assume gazetteer census chapter equence labeling part speech name entity preﬁx sufﬁx tane preﬁx sufﬁx ane preﬁx sufﬁx preﬁx sufﬁx word shape short word shape figure illustrate result add speech tag shape information early example word pos short shape gazetteer bio label jane nnp villanueva nnp unite nnp org airline nnp org hold nnp org discuss vbd chicago nnp loc route figure ner feature sample sentence assume chicago lanueva list location gazetteer assume feature value ﬁrst pos feature example represent nnpg inference training crf ﬁnd good tag sequence ˆyfor give input start argmax argmax argmax argmax argmax ignore exp function denominator exp change argmax denominator constant give observation sequence decode ﬁnd optimal tag sequence hmms turn viterbi algorithm work like hmm chain crf depend timestep previous output token concretely involve ﬁlle appropriate value taine backpointer proceed hmm viterbi table ﬁlle simply follow pointer maximum value ﬁnal column retrieve desire set valuation name entity recognition requisite change hmm viterbi ﬁll cell recall recursive step viterbi equation compute viterbi value time tfor state jas nmax hmm implementation nmax crf require slight change formula replace aandb prior likelihood probability crf feature nmax learn crf rely supervised learning algorithm present logistic regression give sequence observation feature function respond output use stochastic gradient descent train weight mize log likelihood training corpus local nature linear chain crf mean forward backward algorithm introduce hmms appendix extend crf version efﬁciently compute necessary tive logistic regression regularization important evaluation name entity recognition speech tagger evaluate standard metric accuracy name entity recognizer evaluate recall precision recall recall ratio number correctly label response total label precision ratio number correctly label response total label measure harmonic mean know difference ner system icant difference use pair bootstrap test similar randomization test section name entity tagging entity word unit response example fig entity jane villanueva andunite line hold non entity discuss count single response fact name entity tagging segmentation component present task like text categorization speech tagging cause lem evaluation example system label jane jane lanueva person cause error false positive false tive addition entity unit response word unit training mean mismatch training test condition detail section summarize remain detail datum model speech tagging ner begin datum algorithms chapter equence labeling part speech name entity present supervise having label datum essential training testing wide variety dataset exist speech tagging ner universal dependency dataset marneffe pos tag corpora language penn treebank english chinese arabic ontonote corpora label name entity english chinese arabic hovy name entity tag corpora available particular domain biomedical bada literary text bamman rule base method machine learn neural crf sequence model norm academic research commercial approach ner base pragmatic tion list rule small supervise machine learning chiticariu example ibm system architecture user speciﬁes declarative constraint tag task formal query language include regular expression dictionary semantic constraint operator system compile efﬁcient extractor chiticariu common approach repeat rule base pass text start rule high precision low recall subsequent stage machine learning method output ﬁrst pass account approach ﬁrst work coreference lee use high precision rule tag unambiguous entity mention search substre match previously detect name use application speciﬁc list ﬁnd likely domain speciﬁc mention finally apply supervise sequence labeling technique use tag vious stage additional feature rule base method early method speech tagging rule base tagger like english constraint grammar system karlsson outilainen use stage formalism invent morphological analyzer ten thousand word stem entry return part speech word large set thousand constraint apply input sentence rule part speech inconsistent context pos tag morphologically rich language augmentation tag algorithm necessary deal guage rich morphology like czech hungarian turkish productive word formation process result large vocabulary language word token corpus hungarian twice word type similarly sized corpus english oravecz diene million word token corpus turkish contain time word type similarly sized english corpus hakkani large ies mean unknown word unknown word cause signiﬁcant formance degradation wide variety language include czech slovene estonian romanian haji highly inﬂectional language information english code word morphology like case nominative accusative genitive gender masculine feminine information important task like ing coreference resolution speech tagger morphologically rich ummary guage need label word case gender information tagset logically rich language sequence morphological tag single primitive tag turkish example word izinha sible morphological speech tag meaning hakkani yerdeki izintemizlenmesi gerek trace ﬂoor clean parmak izinkalmis ﬁnger print leave girmek izinalman gerekiyor izin need permission enter morphological parse sequence like speech tag greatly increase number part speech tagset time large tag see english large tagset word need morphologically analyze generate list possible morphological tag sequence speech tag word role tagger disambiguate tag method help unknown word morphological parser accept unknown stem segment afﬁxe properly summary chapter introduce part speech andname entity task speech tagging andname entity recognition language generally small set close class word highly frequent ambiguous act function word open class word like noun verb adjective speech tagset exist tag speech tagging process assign speech label sequence word entity word proper noun refer mainly people place organization extend type strictly entity proper noun common approach sequence modeling generative approach hmm tagging discriminative approach crf tagging neural approach follow chapter probability hmm tagger estimate maximum likelihood timation tag label training corpora viterbi algorithm decode ﬁnde likely tag sequence random field orcrf tagger train log linear model choose good tag sequence give observation sequence base feature condition output tag prior output tag entire input sequence current timestep use viterbi algorithm inference choose good sequence tag version forward backward algorithm appendix chapter equence labeling part speech name entity historical note probably early speech tagger parser zellig harris transformation discourse analysis project tdap implement tween june july university pennsylvania harris early system speech dictionary tdap write rule speech disambiguation use speech tag quence relative frequency tag word preﬁgure modern algorithm parser implement essentially cascade ﬁnite state transducer joshi hopely karttunen reimplementation computational grammar coder cgc klein simmon component lexicon morphological analyzer context tor small word lexicon list function word irregular word morphological analyzer inﬂectional derivational sufﬁxe sign speech class run word produce candidate part speech disambiguate set context rule rely surround island unambiguous word example rule say article verb allowable sequence adj noun adverb noun noun taggit tagger greene rubin architecture klein simmon big dictionary tag taggit apply brown corpus accord francis ˇcera accurately tag corpus remainder brown corpus tag hand early algorithm base stage architecture dictionary ﬁrst assign word set potential part speech list handwritten disambiguation rule winnow set single speech word probability tag stolz complete bilistic tagger viterbi decode sketch bahl mercer lancaster oslo bergen lob corpus british english equivalent brown pus tag early claws tagger marshall shall garside probabilistic algorithm approximate simpliﬁed hmm tagger algorithm tag bigram probability instead store word likelihood tag algorithm mark tag rare normally frequent derose develop quasi hmm algorithm include use namic programming compute year probabilistic part tagger church probably ﬁrst implement hmm tagger describe correctly church church describe computation incorrectly church explain simpliﬁed pedagogical pose probability idea understandable store lexicon standard form later tagger explicitly introduce use hide markov model kupiec weischedel sch singer merialdo show fully unsupervise work tag task reliance hand label datum important charniak show importance frequent tag baseline number abney brent hmm tagger implementation detail e extension trigram contexts use sophisticated unknown word feature performance close state art log linear model pos tagging introduce ratnaparkhi introduce system call mxpost implement maximum entropy markov model memm slightly simple version crf time sequence labeler apply task name entity tagging ﬁrst hmms bikel memms mccallum crf develop lafferty apply ner callum wide exploration feature follow zhou neural approach ner mainly follow pioneer result collobert apply crf convolutional net bilstms word character base embedding input follow shortly standard neural algorithm ner huang hovy lample follow recent use transformer bert idea letter sufﬁxe unknown word old early klein simmon system check ﬁnal letter sufﬁxe length know word feature describe page come mainly ratnaparkhi augmentation toutanova man state art pos tagger use neural algorithm bidirectional rnns transformer like bert chapter chapter hmm brent thede harper crf tagger accuracy likely tad low manning investigate remain error high perform tagger toutanova suggest half remain error error inconsistency training datum able rich linguistic model remainder task underspeciﬁed unclear supervise tagging relie heavily domain training datum hand label expert way relax assumption include unsupervised algorithm ing word speech like class summarize christodoulopoulos way combine label unlabeled datum example training clark søgaard householder historical note part speech sampson garside provenance brown tagset exercise find tagging error follow sentence tag penn treebank tagset prp need vbp ﬂight atlanta vbz ﬂight serve dinner nn prp friend living vbg denver nnp vbp prp list nonstop afternoon ﬂight nn use penn treebank tagset tag word following sentence damon runyon short story ignore punctuation difﬁcult good nice night crap game garage second street take newspaper sell tall skinny guy long sad mean looking kisser mournful chapter equence labeling part speech name entity sit mindy restaurant put geﬁllte ﬁsh dish fond guy doll take peek forth compare tag previous exercise friend answer word disagree implement likely tag baseline find pos tag training set use compute word tag maximize need implement simple tokenizer deal sentence boundary start assume unknown word compute error rate known unknown word write ﬁve rule well job tag unknown word difference error rate build bigram hmm tagger need speech tag corpus split corpus training set test set label training set train transition observation probability hmm tagger rectly hand tag datum implement viterbi algorithm decode test sentence run algorithm test set report error rate compare performance frequent tag baseline error analysis tagger build confusion matrix investigate frequent error propose feature improve mance tagger error develop set regular expression recognize character shape feature describe page bio labeling scheme give chapter possible example btag reserve situation ambiguity exist adjacent entity propose new set biotag use ner system experiment compare performance scheme present chapter name work art book movie video game etc different kind name entity discuss chapter collect list name work art particular category web base source etc analyze list example way name likely problematic technique describe chapter develop ner system speciﬁc category name collect exercise evaluate system collection text likely contain instance name free grammar constituency parse night bruce springsteen patti smith fire time james baldwin winter night traveler italo calvino love actually richard curtis suddenly summer tennessee williams scanner darkly philip dick title constituent geoffrey pullum language log point incredible rarity morning shoot elephant pajama get pajama know groucho marx animal cracker study grammar ancient pedigree grammar sanskrit describe indian grammarian turie bce famous treatise book word syntax syntax come greek mean set arrangement refer way word arrange see syntactic notion vious chapter like use speech category chapter chapter introduce formal model capture sophisticated tion grammatical structure algorithm parse structure focus chapter context free grammar cky algorithm parse context free grammar backbone formal el syntax natural language matter computer language syntactic parsing task assign syntactic structure sentence parse tree context free grammar dependency ccg formalism introduce follow chapter application grammar checking sentence parse grammatical error hard read parse tree intermediate stage representation mal semantic analysis parser grammatical structure assign sentence useful text analysis tool text datum science application require modeling relationship element sentence chapter introduce context free grammar small sample mar english introduce formal deﬁnition context free grammar grammar normal form talk treebank corpora tat syntactic structure discuss parse ambiguity problem present turn parse give famous cocke kasami young cky algorithm kasami young standard dynamic ming approach syntactic parsing cky algorithm return efﬁcient sentation set parse tree sentence tell parse tree right need augment cky score possible constituent neural span base parser finally introduce standard set metric evaluate parser chapter ontext grammar constituency parse constituency syntactic constituency idea group word behave single unit constituent develop grammar involve build inventory constituent language word group english consider thenoun phrase sequence word surround noun noun phrase example noun phrase thank damon runyon harry horse high class spot mindy broadway copper reason come hot box party brooklyn evidence word group form constituent piece evidence appear similar syntactic environment example verb party brooklyn arrive high class spot mindy attract broadway copper love sit noun phrase occur verb true individual word noun phrase follow ical sentence english recall use asterisk mark fragment grammatical english sentence arrive spot sit correctly describe fact ordering word english able thing like noun phrase occur verb let formal way context free grammar widely formal system model constituent structure natural guage context free grammar orcfg context free grammar call cfg phrase structure grammar formalism equivalent backus naur form orbnf idea base grammar constituent structure date chologist wilhelm wundt formalize chomsky independently backu context free grammar consist set rule orproduction rule express way symbol language group order gether lexicon word symbol example follow production lexicon express phrase compose propernoun determiner det follow nominal anominal turn consist ontext grammar noun nominal noun context free rule hierarchically embed combine previous rule like following express fact lexicon symbol cfg divide class symbol correspond word language nightclub call terminal terminal symbol lexicon set rule introduce terminal symbol symbol express abstraction terminal call non terminal non terminal context free rule item right arrow ordered list terminal non terminal left arrow single non terminal symbol express cluster generalization non terminal associate word lexicon lexical category speech cfg think way device generate sentence device assign structure give sentence view cfg generator read arrow rewrite symbol left string symbol right start symbol use ﬁrst rule rewrite npas det nominal rewrite nominal noun ﬁnally rewrite part speech ﬂight string ﬂight derive non terminal cfg generate set string sequence rule expansion call derivation string word common represent derivation parse derivation tree commonly show invert root figure show tree parse tree representation derivation nom noun ﬂightdet figure parse tree ﬂight parse tree show fig node npdominate dominate node tree det nom noun immediately dominate node detandnom formal language deﬁne cfg set string derivable designated start symbol grammar designate start start symbol talk rule pronounce rightarrow go read ﬁrst rule go det chapter ontext grammar constituency parse symbol call context free grammar deﬁne sentence sis usually interpret sentence node set string derivable sis set sentence simpliﬁed version english let add additional rule inventory follow rule express fact sentence consist noun phrase follow verb phrase verb phrase prefer morning ﬂight verb phrase english consist verb follow assorted thing example kind verb phrase consist verb follow noun phrase prefer morning ﬂight verb follow noun phrase prepositional phrase leave boston morning verb phrase verb follow prepositional phrase leave thursday prepositional phrase generally preposition follow noun phrase example common type prepositional phrase atis corpus indicate location direction los angeles thenpinside ppneed location pps time date noun arbitrarily complex example atis corpus seattle ﬂight minneapolis ground transportation chicago wednesday round trip ﬂight united airlines evening ﬁfty seven ﬂight ninth july stopover nashville figure give sample lexicon fig summarize grammar rule see far note use symbol jto indicate non terminal alternate possible expansion stopjﬁrstjlat jotherjdirect proper angeles jchicagojunitedjamerican figure lexicon use grammar generate sentence atis language start expand choose random expansion ontext grammar grammar rule example want morning ﬂight jproper noun los angeles jdet nominal ﬂight noun morning ﬂight jnoun ﬂights jverb want ﬂight jverb leave boston morning jverb leave thursday los angeles figure grammar example phrase rule nom noun ﬂightnom noun morningdet averb prefernp pro figure parse tree prefer morning ﬂight accord grammar random expansion verb generate string prefer morning ﬂight figure show parse tree represent complete derivation prefer morning ﬂight represent parse tree compact format call bracketed notation bracketed representation parse tree fig bracket notation nom nmorne nom nﬂight cfg like formal language sentence string word derive grammar formal language deﬁne mar call grammatical sentence sentence derive grammatical give formal grammar language deﬁne grammar refer ungrammatical hard line characterize ungrammatical formal language simpliﬁed model natural language work determine give sentence give natural language english depend context linguistic use formal language model natural language call generative grammar sincegenerative grammar language deﬁne set possible sentence generate grammar note different sense word generate talk chapter ontext grammar constituency parse language model generate text formal deﬁnition context free grammar conclude section quick formal description context free mar language generate context free grammar gis deﬁne parameter tuple set non terminal symbol orvariable set terminal symbol disjoint set rule production form ais non terminal bis string symbol inﬁnite set string designate start symbol member remainder book adhere follow convention cuss formal property context free grammar oppose explain particular fact english language capital letter like non terminal start symbol low case greek letter like string draw low case roman letter like string terminal language deﬁne concept derivation string derive rewrite second series rule application formally follow hopcroft ullman production randaandgare string set aagdirectly derive abg directly derive derivation generalization direct derivation ambe string derive formally deﬁne language lggenerate grammar gas set string compose terminal symbol derive designated start symbol fwjwis problem mapping string word parse tree call tactic parsing section parse treebank corpus sentence annotate parse tree call treebank reebank treebank play important role parse linguistic investigation syntactic phenomenon treebank generally run parser sentence have result parse hand correct human linguist figure show sentence penn treebank project include treebank penn treebank english arabic chinese penn treebank speech tagset deﬁne chapter minor formatting difference treebank use lisp style parenthesize notation tree extremely common resemble bracketed notation see early familiar standard node line tree representation fig sbj cold sky vbd adjp prd fire light sbj flight arrive tmp tmp tomorrow figure parse ldc brown atis sentence adjp prd lightcc andnn ﬁrein ofjj fullvbd wasnp sbj skyjj colddt figure tree correspond brown corpus sentence previous ﬁgure sentence treebank implicitly constitute grammar language example parse sentence fig extract cfg rule show fig rule sufﬁxe strip simplicity grammar parse penn treebank ﬂat result rule chapter ontext grammar constituency parse grammar lexicon adjp figure cfg grammar rule lexicon treebank sentence fig approximately different rule expand vps separate rule sequence length possible arrangement verb argument advp advp grammar equivalence normal form formal language deﬁne possibly inﬁnite set string word gest ask grammar equivalent ask generate set string fact possible distinct context free grammar generate language grammar strongly equivalent ifstrongly equivalent generate set string andif assign phrase structure sentence allow merely rename non terminal symbol grammar weakly equivalent generate set string notweakly equivalent assign phrase structure sentence useful normal form grammar normal form production take particular form example context free grammar chomsky normal form cnf chomsky additionchomsky normal form production form right hand rule non terminal symbol terminal symbol chomsky normal form grammar binary branch binary tree downbinary branching prelexical node use binary branch property cky parse algorithm section context free grammar convert weakly equivalent chomsky normal form grammar example rule form convert follow cnf rule exercise ask reader mbiguity grammar lexicon noun proper nominal noun figure english grammar lexicon formulate complete algorithm binary branching actually produce small grammar example sentence characterize vbd represent penn treebank series rule generate follow rule grammar generation symbol potentially inﬁnite sequence symbol rule form bis know chomsky adjunction adjunction ambiguity ambiguity problem face syntactic parser chapter duce notion speech ambiguity andpart speech tion introduce new kind ambiguity call structural ambiguity structural ambiguity illustrate new toy grammar show figure add rule structural ambiguity occur grammar assign parse sentence groucho marx know line captain spaulde chapter ontext grammar constituency parse nominal pajamasnominal noun elephantdet anverb shotnp pronoun pajamasvp nominal noun elephantdet anverb shotnp pronoun figure parse tree ambiguous sentence parse left correspond humorous reading elephant pajama parse right correspond reading captain spaulde shooting pajama cracker ambiguous phrase pajama head elephant verb phrase head shot figure trate analysis marx line rule structural ambiguity appropriately come form common kind ambiguity attachment ambiguity andcoordination ambiguity sentence attachment ambiguity particular constituent attach toattachment ambiguity parse tree place groucho marx sentence example ofpp attachment ambiguity preposition phrase attach partpp attachment ambiguity kind adverbial phrase subject kind ambiguity instance follow example gerundive ﬂying paris gerundive sentence subject eiffel tower adjunct modifying head saw see eiffel tower ﬂye paris incoordination ambiguity phrase conjoin conjunction like ambiguity example phrase old man woman bracket old man woman refer old man andold woman old man woman case man old ambiguity combine complex way real sentence like follow news sentence brown corpus president kennedy today push aside white house business devote time attention work berlin crisis address deliver tomorrow night american people nationwide television radio sentence number ambiguity semantically unreasonable require careful reading noun phrase parse nationwide television radio television radio direct object push aside white house business bizarre phrase white house business devote time attention working structure like kennedy afﬁrme intention propose new budget address deﬁcit phrase berlin crisis address cky arse ynamic programming approach deliver tomorrow night american people adjunct modifying verb push applikeover nationwide television radio attach high vps ornps modify people ornight fact grammatically correct semantically able parse naturally occur sentence irksome problem affect parser fortunately cky algorithm design efﬁciently handle structural ambiguity following section augment cky neural method choose single correct parse syntactic disambiguation cky parse dynamic programming approach dynamic programming provide powerful framework address lem cause ambiguity grammar recall dynamic programming proach systematically ﬁll table solution subproblem complete table solution subproblem need solve problem case syntactic parsing subproblem represent parse tree constituent detect input dynamic programming advantage arise context free nature grammar rule constituent discover segment input record presence available use subsequent derivation require provide time storage efﬁciencie subtree look table reanalyze section present cocke young cky algorithm widely dynamic programming base proach parse chart parse kaplan kay related approach dynamic programming method refer chart parse method chart parse conversion chomsky normal form cky algorithm require grammar ﬁrst chomsky normal form cnf recall section grammar cnf restrict rule form right hand rule expand non terminal single terminal restrict grammar cnf lead loss expressiveness context free grammar convert corresponding cnf grammar accept exactly set string original grammar let start process convert generic cfg represent cnf assume deal grammar situation need address generic grammar rule mix terminal non terminal right hand rule single non terminal right hand rule length right hand great remedy rule mix terminal non terminal simply introduce new dummy non terminal cover original terminal example rule inﬁnitive verb phrase inf replace rule inf rule single non terminal right call unit production weunit production eliminate unit production rewrite right hand original rule right hand non unit production rule ultimately lead formally chain unit production chapter ontext grammar constituency parse non unit production grammar add rule grammar discard intervene unit production demonstrate toy grammar lead substantial ﬂattening grammar consequent promotion terminal fairly high level result tree rule right hand side long normalize tion new non terminal spread long sequence new rule formally rule like replace leftmost pair non terminal new non terminal introduce new production result follow new rule case long right hand side simply iterate process fending rule replace rule length choice replace leftmost pair non terminal purely arbitrary systematic scheme result binary rule sufﬁce current grammar rule replace rule entire conversion process summarize follow copy conform rule new grammar unchanged convert terminal rule dummy non terminal convert unit production rule binary add new grammar figure show result apply entire conversion procedure introduce early page note ﬁgure original lexical rule original lexical rule cnf carry unchanged new grammar figure place process eliminate unit production effect create new lexical rule example original verb promote vps converted grammar cky recognition grammar cnf non terminal node speech level parse tree exactly daughter dimensional matrix encode structure entire tree sentence length work upper triangular portion cell matrix contain set non terminal represent constituent span position ithrough jof input indexing scheme begin natural think index point gap input word gap call fencepost metaphor fencepost post segment fencing follow cell represent entire input reside position matrix non terminal entry table daughter parse low constituent represent entry position input split part give cky arse ynamic programming approach cnf noun nominal nominal noun noun figure conversion cnf note show original lexical entry unchanged position ﬁrst constituent lie left entry row second entry lie beneath column concrete consider follow example complete parse matrix show fig book ﬂight houston superdiagonal row matrix contain part speech word input subsequent diagonal superdiagonal contain constituent cover span increase length input give setup cky recognition consist ﬁlle parse table right way proceed fashion point ﬁlle cell cell contain part contribute entry cell left cell ﬁlle algorithm give fig ﬁll upper triangular matrix column time work left right column ﬁlle right fig illustrate scheme guarantee point time information need left column left ﬁlle ﬁlle mirror line processing ﬁlle column leave right correspond process word time outermost loop algorithm give fig iterate column second loop iterate row purpose innermost loop range place substre span itojin input split krange place string split pair cell consider lockstep right row iand column figure illustrate general case ﬁlle cell chapter ontext grammar constituency parse bookthe flight throughhouston verb nominal noun nounnominalprepppnp proper figure complete parse table book ﬂight houston function cky arse word grammar return table forj word table table fori fork table table figure cky algorithm split algorithm consider content cell combine way sanction rule grammar rule exist non terminal left hand enter table figure show ﬁve cell column table ﬁlle word houston read arrow point span add entry table note action cell presence alternative parse input ppmodiﬁes ﬂight modiﬁes booking capture second argument original rule capture indirectly rule cky parse algorithm give fig recognizer parser tell valid parse exist give sentence base ﬁnd ansin cell provide derivation actual job parser turn parser capable return possible parse give input simple change algorithm ﬁrst change augment entry table non terminal pair pointer table entry derive show fig second change permit multiple version non terminal enter table show fig change complete table contain possible parse give input return arbitrary cky arse ynamic programming approach figure way ﬁll cell cky table parse consist choose sfrom cell recursively retrieve component constituent table course instead return parse sentence usually want good parse section cky practice finally note restriction cnf pose problem theoretically pose non trivial problem practice return cnf tree consistent original grammar build grammar oper complicate syntax drive approach semantic analysis approach get problem information transform tree original grammar post processing step parse trivial case transformation rule length great simply delete new dummy non terminal promote daughter restore original tree case unit production turn convenient alter sic cky algorithm handle directly store information need recover correct tree exercise ask change probabilistic parser present appendix use cky algorithm alter chapter ontext grammar constituency parse bookthe flight throughhouston verb nominal noun nounnominalprepnp proper flight throughhouston verb nominal noun nounprepppnp proper bookthe flight throughhouston verb nominal noun nounnominalprepppnp proper flight throughhouston verb nominal noun nounnominalprepppnp proper bookthe flight throughhouston verb nominal noun nounnominalprepppnp proper figure fill cell column read word houston pan base neural constituency parse manner span base neural constituency parsing cky parse algorithm see far great enumerate possible parse tree sentence large problem tell parse correct disambiguate possible parse solve disambiguation problem use simple neural extension cky algorithm intuition parse algorithm call span base constituency parsing orneural cky train neural classiﬁer assign score constituent use modiﬁed version cky combine constituent score ﬁnd well score parse tree describe version algorithm kitaev parser learn map span word constituent like cky hierarchically combine large large span build parse tree unlike sic cky parser use hand write grammar constrain stituent combine instead rely learn neural representation span encode likely combination computing score span let begin consider constituent span lie span fencepost position iand jwith non terminal symbol label build system assign score constituent span subwordsmap score spanrepresent spancky compute good parse postprocessing layer figure simpliﬁed outline compute span score span ﬂight label fig sketch architecture input word token embed chapter ontext grammar constituency parse pass pretraine language model like bert bert ate level subword wordpiece token word ﬁrst need convert bert output word representation standard way simply use ﬁrst subword unit representation entire word ing subword unit sum subword unit common embedding pass postprocessing layer kitaev example use transformer layer result word encoder output ytare compute span score map word encoding index word position span ing index fencepost represent fencepost separate value intuition span endpoint right word represent different information span endpoint left word convert word output ytinto leftward point value span end fencepost rightward point ytfor span begin fencepost splitting ytinto half span stretch double vector post following representation ﬂight span start book ﬂight traditional way represent span develop originally rnn base model wang chang extend transformer ence embedding start end represent span subtract embedding ifrom embedding represent span concatenate difference fencepost component span vector vis pass mlp span classiﬁer connect layer relu activation function output dimensionality number possible non terminal label layernorm mlp output score possible non terminal integrate span score parse score label constituent span need score entire parse tree formally tree tis represent set jtjsuch label span tthspan start position itand end position label score span parser compute score tree sum score constituent span valuating parser choose ﬁnal parse tree tree maximum score argmax simple method produce likely parse greedily choose high scoring label span greedy method guarantee produce tree good label span complete tree practice greedy method tend ﬁnd tree experiment gaddy ﬁnd predict bracketing form valid tree nonetheless common use variant cky algorithm ﬁnd parse variant deﬁne gaddy work follow let deﬁne score good subtree spanning span length choose good label max span recursion max max note parser max label span max label span worry decision sense give grammar role grammar classical parsing help constrain possible combination constituent nps like follow vps contrast neural model learn kind contextual constraint mapping span non terminal detail span base parsing include margin base training gorithm stern gaddy kitaev klein kitaev evaluate parser standard tool evaluate parser assign single parse tree sentence parsev metric black parsev metric measure parsev constituent hypothesis parse tree look like constituent hand label reference parse parsev require human label reference gold standard parse tree sentence test set generally draw reference parse treebank like penn treebank constituent hypothesis parse chof sentence sis label correct constituent reference parse crwith starting point end point non terminal symbol measure precision recall task see like name entity tagging label recall correct constituent hypothesis parse total constituent reference parse label precision correct constituent hypothesis parse total constituent hypothesis parse chapter ontext grammar constituency parse worker figure lexicalize tree collin usual report combination additionally use new metric cross bracket sentence cross bracket number constituent reference parse bracketing hypothesis parse bracketing compare parser use different grammar parsev metric clude canonicalization algorithm remove information likely speciﬁc auxiliary pre inﬁnitival etc compute simpliﬁed score black canonical implementation parsev metric call evalb sekine collins evalb head head find syntactic constituent associate lexical head nis head vis head idea head constituent date ﬁeld central dependency grammar dependency parse introduce chapter head way map stituency dependency parse head important probabilistic ing appendix constituent base grammar formalism like head drive phrase structure grammar pollard sag simple model lexical head context free rule associate head charniak collin head word phrase grammatically important head pass parse tree non terminal parse tree annotate single word lexical head figure show example tree collin non terminal annotate head generation tree cfg rule augment identify right constituent head child headword node set headword head child choose head child simple textbook example nni head complicated controversial ummary phrase complementizer toor verb head inﬁnite verb phrase modern linguistic theory syntax generally include component deﬁne head pollard sag alternative approach ﬁnde head practical computational system instead specify head rule grammar head identiﬁed dynamically context tree speciﬁc sentence word sentence parse result tree walk decorate node appropriate head current system rely simple set handwritten rule practical penn treebank grammar give collin develop originally magerman example rule ﬁnde head npis follow collin word tag pos return word search right leave ﬁrst child nnp nnps pos jjr search left right ﬁrst child search right leave ﬁrst child adjp prn search right leave ﬁrst child search right leave ﬁrst child jjs return word select rule set show fig example rule form algorithm start left ynlooking ﬁrst yiof type tos find search ﬁrstyiof type vbd vbds find search vbn collin detail parent direction priority list adjp leave nns advp vbn vbg adjp jjr jjs rbr rbs sbar advp right rbr rbs advp jjr jjs prn leave prt right leave nns ncd jjr jjs leave sbar adjp ucp sbar leave whnp whpp whadvp whadjp sinv sbar frag leave vbd vbn vbz vbg vbp adjp nns figure head rule collin head rule call head percolation table summary chapter introduce constituency parsing summary main point language group consecutive word act group stituent model context free grammar know phrase structure grammar context free grammar consist set rule orproduction express set non terminal symbol set terminal symbol formally particular context free language set string derive particular context free grammar chapter ontext grammar constituency parse ambiguity signiﬁcant problem parser common source structural ambiguity include attachment andcoordination ambiguity programming parse algorithm cky use table partial parse efﬁciently parse ambiguous sentence restrict form grammar chomsky normal form cnf basic cky algorithm compactly represent possible parse tence choose single good parse choose single parse possible parse disambiguation neural constituency parser span base neural constituency parse train neural classiﬁer assign score constituent use modiﬁed version cky combine constituent score ﬁnd well score parse tree parser evaluate metric label recall label precision andcross bracket parse andchunke method identify shallow tic constituent text solve sequence model train syntactically annotate datum historical note accord percival idea break sentence hierarchy constituent appear groundbreaking psychologist wilhelm wundt wundt den sprachlichen ausdruck die willk gliederung einer sammtvorstellung ihre logische beziehung zueinander gesetzten bestandteile linguistic expression arbitrary division total idea constituent part place logical relation wundt idea constituency take linguistic leonard ﬁeld early book introduction study language bloomﬁeld time later book language bloomﬁeld call immediate constituent analysis establish method syntactic study united states contrast traditional european grammar date classical period deﬁne relation word constituent ropean syntacticians retain emphasis dependency grammar ject chapter dependency constituency grammar vogue computational linguistic different time american structuralism see number speciﬁc deﬁnition immediate constituent couch term search discovery procedure ological algorithm describe syntax language general attempt capture intuition primary criterion immediate constituent degree combination behave simple unit bazell known speciﬁc deﬁnition harris idea distributional similarity individual unit substitutability test essentially method proceed break construction constituent attempt substitute simple structure possible constituent substitution simple form exercise man substitutable construction complex set like intense young man form intense young man probably constituent harris test beginning intuition constituent kind equivalence class context free grammar formalization idea hierarchical constituency deﬁne chomsky expand argue chomsky chomsky shortly chomsky initial work context free grammar reinvent backu pendently naur description algol programming language backus note inﬂuence production emil post naur work independent backus early work great number computational model natural language processing base context free grammar early development efﬁcient ing algorithm dynamic programming parsing history independent discovery cord late martin kay personal communication dynamic programming parser contain root cky algorithm ﬁrst implement john cocke later work extend formalize algorithm e time complexity kay young kasami relate form substre table wfst independently propose wfst kuno data structure store result previous computation course parse base generalization cocke work similar datum structure independently describe kay kay application dynamic programming parsing describe earley dissertation earley earley sheil show equivalence wfst earley algorithm norvig show efﬁciency fere dynamic programming capture language memoization function lisp simply wrap memoization operation simple parser early disambiguation algorithm parse base probabilistic context free grammar ﬁrst work booth salomaa seeprobabilistic context free grammarsappendix history neural method ﬁrst apply parse time statistical parse method develop henderson early work neural network estimate probability statistical constituency parser henderson emami jelinek decade see wide variety neural parse algorithm include cursive neural architecture socher encoder decoder model vinyal choe charniak idea focus span cross huang span base self attention approach describe chapter stern gaddy kitaev klein kitaev chapter parallel history neural dependency parsing classic reference parse algorithms aho ullman focus book computer language algorithm apply natural language exercise implement algorithm convert arbitrary context free grammar chapter ontext grammar constituency parse apply program implement cky algorithm test converted rewrite cky algorithm give fig page accept grammar contain unit production discuss augment parser deal input incorrect example contain spelling error mistake arise automatic speech recognition implement parsev metric describe section use parser treebank compare metric standard tion analyze error parse tout mot qui fait partie phrase entre lui ses voisin aperc des connexion forme charpente phrase word sentence neighbor mind perceive nection connection form scaffolding sentence lucien tesni ere syntaxe structurale focus chapter context free grammar base representation present important family grammar malism call dependency grammar dependency formalism phrasal con dependency grammar stituent phrase structure rule play direct role instead syntactic structure sentence describe solely term direct binary grammatical relation word following dependency parse iprefer themorne ﬂight denvernsubjobj det compoundnmod caseroot relation word illustrate sentence direct label arc head todependent type dependency structure becausetype dependency label draw ﬁxed inventory grammatical relation root node explicitly mark root tree head entire structure figure page show dependency analysis visualize tree alongside correspond phrase structure analysis kind give prior chapter note absence node correspond phrasal stituent lexical category dependency parse internal structure dependency parse consist solely direct relation word dependent relationship directly encode important information bury complex phrase structure parse example argument verb prefer directly link dependency structure connection main verb distant phrase structure tree similarly morning anddenver modiﬁer ﬂight link directly dependency structure fact head dependent relation good proxy semantic tionship predicate argument important reason dency grammar currently common constituency grammar natural language processing major advantage dependency grammar ability deal language relatively free word order example word order czech free word order ﬂexible english grammatical object occur location adverbial phrase structure grammar need separate chapter ependency parse prefer ﬂight denver throughmorning theis nom pro denverp throughnom noun ﬂightnom noun morningdet theverb prefernp pro figure dependency constituent analysis prefer morning ﬂight denver possible place parse tree adverbial phrase occur dependency base approach link type represent lar adverbial relation dependency grammar approach abstract away bit word order information follow section inventory relation dependency parsing discuss family parse algorithm transition base base discuss evaluation dependency relation traditional linguistic notion grammatical relation provide basis thegrammatical relation binary relation comprise dependency structure argument relation consist head dependent head play role central head dependent organizing word dependent kind modiﬁer head dependent tionship explicit directly link head word immediately dependent addition specify head dependent pair dependency grammar allow classify kind grammatical relation grammatical function thegrammatical function dependent play respect head include familiar notion subject direct object andindirect object english notion strongly late means determine position sentence constituent type somewhat redundant kind information find phrase structure tree language ﬂexible word order information encode directly grammatical relation critical base constituent syntax provide little help linguist develop taxonomy relation iar notion subject object considerable variation ependency relation clausal argument relation description nsubj nominal subject obj direct object iobj indirect object ccomp clausal complement nominal modiﬁer relation description nmod nominal modiﬁer amod adjectival modiﬁer appos appositional modiﬁer det determiner case preposition postposition case marker notable relation description conj conjunct coordinate conjunction figure universal dependency relation marneffe theory commonality cross linguistic standard develop universal dependency project marneffe dependencie open community effort annotate dependency aspect grammar language provide inventory dependency relation fig show subset relations fig provide example motivation relation universal dependency scheme scope chapter core set frequently relation break set clausal relation describe syntactic role respect predicate verb modiﬁer relation categorize way word modify head consider example follow sentence unite cancel themorne ﬂights tohoustonnsubjobj det compoundnmod caseroot clausal relation nsubj objidentify subject direct object predicate cancel nmod det case relation denote modiﬁer noun ﬂight andhouston dependency formalism dependency structure represent direct graph consist set vertex set order pair vertex arc assume set vertex correspond exactly set word give sentence correspond punctuation deal morphologically complex language set vertex consist stem afﬁxe set arc capture dependent grammatical function relationship element different grammatical theory formalism place constraint dependency structure frequent restriction ture connect designate root node acyclic planar relevance parsing approach discuss chapter chapter ependency parse relation example head anddependent nsubj united cancel ﬂight obj united divert theﬂight reno webooke ﬁrst ﬂight miami iobj webooke herthe ﬂight miami compound take morning ﬂight nmod ﬂight tohouston amod book cheap ﬂight appos united aunit ual match fare det ﬂight cancel ﬂight delay conj weﬂewto denver drive steamboat ﬂew denver anddrove steamboat case book ﬂight houston figure example universal dependency relation computationally motivate restriction rooted tree dependency treedependency tree direct graph satisﬁes following constraint single designate root node incoming arc exception root node vertex exactly incoming arc unique path root node vertex take constraint ensure word single head dependency structure connect single root node follow unique direct path word sentence projectivity notion projectivity impose additional constraint derive order word input arc head dependent say projective path head word lie head projective dependent sentence dependency tree say projective arc projective dependency tree see far projective valid construction lead non projective tree particularly language relatively ﬂexible word order consider follow example jetblue cancel ﬂight morning latensubjobjobl detacl relcl det nsubjcop advroot example arc ﬂight modiﬁer lateis non projective path ﬂight intervene word thisandmorne diagram projectivity non projectivity detect way draw tree dependency tree projective draw crossing edge way link ﬂight dependent latewithout cross arc link morning ependency relation concern projectivity arise related issue widely english dependency treebank automatically derive structure treebank use rule tree generate fashion projective incorrect non projective example like encounter second computational limitation widely family parse algorithm transition base approach discuss section produce projective tree sentence non projective structure necessarily contain error limitation motivation ﬂexible graph base parse approach describe section dependency treebank treebank play critical role development evaluation dependency parser training parser act gold label evaluate parser provide useful information corpus linguistic study dependency treebank create have human annotator directly generate dependency structure give corpus hand correct output automatic parser early treebank base deterministic process translate exist constituent base treebank dependency tree large open community project building dependency tree sal dependencie project introduce currently dependency treebank guage marneffe example show dependency tree sentence spanish basque mandarin chinese verb adp det noun adp det num punct subiremo tren las cinco board train ﬁve detcase detobl tmod casepunct spanish subiremos tren las cinco board train ﬁve noun noun verb aux punct ekaitzak itsasontzia hondoratu storm erg ship ab sunk obj auxpunct basque ekaitzak itsasontzia hondoratu storm sink ship chapter ependency parse adv pron noun adv verb verb noun 但我昨天 收到信 yesterday receive arrive letter nsubj obj tmod advmod compound vvobj receive letter yesterday transition base dependency parse ﬁrst approach dependency parsing call transition base parsing transition base architecture draw shift reduce parsing paradigm originally develop analyze programming language aho ullman transition base parse stack build parse buffer token parse parser take action parse predictor call oracle illustrate fig figure basic transition base parser parser examine element stack select action consult oracle examine current conﬁguration parser walk sentence leave right successively shift item buffer stack time point examine element stack oracle make decision transition apply build parse possible transition correspond intuitive action create dependency tree examine word single pass input left right covington assign current word head previously see word assign previously see word head current word postpone deal current word store later processing formalize intuition following transition operator operate element stack arc assert head dependent relation word stack second word remove second word stack arc assert head dependent relation second word stack word remove word ransition dependency parse remove word input buffer push stack operation like leave arcand right arcreduce operation base metaphor shift reduce parsing reduce mean ing element stack precondition operator leave arcoperator apply root second element stack deﬁnition root node incoming arc leave arcand right arcoperator require element stack apply particular set operator implement know arc standard arc standard approach transition base parsing covington nivre arc standard parse transition operator assert relation element stack element assign head remove stack available processing tive transition system demonstrate different parsing behavior arc standard approach effective simple implement speciﬁcation transition base parser simple base sente current state parse conﬁguration stack input buffer conﬁguration word token set relation represent dependency tree parse mean make sequence transition space possible tion start initial conﬁguration stack contain root node buffer token sentence set relation sent parse ﬁnal goal state stack word list set relation represent ﬁnal parse fig give algorithm function dependency parse word return dependency tree state word initial conﬁguration state ﬁnal oracle state choose transition operator apply state apply state apply create new state return state figure generic transition base dependency parser step parser consult oracle come shortly provide correct transition operator use give current conﬁguration apply operator current conﬁguration produce new conﬁguration process end word sentence consume root node element remain stack efﬁciency transition base parser apparent algorithm complexity linear length sentence base single leave right pass word sentence word ﬁrst shift stack later reduce note unlike dynamic programming search base approach cuss chapter approach straightforward greedy algorithm acle provide single choice step parser proceed choice option explore backtracking employ single parse return end figure illustrate operation parser sequence chapter ependency parse lead parse following example book methemorning ﬂightiobjobj det compoundroot let consider state conﬁguration step word mehas push stack stack word list relation root book morning ﬂight correct operator apply right arcwhich assign book head meand pop mefrom stack result following conﬁguration stack word list relation root book morning ﬂight subsequent application shift operator conﬁguration step look like follow stack word list relation root book morning ﬂight remain word pass stack leave apply appropriate reduce operator current conﬁguration employ left arcoperator result following state stack word list relation root book ﬂight morning ﬂight point parse sentence consist following structure book methemorne ﬂightiobj compound important thing note examine sequence figure sequence give lead reasonable parse general path lead result ambiguity transition sequence lead different equally valid parse second assume oracle provide correct operator point parse assumption unlikely true practice result give greedy nature algorithm incorrect choice lead incorrect parse parser opportunity pursue alternative choice section introduce technique allow transition base approach explore search space ransition dependency parse step stack word list action relation add root book morning ﬂight shift root book morning ﬂight shift root book morning ﬂight right arc root book morning ﬂight shift root book morning ﬂight shift root book morning ﬂight shift root book morning ﬂight leave arc morning ﬂight root book ﬂight leave arc ﬂight root book ﬂight right arc root book right arc root figure trace transition base parse finally simplicity illustrate example label dependency relation produce label tree parameterize leave arcand right arcoperator dependency label left right equivalent expand set transition operator original set set include leave arcand right arcoperator relation set dependency relation plus additional shift operator course make job oracle difﬁcult large set operator choose create oracle oracle greedily select appropriate transition train supervised machine learning supervise machine learn method need training datum conﬁguration annotate correct transition draw dependency tree need extract feature ﬁguration introduce neural classiﬁer represent conﬁguration embedding classic system use hand design feature generate training datum oracle algorithm fig take input conﬁguration return transition operator train classiﬁer need conﬁguration pair transition operator left arc right arc shift unfortunately treebank pair entire sentence correspond tree conﬁguration transition generate require training datum employ oracle base parse rithm clever way supply oracle training sentence parse corresponding reference parse treebank produce ing instance simulate operation parser run algorithm rely new training oracle correct transition operator training oracle successive conﬁguration work let ﬁrst review operation parser begin default initial conﬁguration stack contain root input list list word set relation left arcand right arc operator add relation word stack set relation accumulate give sentence gold standard reference parse training sentence know dependency relation valid give sentence use reference parse guide chapter ependency parse step stack word list predict action root book ﬂight houston shift root book ﬂight houston shift root book ﬂight houston shift root book ﬂight houston leave arc root book ﬂight houston shift root book ﬂight houston shift root book ﬂight houston left arc root book ﬂight houston right arc root book ﬂight right arc root book right arc root figure generating training item consist conﬁguration predict action pair simulate parse give reference parse selection operator parser step sequence conﬁguration precise give reference parse conﬁguration training oracle proceed follow choose leave arcif produce correct head dependent relation give reference parse current conﬁguration choose right arcif produce correct head dependent lation give reference parse dependent word stack assign choose shift restriction select right arcoperator need ensure word pop stack lose processing dependent assign formally train oracle access follow current conﬁguration stack sand set dependency relation reference parse consist set vertex vand set dependency relation give information oracle choose transition follow leave right shift let walk processing follow example show fig book theﬂight houstonobj detnmod caseroot step leave arcis applicable initial conﬁguration assert relation root book reference answer right arcdoe assert relation contain ﬁnal answer root book book attach dependent defer leave shift ransition dependency parse possible action condition hold step step leave arc select link theto head consider situation step stack word buffer relation root book ﬂight houston ﬂight tempt add dependency relation book andﬂight present reference parse prevent later attachment houston ﬂight remove stack tunately precondition choose right arcprevent choice leave shift viable option remain choice complete set operator need example recap derive appropriate training instance consist transition pair treebank simulate operation parser text reference dependency tree deterministically record correct parser action step progress training example create training set require feature base classiﬁer introduce classiﬁer choose transition classic base algorithm section neural classiﬁer embed feature feature base classiﬁer generally use feature see speech tagging partial parsing word form lemmas part speech head dependency relation head feature relevant language example morphosyntactic feature like case mark subject object feature extract training conﬁguration consist stack buffer current set relation useful feature reference level stack word near buffer dependency relation associate element use feature template sentiment analysis speechfeature template tagging feature template allow automatically generate large number ciﬁc feature training set example consider follow feature plate base single position conﬁguration feature denote location property stack word buffer word form speech operator feature word form stack speech tag buffer concatenate feature wtrepresent word form concatenate speech word stack consider apply template follow intermediate conﬁguration derive training oracle stack word buffer relation root cancel ﬂights houston chapter ependency parse correct transition shift convince proceeding application set feature template conﬁguration result following set instantiated feature shifti shifti shifti shifti shifti shifti shifti give left right arc transition operate element stack feature combine property position useful example feature like tconcatenate speech tag word stack tag word beneath shifti give training datum feature classiﬁer like multinomial logistic gression support vector machine neural classiﬁer oracle implement neural classiﬁer standard architecture simply pass sentence encoder presentation word stack ﬁrst word buffer concatenate present feedforward network predict transition kiperwasser goldberg kulmizev fig sketch model learning cross entropy loss figure neural classiﬁer oracle transition base parser parser take word stack ﬁrst word buffer represent encoding run sentence encoder concatenate embedding pass softmax choose parser action ransition dependency parse advanced method transition base parse basic transition base approach elaborate number way prove performance address obvious ﬂaw approach alternative transition system arc standard transition system describe possible tem frequently alternative arc eager transition system arc eager arc eager approach get ability assert rightward relation soon arc standard approach let revisit arc standard trace example repeat book theﬂight houstonobj detnmod caseroot consider dependency relation book andﬂight analysis show fig arc standard approach assert relation step despite fact book andﬂight ﬁrst come stack early step reason relation capture point presence postnominal modiﬁer houston arc standard approach dent remove stack soon assign head ﬂight assign book head step long available serve head houston delay cause issue example general long word wait assign head opportunity awry arc eager system address issue allow word attach head early possible subsequent word dependent see accomplish minor change theleft arcand right arcoperator addition new reduce operator arc assert head dependent relation word input buffer word stack pop stack arc assert head dependent relation word stack word input buffer shift word input buffer stack remove word input buffer push stack pop stack left arcand right arcoperator apply stack input buffer instead element stack arc standard approach right arcoperator move dependent stack buffer remove make available serve head follow word new reduce operator remove element stack change permit word eagerly assign head allow serve head later dependent trace show fig illustrate new decision sequence example addition demonstrate arc eager transition system example strate power ﬂexibility overall transition base approach able swap new transition system have change chapter ependency parse step stack word list action relation add root book ﬂight houston right arc root book ﬂight houston shift root book ﬂight houston leave arc ﬂight root book ﬂight houston right arc root book ﬂight houston shift root book ﬂight houston leave arc houston root book ﬂight houston right arc root book ﬂight houston reduce root book ﬂight reduce root book reduce root figure processing trace book ﬂight houston arc eager transition operator underlie parse algorithm ﬂexibility lead development verse set transition system address different aspect syntax semantic include assign speech tag choi palmer allow generation non projective dependency structure nivre assign tic role choi palmer parse text contain multiple language bhat beam search computational efﬁciency transition base approach discuss early rive fact make single pass sentence greedily make decision consider alternative course weakness decision undo face overwhelming evidence arrive later sentence use beam search explore beam search tive decision sequence recall chapter beam search use search strategy heuristic ﬁlter prune search frontier stay ﬁxed size beam width beam width apply beam search transition base parsing elaborate gorithm give fig instead choose single good transition operator iteration apply applicable operator state agenda score result conﬁguration add new tion frontier subject constraint room beam long size agenda speciﬁed beam width add new conﬁguration agenda agenda reach limit add new conﬁguration well bad conﬁguration agenda remove bad element stay limit finally insure retrieve good possible state agenda loop continue long state agenda beam search approach require elaborate notion scoring greedy algorithm assume oracle supervise classiﬁer choose good transition operator base feature current conﬁguration choice view assign score possible transition pick good argmaxscore beam search search space decision sequence make sense base score conﬁguration entire history deﬁne score new conﬁguration score predecessor plus raph dependency parse score operator produce conﬁgscore conﬁgscore conﬁgscore score ﬁltere agenda select ﬁnal answer new beam search version transition base parsing give fig function dependency beam parse word width return dependency tree state word conﬁguration agenda hstatei initial agenda agenda contain state newagenda operator state gdo child apply state newagenda addtobeam child newagenda width agenda newagenda return function addtobeam state agenda width return update agenda iflength agenda width agenda insert state agenda score state score bad agenda remove bad agenda insert state agenda return agenda figure beam search apply transition base dependency parsing graph base dependency parse graph base method second important family dependency parse rithm graph base parser accurate transition base parser cially long sentence transition base method trouble head far dependent mcdonald nivre graph base method avoid difﬁculty score entire tree rely greedy local cision furthermore unlike transition base approach graph base parser produce non projective tree projectivity signiﬁcant issue english deﬁnitely problem world language graph base dependency parser search space possible tree give sentence tree tree maximize score method encode search space direct graph employ method draw graph theory search space optimal solution formally give sentence look good dependency tree space possible tree sentence maximize score argmax chapter ependency parse simplifying assumption score edge factor edge factor meaning overall score tree sum score score edge comprise tree score graph base algorithm solve problem assign score edge ﬁnde good parse tree give score potential edge section introduce solution problem begin second problem ﬁnding tree give feature base neural algorithm solve ﬁrst problem assign score parse ﬁnde maximum span tree graph base parsing give sentence swe start create graph gwhich fully connect weight direct graph vertex input word direct edge represent possible head dependent assignment include additional root node outgoing edge direct vertex weight edge greﬂect score possible head dependent relation assign scoring algorithm turn ﬁnde good dependency parse sis equivalent ﬁnding themaximum span tree span tree graph gis subsetmaximum span tree ofgthat tree cover vertex span tree gthat start root valid parse maximum span tree span tree high score maximum span tree gemanate root optimal dependency parse sentence direct graph example book ﬂight show fig maximum span tree correspond desire parse show blue ease exposition describe algorithm unlabeled dependency parsing figure initial root direct graph book ﬂight describe algorithm useful consider intuition recte graph span tree ﬁrst intuition begin fact vertex span tree exactly incoming edge follow connected component span tree set vertex link path edge incoming edge second intuition absolute value edge score critical determine maximum span tree instead relative weight edge enter vertex matter subtract constant edge enter give vertex impact choice raph dependency parse maximum span tree possible span tree decrease exactly ﬁrst step algorithm straightforward vertex graph incoming edge represent possible head assignment high score choose result set edge produce span tree formally give original fully connect graph subgraph span tree cycle vertex root exactly edge enter greedy selection process produce tree well possible unfortunately approach lead tree set edge select contain cycle fortunately case multiple discovery straightforward way eliminate cycle generate greedy lection phase chu liu edmond independently develop approach begin greedy selection follow elegant recursive cleanup phase eliminate cycle cleanup phase begin adjust weight graph subtract score maximum edge enter vertex score edge enter vertex intuition mention early come play scale value edge weight edge cycle bearing weight anyof possible span tree subtract value edge maximum weight edge enter vertex result weight zero edge select greedy selection phase include edge involve cycle having adjust weight algorithm create new graph select cycle collapse single new node edge enter leave cycle alter enter leave newly collapse node edge touch cycle include edge cycle drop know maximum span tree new graph need eliminate cycle edge maximum span tree recte vertex represent collapse cycle tell edge delete order eliminate cycle ﬁnd maximum span tree new graph recursively apply algorithm new graph result span tree graph cycle recursion continue long cycle encounter recursion complete expand collapse vertex restore vertex edge cycle tion single edge delete put maximum span tree algorithm consist greedy edge selection scoring edge cost recursive cleanup phase need algorithm show fig fig step algorithm book ﬂight example ﬁrst row ﬁgure illustrate greedy edge selection edge choose show blue correspond set fin algorithm result cycle thatandﬂight scale weight maximum value enter node show graph right collapse cycle andﬂight single node label recurse newly scale cost show second row greedy tion step recursion yield span tree link roottobook edge link book contract node expand contract node edge correspond edge book toﬂight original graph turn tell edge drop eliminate chapter ependency parse function maxspanning root score return span tree score bestinedge span tree return cycle contract maxspanning score expand return function contract contract graph function expand expand graph figure chu liu edmond algorithm ﬁnde maximum span tree weight direct graph arbitrary direct graph version cle algorithm run time mis number edge nis number node ticular application algorithm begin construct fully connect graph running time gabow present cient implementation running time feature base algorithm assign score recall give sentence candidate tree edge factor parse model simpliﬁcation score tree sum score edge comprise tree feature base algorithm compute edge score weighted sum ture extract succinctly give formulation need identify relevant feature train weight feature feature combination train edge factor model ror training transition base parser raph dependency parse rootbooktf delete cycle figure chu liu edmonds graph base example book ﬂight wordform lemma part speech headword dependent corresponding feature contexts word word embedding dependency relation direction relation right left distance head dependent give set feature problem learn set weight ing unlike learning problem discuss early chapter train model associate training item class label parser action instead seek train model assign high score rect tree incorrect one effective framework problem like useinference base learning combine perceptron learning rule thisinference base learning framework parse sentence perform inference training set initially random set initial weight result parse match respond tree training datum weight ﬁnd feature incorrect parse notpresent reference parse lower weight small base learning rate incrementally sentence training datum weights chapter ependency parse neural algorithm assign score state art graph base multilingual parser base neural network stead extract hand design feature represent edge word andwj parser run sentence encoder pass encoded representation word wiandwjthrough network estimate score edge figure computing score single edge book ﬂight biafﬁne parser dozat manning dozat parser use distinct feedforward work turn encoder output word head dependent representation word biafﬁne function turn head embed head dependent embedding dependent score dependency edge sketch biafﬁne algorithm dozat manning dozat show fig draw work test version algorithm step system algorithm ﬁrst run sentence encoder produce contextual embed representation token embed token pass separate feedforward network produce representation token head produce representation token dependent hhead hdep assign score direct edge wiis head wji dent feed head representation hhead dependent representation ofj hdep biafﬁne scoring function score valuation bare weight learn model idea biafﬁne function allow system learn multiplicative interaction torsxandy pass score softmax end probability bution token potential head token sentence softmax score probability pass maximum span tree algorithm section ﬁnd good tree train optimize cross entropy loss note algorithm describe unlabeled label algorithm dozat manning algorithm actually train classiﬁer ﬁrst classiﬁer edge scorer describe assign probability word wiandwj maximum span tree algorithm run single good dependency parse tree second apply second classiﬁer label scorer job ﬁnd maximum ability label edge parse second classiﬁer form instead train predict binary softmax probability edge exist word train softmax dependency label predict dependency label word evaluation phrase structure base parsing evaluation dependency parser ceed measure work test set obvious metric exact match sentence parse correctly metric pessimistic sentence mark wrong measure grain guide development process metric need sensitive tell actual improvement reason common method evaluate dependency parser label unlabeled attachment accuracy label attachment refer proper assignment word head correct dependency relation unlabeled attachment simply look correctness assign head e dependency relation give system output corresponding reference parse accuracy simply percentage word input assign correct head correct relation metric usually refer label attachment score las unlabeled attachment score uas finally use label accuracy score percentage token correct label ignore relation come example consider reference parse system parse follow example show fig book ﬂight houston system correctly ﬁnd dependency relation present reference parse receive las incorrect relation find system hold book andﬂight head dependent relation reference parse system achieve uas attachment score interested system perform particular kind dependency relation example nsubj chapter ependency parse book methe ﬂight houston referenceiobjobj detnmod caseroot book methe ﬂight houston systemxcomp nsubj detnmod caseroot figure reference system parse book ﬂight houston result las uas development corpus use notion precision recall introduce chapter measure percentage relation label nsubj system correct precision percentage nsubj relation present development set fact discover system recall employ confusion matrix track dependency type confuse summary chapter introduce concept dependency grammar dependency parsing summary main point cover dependency base approach syntax structure sentence scribe term set binary relation hold word sentence large notion constituency directly encode dency analysis relation dependency structure capture head dependent ship word sentence dependency base analysis provide information directly useful language processing task include information extraction semantic parsing question answer transition base parsing system employ greedy stack base algorithm create dependency structure graph base method create dependency structure base use maximum span tree method graph theory transition base graph base approach develop vise machine learn technique treebank provide datum need train system dependency bank create directly human annotator automatic mation phrase structure treebank evaluation dependency parser base label unlabeled accuracy score measure withhold development test note historical note dependency base approach grammar old relatively recent phrase structure constituency grammar date century pendency grammar date indian grammarian century bce ancient greek linguistic tradition contemporary theory dependency grammar draw heavily tury work tesni ere automatic parsing dependency grammar ﬁrst introduce tational linguistic early work machine translation rand corporation lead david hays work dependency parse closely parallel work constituent parsing explicit use grammar guide parsing process early period computational work dependency parsing remain mittent following decade notable implementation dependency parser english period include link grammar sleator temperley constraint grammar karlsson minipar lin dependency parsing see major resurgence late ance large dependency base treebank associated advent datum drive approach describe chapter eisner develop efﬁcient dynamic programming approach dependency parsing base bilexical grammar derive penn treebank covington introduce deterministic word word approach underlie current transition base approach yamada sumoto kudo matsumoto introduce shift reduce paradigm use supervise machine learning form support vector machine dependency parsing transition base parsing base shift reduce parse algorithm inally develop analyze programming language aho ullman shift reduce parsing make use context free grammar input token successively shift stack element stack match right hand rule grammar match find match element replace stack reduced non terminal leave hand rule match transition base dependency parse skip grammar alter reduce operation add dependency relation word head nivre deﬁne modern deterministic transition base approach dependency parsing subsequent work nivre colleague formalize analyze performance numerous transition system training method method deal non projective language nivre scholz nivre nivre nilsson nivre nivre neural proach pioneer chen manning extend kiperwasser goldberg kulmizev graph base maximum span tree approach dependency parsing introduce mcdonald mcdonald neural classiﬁer introduce kiperwasser goldberg long run prague dependency treebank project haji signiﬁcant effort directly annotate corpus multiple layer morphological syntactic semantic information pdt contain token bej ˇcek universal dependency marneffe open chapter ependency parse project create framework dependency treebank annotation nearly treebank language annotation scheme evolve distinct effort include stanford dependency marneffe effe manning marneffe google universal speech tag petrov interset interlingua morphosyntactic tagset zeman conference natural language learning conll conduct ﬂuential series share task relate dependency parsing year holz marsi nivre surdeanu haji recent evaluation focus parser robustness respect logically rich language seddah non canonical language form social medium text speak language petrov mcdonald choi present performance analysis dependency parser range metric depend able robust parser evaluation tool exerciseschapter extraction relation event time time explain jane austen persuasion imagine analyst investment ﬁrm track airline stock give task determine relationship airline nouncement fare increase behavior stock day torical datum stock price easy come airline nouncement need know airline nature propose fare hike date announcement possibly response airline fortunately find news article like cite high fuel price united airlines say friday increase fare round trip ﬂights city serve cost carrier american airlines unit amr corp immediately match spokesman tim wagner say united unit ual corp say increase take effect thursday apply route compete discount carrier chicago dallas denver san francisco chapter present technique extract limited kind semantic tent text process information extraction turn unstructuredinformation extraction information embed text structured datum example populate relational database enable processing begin task relation extraction ﬁnding classify semanticrelation extraction relation entity mention text like child child geospatial relation relation extraction close link e relational database knowledge graph dataset structured relationalknowledge graph knowledge useful way search engine present information user discuss event extraction task ﬁnde event event extraction titie participate like sample text fare increase united andamerican reporting event say andcite event situate time occur particular date time event relate temporally happen simultaneously need recognize temporal sion like friday thursday ortwo day time normalize speciﬁc calendar date time need link friday time united announcement thursday previous day fare increase need produce timeline united announcement follow fare increase american announcement follow event related task template ﬁlling ﬁnd recur stereotypical event template ﬁlling situation document ﬁll template slot consist text segment extract directly text concept like time amount ontology entity infer additional processing chapter nformation extraction relation event time artifactgeneralaffiliationorgaffiliationpart wholeperson socialphysicallocatednearbusinessfamilylaste personalcitizen resident ethnicity religionorg location originfounderemploymentmembershipownershipstudent aluminvestoruser owner inventor manufacturergeographicalsubsidiary sport affiliation figure relation ace relation extraction task text present stereotypical situation airline raise fare wait competitor follow identify unite lead line initially raise fare thursday increase date andamerican airline follow lead ﬁlled template like following fare raise attempt airline nited airline effective date follower merican relation extraction let assume detect name entity sample text technique chapter like discern relationship exist detect entity cite high fuel price org united airlines say time friday increase fare money round trip ﬂights city serve low cost carrier org american airlines unit org amr corp immediately match spokesman tim wagner say org united unit org ual corp say increase take effect time thursday apply route compete discount carrier loc chicago loc dallas loc denver loc san francisco text tell example tim wagner spokesman american airline united unit ual corp american unit amr binary relation instance generic relation employ fairly frequent news style text figure list relation ace relation extraction evaluation fig show sample relation extract domain speciﬁc relation notion airline route example text conclude united route chicago dallas denver san elation extraction relation type example physical locate gpe hewa tennessee subsidiary org org xyz parent company abc person social family yoko husband john org aff founder org steve job founder apple figure semantic relation example name entity type involve set relation deﬁne domain example umls uniﬁed medical language system national library medicine network deﬁne broad subject category entity type relation entity follow entity relation entity injury disrupt physiological function bodily location location biologic function anatomical structure organism pharmacologic substance cause pathological function pharmacologic substance treat pathologic function give medical sentence like doppler echocardiography diagnose left anterior descend artery stenosis patient type diabete extract umls relation echocardiography doppler diagnosis acquire stenosis wikipedia offer large supply relation draw infoboxe infoboxe ture table associate certain wikipedia article example wikipedia infobox stanford include structured fact like state california president marc tessi lavigne fact turn tion like president orlocate relation metalanguage call rdf rdf resource description framework rdf triple tuple entity rdf triple entity call subject predicate object expression sample rdf triple subject predicate object golden gate park location san francisco example crowdsourced dbpedia bizer ontology rive wikipedia contain billion rdf triple dataset wikipedia infoboxe freebase bollacker wikidata vrande freebase relation people nationality location location contain wordnet ontology offer useful ontological relation express archical relation word concept example wordnet aor hypernym relation class hypernym giraffe ruminant ungulate mammal vertebrate wordnet instance relation individual class example san francisco instance relation city extract relation important step extend building ontology finally large dataset contain sentence hand label relation design training testing relation extractor tacre dataset zhang contain example relation triple particular people organization label sentence news web text draw chapter nformation extraction relation event time annual tac knowledge base population tac kbp challenge tacre contain relation type like city birth org subsidiary org member spouse plus relation tag example show fig example annotate relation have sufﬁcient negative datum important training supervise classiﬁer example entity type label carey succeed cathleen black hold position year new role chairwoman hearst zine company relation title irene morgan kirkaldy bear rear baltimore live long island run child care center queen second husband stanley relation city ofbirth baldwin decline comment say jetblue chief executive dave barger person relation norelation figure example sentence label tacre dataset zhang standard dataset produce semeval task detect relation nominal hendrickx dataset ple pair nominal untyped hand label direct relation like product producer afactory manufacture suit orcomponent myapartment large kitchen relation extraction algorithm ﬁve main class algorithm relation extraction handwritten tern supervise machine learning semi supervised viabootstrappe tant supervision unsupervised introduce section pattern extract relation early common algorithm relation extraction lexico syntactic pattern ﬁrst develop hearst call hearst tern consider following sentence hearst pattern agar substance prepare mixture red algae lidium laboratory industrial use hearst point human reader know gelidium readily infer kind hyponym algae suggest follow lexico syntactic pattern imply follow semantic allow infer hyponym algae elation extraction algorithm npf temple treasury important civic building nphsuch asfnp red algae gelidium hasfnp author herrick goldsmith shakespeare nphf gincludingfnp common law country include canada england nphf european country especially france england spain figure hand build lexico syntactic pattern ﬁnde hypernyms fgto mark optionality hearst hearst figure show ﬁve pattern hearst suggest infer hyponym relation show npha parent hyponym modern version pattern base approach extend add name entity constraint example goal answer question hold ofﬁce organization use pattern like follow position org george marshall secretary state united states namedjappointedjchosejetc prep position truman appoint marshall secretary state namedjappointedjetc prep org position george marshall name secretary state hand build pattern advantage high precision tailor speciﬁc domain hand low recall lot work create possible pattern relation extraction supervised learning supervise machine learning approach relation extraction follow scheme familiar ﬁxed set relation entity choose training corpus hand annotate relation entity annotated text train classiﬁer annotate unseen test set straightforward approach illustrate fig find pair name entity usually sentence apply relation classiﬁcation pair classiﬁer use supervise technique logistic regression rnn transformer random forest etc optional intermediate ﬁltere classiﬁer speed ing make binary decision give pair name entity relate relation train positive example extract directly tion annotated corpus negative example generate sentence entity pair annotate relation feature base supervised relation classiﬁer let consider sample feature feature base classiﬁer like logistic regression random forest classify relationship american airline mention tim wagner tion sentence american airline unit amr immediately match spokesman tim wagner say include word feature embedding hot stem headword concatenation airline wagner airlines chapter nformation extraction relation event time function findrelation word return relation relation nil entity findentitie word forall entity pair ifrelate relation relation classify relation figure finding classify relation entity text bag word bigram american airlines tim wagner american airlines tim wagner word bigram particular position spokesman say bag word bigram amr immediately match spokesman unit name entity feature name entity type concatenation org org entity level set nominal pronoun pronoun company nominal number entity argument case amr syntactic structure useful signal represent dependency constituency syntactic path traverse tree entity constituent path dependency tree path airline sub jmatche comp jwagner neural supervise relation classiﬁer neural model relation extraction ilarly treat task supervised classiﬁcation let consider typical system ply tacre relation extraction dataset task zhang tacre give sentence span subject person organization object entity task assign relation tac relation include relation typical transformer encoder algorithm show fig simply take pretraine encoder like bert add linear layer sentence sentation example bert cls token linear layer ﬁnetune classiﬁer assign label input bert encoder partially lexiﬁed subject object entity replace input ner tag help system overﬁtte individual lexical item zhang bert type transformer relation extraction help use version bert like roberta liu spanbert joshi sequence separate sep token instead form input single long sequence sentence general test set similar training set hand label datum supervise relation extraction system high elation extraction algorithm obj figure relation extraction linear layer encoder case bert subject object entity replace input ner tag zhang joshi curacy label large training set extremely expensive supervised model brittle generalize different text genre son research relation extraction focus semi supervised unsupervised approach turn semisupervise relation extraction bootstrappe supervise machine learning assume lot label datum nately expensive suppose high precision seed tern like section seed tuple seed pattern seed tuple bootstrap classiﬁer bootstrappe proceed take entity seed bootstrappe pair ﬁnde sentence web dataset contain entity sentence extract generalize context entity learn new pattern fig sketch basic algorithm function bootstrap relation return new relation tuple tuple gather set seed tuple relation iterate sentence ﬁnd sentence contain entity tuple pattern generalize context entity sentence newpair usepattern identify tuple newpair newpair high conﬁdence tuple tuple newpair return tuple figure bootstrappe seed entity pair learn relation suppose example need create list airline hub pair know ryanair hub charleroi use seed fact discover new pattern ﬁnde mention relation corpus search term ryanair charleroi andhubin proximity ﬁnd following set sentence budget airline ryanair use charleroi hub scrap weekend ﬂight airport ﬂight ryanair hub charleroi airport ground friday spokesman charleroi main hub ryanair estimate passenger chapter nformation extraction relation event time result use context word entity mention word mention word mention name entity type mention feature extract general pattern following org use loc hub org hub loc loc main hub org new pattern search additional tuple bootstrappe system assign conﬁdence value new tuple avoid conﬁdence value mantic drift semantic drift erroneous pattern lead introduction semantic drift erroneous tuple turn lead creation problematic pattern meaning extract relation drift consider follow example sydney ferry hub circular quay accept positive example expression lead incorrect troduction tuple pattern base tuple propagate error database conﬁdence value pattern base balance factor pattern performance respect current set tuple pattern productivity term number match produce document collection formally give document collection current set tuple propose pattern need track factor set tuple tthatpmatche look total set tuple pﬁnd follow equation balance consideration riloff jones metric generally normalize produce probability assess conﬁdence propose new tuple combine evidence support pattern match tuple vano way combine evidence noisy technique assume noisy give tuple support subset pattern conﬁdence assess noisy model basic tion propose tuple false allof support pattern error second source individual failure independent loosely treat conﬁdence measure probability probability individual pattern pfaile probability support pattern tuple wrong product individual failure probability leave follow equation conﬁdence new tuple set conservative conﬁdence threshold acceptance new pattern tuple bootstrapping process help prevent system drift away targeted elation extraction algorithm distant supervision relation extraction hand labeling text relation label expensive produce way ﬁnd indirect source training datum distant supervision methoddistant supervision mintz combine advantage bootstrappe supervised ing instead handful seed distant supervision use large database acquire huge number seed example create lot noisy pattern feature example combine supervised classiﬁer example suppose try learn place birth relationship tween people birth city seed base approach example start wikipedia base database like dbpedia freebase ten thousand example relation include ample place birth edwin hubble marshfield albert einstein ulm etc step run name entity tagger large amount text mintz article wikipedia extract sentence name entity match tuple like follow hubble bear marshﬁeld einstein bear ulm hubble birthplace marshﬁeld training instance extract datum training instance identical tuple relation training instance bear edwin hubble marshfield bear albert einstein ulm bear year albert einstein apply feature base neural classiﬁcation feature base classiﬁcation use standard supervise relation extraction feature like name entity label mention word dependency path tween mention neighboring word tuple feature lecte training instance feature vector single training instance like bear albert einstein ulm lexical syntactic feature different sentence mention einstein ulm distant supervision large training set able use rich feature conjunction individual feature extract thousand pattern conjoin entity type intervene word dependency path like bear loc bear xxxx loc birthplace loc return run example sentence american airline unit amr immediately match spokesman tim wagner say learn rich conjunction feature like org result supervised classiﬁer huge rich set feature use detect relation test sentence chapter nformation extraction relation event time relation classiﬁer need able label example relation label train randomly select entity pair appear freebase relation extract feature build feature vector tuple ﬁnal algorithm sketch fig function distant supervision database text return relation classiﬁer foreach relation foreach tuple entity relation rind sentence sentence tthat contain frequent feature sentence observation observation new training tuple train supervise classiﬁer observation return figure distant supervision algorithm relation extraction neural classiﬁer skip feature set distant supervision share advantage method ine like supervised classiﬁcation distant supervision use classiﬁer lot feature supervise detailed hand create knowledge like pattern base classiﬁer use high precision evidence relation titie distance supervision system learn pattern like hand build pattern early relation extractor example aorhypernym extraction system snow hypernym hyponym pair wordnet distant supervision learn new pattern large amount text system induce exactly original template pattern hearst additional pattern include nphlike hormone like leptin nphcalle markup language call xhtml ruby programming language ibm company long ability use large number feature simultaneously mean like iterative expansion pattern seed base system semantic drift like unsupervised classiﬁcation use label training corpus text sensitive genre issue training corpus rely large amount unlabeled datum distant supervision advantage create training tuple neural classiﬁer feature require main problem distant supervision tend produce low precision result current research focus way improve precision furthermore distant supervision help extract relation large database exist extract new relation dataset relation new domain purely unsupervised method unsupervised relation extraction goal unsupervised relation extraction extract relation web label training datum list relation task call open information extraction oropen open relationsopen information elation extraction algorithm simply string word usually begin verb example reverb system fader extract relation sentence sin step run speech tagger entity chunker verb ﬁnd long sequence word wthat start verb satisfy syntactic lexical constraint merge adjacent match phrase ﬁnd near noun phrase xto left relative pronoun word existential find near noun phrase yto right assign conﬁdence cto relation conﬁdence classiﬁer return relation accept meet syntactic lexical constraint syntactic constraint ensure verb initial sequence include noun relation begin light verb like ordooften express core relation noun like hub verb particle adv nounjadjjadvjpronjdet prepjparticlejinﬁnitive lexical constraint base dictionary dthat prune rare long relation string intuition eliminate candidate relation cur sufﬁcient number distinct argument type likely bad example system ﬁrst run relation extraction algorithm ofﬂine million web sentence extract list relation occur malize remove inﬂection auxiliary verbs adjective adverb relation ris add dictionary occur different argument fader dictionary million normalize relation finally conﬁdence value compute relation logistic gression classiﬁer classiﬁer train take random web sentence run extractor hand labeling extract relation correct rect conﬁdence classiﬁer train hand label datum feature relation surround word fig show sample feature classiﬁcation cover word preposition risfor preposition rison coordinate conjunction left rin rmatche lone syntactic constraint preposition left xin right yin figure feature classiﬁer assign conﬁdence relation extract open information extraction system reverb fader example follow sentence united hub chicago headquarters united continental chapter nformation extraction relation event time relation phrase hub andis headquarters hasand long phrase prefer step ﬁnd unite left chicago right hub skip ﬁnd chicago left headquarters ﬁnal output united hub chicago chicago headquarters united continental holdings great advantage unsupervised relation extraction ability handle huge number relation have specify advance advantage need map string canonical form add database knowledge graph current method focus heavily relation press verb miss relation express nominally evaluation relation extraction supervise relation extraction system evaluate test set annotate gold standard relation compute precision recall measure label precision recall require system classify relation correctly unlabeled method simply measure system ability detect entity relate semi supervised andunsupervised method difﬁcult eat extract totally new relation web large text method use large amount text generally possible run solely small label test set result possible pre annotate gold set correct instance relation method possible approximate precision draw random sample relation output have human check accuracy relation usually approach focus tuple extract body text relation mention system need detect mention relation score correctly instead evaluation base set tuple occupy database system ﬁnishe want know system discover ryanair hub charleroi care time discover estimate precision ˆpis correctly extract relation tuple sample total extract relation tuple approach give little bit information recall pute precision different level recall assume system able rank relation produce probability conﬁdence separately pute precision new relation new relation case random sample set precision curve behave extract tuple way directly evaluate recall extract event task event extraction identify mention event text theevent extraction purpose task event mention expression denote event state assign particular point interval time follow markup sample text page show event epresente time event cite high fuel price united airlines event say day event increase fare round trip ﬂights city serve low cost carrier american airlines unit amr corp immediately event match event spokesman tim wagner event say united unit ual corp event say event increase take effect thursday event apply route event compete discount carrier chicago dallas denver san francisco english event mention correspond verbs verb introduce event example case event introduce noun phrase andthe increase verb fail introduce event phrasal verb take effect refer event begin event similarly light verb light verbs andhave fail denote event light verb verb little meaning associated event instead express direct object noun light verb example like take ﬂight word ﬂight deﬁne event light verb provide syntactic structure noun argument version event extraction task exist depend goal example tempeval share task verhagen goal extract event aspect like aspectual temporal property event classiﬁe action state report event report tell explain perceptionreporte event event aspect tense modality event need extract example say event sample text annotate class report tense past aspect perfective event extraction generally model supervised learning detect event iob sequence model assign event class attribute multi class classiﬁer input neural model start encoder classic base model feature like fig feature explanation character afﬁxe character level preﬁxe sufﬁxe target word nominalization sufﬁx character level sufﬁxe nominalization speech speech target word light verb binary feature indicate target govern light verb subject syntactic category syntactic category subject sentence morphological stem stem version target word verb root root form verb basis nominalization wordnet hypernyms hypernym set target figure feature commonly classic feature base approach event detection represent time let begin introduce basic temporal logic human language temporal logic convey temporal information straightforward theory time hold ﬂow inexorably forward event associate point val time timeline order distinct event situate timeline event precede ﬂow time lead ﬁrst chapter nformation extraction relation event time second accompany notion theory idea rent moment time combine notion idea temporal ordering relationship yield familiar notion past present future kind temporal representation system talk poral ordering relationship commonly computational eling interval algebra allen allen model event time interval algebra expression interval representation point interval short order deal interval point identiﬁes tive relation hold temporal interval fig show relation allen relation baba aab bab time overlap overlaps meet meet equal equal start starts finish finish figure temporal relation allen reichenbach reference point relation simple verb tense point time mean forward present tense refer future event example san francisco boston consider follow example flight arrive late flight arrive late refer event past represent way wrong second example unnamed event lurk background flight arrive late epresente time account phenomena reichenbach introduce notion areference point simple temporal scheme current moment time reference point equate time utterance reference point event occur reichenbach approach notion reference point separate utterance time event time follow example illustrate basic approach mary ﬂight depart eat lunch mary ﬂight depart eat lunch example eat event happen past prior utterance verb tense ﬁrst example indicate eating event begin ﬂight depart second example indicate eating accomplish prior ﬂight departure reichenbach term departure event speciﬁes reference point fact modate additional constraint relate eat anddeparture event ﬁrst example reference point precede eat event second ple eat precede reference point figure illustrate reichenbach approach primary english tense exercise ask represent example fol past perfectsimple pastpresent perfect simple futurefuture perfectpresentee eer rur eur figure reichenbach approach apply english tense diagram time ﬂow left right edenote time event rdenote reference time andudenote time utterance language way convey temporal information tense useful purpose temporal expression like morning orafterward like morning noon want train incidentally temporal expression display fascinating metaphorical conceptual organization temporal expression english frequently express spatial term illustrate use near example lakoff johnson jackendoff metaphorical organization domain systematically express term common language chapter nformation extraction relation event time represent aspect related notion time aspect way event aspect categorize internal temporal structure temporal contour mean question like event ongoing end conceptualize happen point time interval notion temporal contour divide event expression class aristotle set class introduce vendler german term aktionsart refer class aktionsart basic aspectual distinction event involve change event andstate involve change stative expression represent notion state stative event participant state have particular property give point time stative expression capture aspect world single point time conceptualize participant unchanging continuous consider follow atis example like express train need cheap fare want ﬁrst class example like event participant denote subject see experience speciﬁc point time involve kind internal change time liking need conceptualize continuous unchanging non state refer event divide subclass introduce activity expression describe event undertake activity ipant occur span time conceptualize single point time like stative expression particular end point course practice thing end meaning expression represent fact consider follow example drive mazda live brooklyn example specify subject engage engage activity speciﬁed verb period time specify driving living stop class expression achievement expression ment expression describe event place time conceptualize event have particular kind endpoint goal greek word telo mean end goal event describe kind expression call telic event telic accomplishment expression describe event natural end point andaccomplishment expression result particular state consider follow example book reservation train get new york city example event see occur period time end intended state accomplish state have reservation new york city ﬁnal aspectual class achievement expression subtly different thanachievement expression accomplishment consider emporally annotate dataset timebank find gate reach new york like accomplishment expression achievement expression result state unlike accomplishment achievement event punctual think happen instant verb conceptualize process ity lead state event example fact precede extend search ortravele event verb alize precede process conceptualize event correspond toﬁnding andreache point interval summary standard way categorize event expression temporal contours general class stative know departure gate activity john ﬂye accomplishment sally book ﬂight achievement find gate move note event expression easily shift class consider follow example ﬂew ﬂew new york ﬁrst example simple activity natural end point second ample clearly accomplishment event end point result particular state clearly classiﬁcation event solely govern verb semantic entire expression context temporally annotate dataset timebank thetimebank corpus consist american english text annotate temporal timebank information pustejovsky annotation use timeml saur markup language time base allen interval algebra discuss allen type timeml object vent represent event state ime represent time expression like date ink represent relationship event time event event event time time link include temporal link ink allen relation tual link ink aspectual relationship event subevent slink mark factuality consider follow sample sentence corresponding markup show fig select timebank document delta air line earning soar record ﬁscal ﬁrst quarter buck industry trend decline proﬁts text event temporal expression include creation time article serve document time temporal link capture allen relation soaring ﬁscal ﬁrst quarter soar soar bucking chapter nformation extraction relation event time delta air line earning event soar record fiscal quarter event industry trend event profit figure example timebank corpus decline soar visualize link graph timebank snippet represent graph like fig paciﬁc financial corp say approve royal trustco ltd toronto share million thrift hold company say regulatory approval end figure graph text adapt ocal ink show blue ink red ink green automatic temporal analysis introduce common step analyze time text extract temporal expression expression convert standard format event time extract time graph timeline extract temporal expression temporal expression phrase refer absolute point time relative time duration set absolute temporal expression absolute map directly calendar date time day relative temporal relative sion map particular time reference point week tuesday finally duration denote span time vary level duration ity second minute day week century etc figure list sample temporal expression category temporal expression grammatical construction temporal lexical trigger head make easy ﬁnd lexical trigger lexical utomatic temporal analysis absolute relative duration april yesterday hour summer semester week week yesterday day quarter quarter quarter figure example absolute relational durational temporal expression noun proper noun adjective adverb temporal expression consist phrasal projection noun phrase adjective phrase adverbial phrase figure category example noun morning noon night winter dusk dawn proper noun january monday ide easter rosh hashana ramadan tet adjective recent past annual adverb hourly daily monthly yearly figure example temporal lexical trigger task detect temporal expression run text like example show tag pustejovsky ferro fare increase initiate ual corp united airlines match competitor mark second successful fare increase rule base approach use cascade regular expression recognize large large chunk previous stage base pattern contain part speech trigger word february class month chang manning gertz chamber rule sutime chang manning detect expression like year old sequence labeling approach use standard iob scheme mark word temporal expression ofare oincrease oinitiate olast bweek iby oual ocorp statistical sequence labeler train embedding ﬁne tune encoder classic feature extract token context include word lexical trigger pos temporal expression recognizer evaluate usual recall precision andf measure major difﬁculty lexicalize approach avoid expression trigger false positive tell story winston smith classic sunday bloody sunday temporal normalization temporal normalization task map temporal expression pointtemporal normalization time duration point time correspond calendar date time day duration primarily consist length time normalize chapter nformation extraction relation event time date creation time date week match duration weekend mark second duration week figure timeml markup include normalize value temporal expression represent iso standard encode temporal value fig reproduce early example value attribute dateline document date text july iso sentation kind expression yyyy case encoding temporal expression sample text follow date show value value attribute ﬁrst temporal expression text proper refer particular week year iso standard week number ﬁrst week year ﬁrst thursday year week represent template yyyy wnn iso week document date week value week represent temporal expression weekend iso week begin monday weekend occur end week fully contain single week weekend treat duration value value attribute length duration represent accord pattern ni integer denote length xrepresent unit year day example weekend capture case sufﬁcient information anchor particular weekend particular week information encode anchor timeid attribute finally phrase week denote duration capture figure example lot temporal annotation standard consult ferro pustejovsky detail unit pattern sample value fully speciﬁed date yyyy week yyyy wnn weekend pnwe hour clock times date time yyyy ddthh financial quarter figure sample iso pattern represent time duration current approach temporal normalization rule base chang manning str gertz pattern match temporal expression associate semantic analysis procedure example pattern recognize phrase like year old associate predicate duration take argument length unit time pattern result task difﬁcult fully qualiﬁe temporal expression fairly rare real text temporal expression news article incomplete implicitly anchor respect dateline article utomatic temporal analysis document temporal anchor value temporal expression suchtemporal anchor astoday yesterday ortomorrow compute respect temporal anchor semantic procedure today simply assign anchor ment tomorrow andyesterday add day subtract day anchor respectively course give cyclic nature representation month week day time day temporal arithmetic procedure use modulo arithmetic appropriate time unit unfortunately simple expression weekend orwednesday troduce fair complexity current example weekend clearly refer weekend week immediately precede document date case illustrate following example random security check begin yesterday sky harbor continue weekend case expression weekend refer weekend week anchoring date come weekend information signal meaning come tense continue verb govern weekend relative temporal expression handle temporal arithmetic similar today andyesterday document date indicate example article iso week expression week normalize current week minus resolve ambiguous andlastexpression consider distance anchor date near unit friday refer immediately friday friday follow close document date friday likely phrase skip near ambiguity handle encode language domain speciﬁc heuristic temporal attachment temporal ordering event goal temporal analysis link time event event complete timeline ambitious task subject considerable current research solve high level accuracy capability current system somewhat simple useful task impose partial dere event temporal expression mention text ordering provide beneﬁts true timeline example tial ordering determination fare increase american airline come fare increase unite sample text determine ordering view binary relation detection classiﬁcation task partial ordering task assume addition detecting malizing time expression step describe detect event text temporal expression anchor event tione text directly temporal expression consider ing example week storm jetblue issue customer bill right determine jetblue issue customer bill right need determine time storm event need modify time temporal expression week event time detect goal assert link time event create event event event time time time dct event dct time timeml ink training time relation classiﬁer predict correct inkbetween pair time chapter nformation extraction relation event time supervise gold label timebank corpus feature like word bedding parse path tense aspect sieve base architecture rank set classiﬁer introduce chapter commonly system perform task time extraction creation normalization event extraction time event link include tarsqi verhagen clear bethard caevo chamber catena mirza tonelli template fill text contain report event possibly sequence event correspond fairly common stereotypical situation world abstract situation story relate call script schank script son consist prototypical sequence sub event participant role strong expectation provide script facilitate proper classiﬁcation entity assignment entity role relation critically drawing inference ﬁll thing leave unsaid simple form script represent template consist ﬁxed template set slot value belong particular class task oftemplate ﬁlling ﬁnd document invoke particular script ﬁll template ﬁlling slot associate template ﬁller extract text consist text segment extract directly text consist concept infer text element additional cessing ﬁlled template original airline story look like following fare raise attempt airline nited airline effective date follower merican template slot lead airline effective date low section describe standard sequence label approach ﬁlling slot section describe old system base use cascade ﬁnite state transducer design address complex task current learning base system address machine learning approach template fill standard paradigm template ﬁlling give training document text span annotate predeﬁne template slot ﬁller goal create template event input ﬁlle slot text span task generally model train separate supervise system ﬁrst system decide template present particular sentence task call template recognition confusing bit oftemplate recognition terminology event recognition template recognition treat text ﬁcation task feature extract sequence word label training document ﬁlle slot template detect emplate fill set feature token embedding word shape speech tag syntactic chunk tag name entity tag second system job extraction separate classiﬁer extraction train detect role lead binary classiﬁer run noun phrase parse input sentence sequence model run sequence word role classiﬁer train label datum training set usual set feature train individual noun phrase ﬁller single slot multiple non identical text segment label slot bel example sample text string unite orunited airlines label ead airline incompatible choice ence resolution technique introduce chapter provide path solution variety annotated collection evaluate style proach template ﬁlling include set job announcement conference call paper restaurant guide biological text key open question extract template case training datum predeﬁne template induce template set link event chamber jurafsky early finite state template fill system template relatively simple consider task produce template contain information text like grishman sundheim bridgestone sports say friday set joint venture taiwan local concern japanese trading house produce golf club ship japan joint venture bridgestone sports taiwan ize million new taiwan dollar start production january production iron metal wood club month joint venture task message understand conference series government organize information extraction evaluation produce hierarchically link template describe joint venture figure show structure produce fastus system hobbs note ﬁller activity slot tie uptemplate template slot tie relationship tie ompany bridgestone sports taiwan entities bridgestone sports roduct iron metal wood club local concern tart date january japanese trading house joint venture bridgestone sports taiwan activity figure template produce fastus give input text page early system deal complex template base cascade transducer base handwritten rule sketch fig ﬁrst stage use handwritten regular expression grammar rule basic tokenization chunking parse stage recognize entity event recognizer base ﬁnite state transducer fst insert ognize object appropriate slot template fst recognizer chapter nformation extraction relation event time step description token tokenize input stream character complex word multiword phrase number proper name basic phrase segment sentence noun verb group complex phrase identify complex noun group verb group semantic pattern identify entity event insert template merge merge reference entity event figure level processing fastus hobbs level extract speciﬁc type information pass high level hand build regular expression like following indicate noun group verb group match ﬁrst sentence news story ies venture ies result process sentence ﬁve draft template fig merge single hierarchical structure show fig merging algorithm perform coreference resolution merge tie likely describe event template slot value tie entity bridgestone local concern japanese trading house production product golf club tie joint venture bridgestone sports taiwan production company bridgestone sports taiwan start date january production product iron metal wood club figure ﬁve partial template produce stage fastus template merge stage produce ﬁnal template show fig page summary chapter explore technique extract limited form semantic tent text entity extract pattern base approach pervise learn method annotate training datum available lightly supervise bootstrapping method small number seed tuple seed pattern available distant supervision database relation available unsupervise oropen method reasoning time facilitate detection normalization temporal expression note order time sequence model classiﬁer train event label datum like timebank corpus application recognize stereotypical situation text assign element text role represent ﬁxed set slot historical note early work information extraction address task context frump system dejong later work stimulate government sponsor muc conference sundheim sundheim heim sundheim early muc system like circus system lehnert scisor jacobs rau inﬂuential inspire later system like fastus hobbs chinchor describe muc evaluation technique difﬁculty porting system domain attention shift machine learning approach early supervised learning approach cardie cardie riloff soderland huffman focus automate knowledge acquisition process mainly ﬁnite state rule base system success early success hmm base speech recognition lead use sequence labeling hmms bikel memms mccallum crf lafferty wide exploration ture zhou neural approach follow pioneer result collobert apply crf convolutional net progress area continue stimulate formal evaluation share benchmark dataset include automatic content extraction ace evaluation name entity recognition relation extraction temporal kbp knowledge base population evaluation kbp deanu relation extraction task like slot ﬁlling extract attribute slot slot ﬁlling like age birthplace spouse give entity series semeval shop hendrickx semisupervise relation extraction ﬁrst propose hearst extend system like autoslog riloff dipre brin ball agichtein gravano jones distant vision algorithm describe draw mintz ﬁrst term distant supervision suggest chris manning similar idea occur early system like craven kumlien morgan weakly label datum snow weld extension weld riedel ritter open system include know italletzioni textrunner banko everb fader riedel universal schema combine advantage distant supervision open chapter nformation extraction relation event time exercise acronym expansion process associate phrase acronym accomplish simple form relational analysis develop system base relation analysis approach describe chapter populate database acronym expansion focus english letter acronym tlas evaluate system performance compare wikipedia tla page acquire cmu seminar corpus develop system technique mention section analyze system perform compare state art result corpus useful functionality new email calendar application ability associate temporal expression connect event email doctor appointment meeting planning party invitation etc speciﬁc calendar entry collect corpus email contain temporal expression relate event planning expression compare kind expression commonly find news text discuss chapter following sentence fol translation capture temporal relationship event mary ﬂight depart eat lunch mary ﬂight depart eat role labeling seven circumstance associate hermagora aristotle sloan century bce indian grammarian write famous treatise sanskrit grammar book treatise call great monument man intelligence bloomﬁeld work scribe linguistic sanskrit language form sutra efﬁciently memorize express formal rule system brilliantly preﬁgure modern mechanism formal guage theory penn kiparsky set rule describe semantic relationship verb noun argument role like agent instrument destination work early know model linguistic realization event participant task understand participant relate event able answer question central question natural language processing let forward millennia present consider mundane goal understand text purchase stock xyz corporation purchasing event participant describe wide variety surface form event describe verb sell buy noun purchase xyz corp syntactic subject buy indirect object sell genitive noun compound relation noun purchase despite have notionally role xyz corporation buy stock sell stock xyz corporation stock buy xyz corporation purchase stock xyz corporation stock purchase xyz corporation chapter introduce level representation capture ality sentence purchase event participant xyz corp stock xyz corp buyer shallow semantic representation semantic role express role argument predicate event codiﬁe database like propbank framenet introduce semantic role labeling task assign role span sentence tional restriction preference predicate express argument fact theme eatis generally edible show birch bark manuscript kashmir rupavatra grammatical textbook base sanskrit grammar panini image wellcome chapter emantic role label semantic role consider meaning argument sasha pat window door sentence sasha break window pat open door subject sasha andpat breaker break event opener door opening event mon volitional actor animate direct causal responsibility event thematic role way capture semantic commonality thematic role ersandopener subject verb agent agent agent thematic role represent abstract idea volitional tion similarly direct object verb brokenthing andopenedthing prototypically inanimate object affect way action semantic role participant theme theme thematic role deﬁnition agent volitional causer event experiencer experiencer event force non volitional causer event theme participant directly affect event result end product event content proposition content propositional event instrument instrument event beneficiary beneﬁciary event source origin object transfer event goal destination object transfer event figure commonly thematic role deﬁnition thematic role old linguistic model see modern formulation fillmore gruber universally agree set role fig list matic role computational paper rough deﬁnition example thematic role set dozen role set small number role abstract meaning set large number role speciﬁc situation use general term semantic role set role small large semantic role diathesis alternation main reason computational system use semantic role act shallow meaning representation let simple inference possible pure surface string word parse tree extend early example document say company acquire company like know answer query company acquire despite fact sentence different surface syntax similarly shallow semantic act useful intermediate language machine iathesis alternation thematic role example agent waiter spill soup experiencer john headache force wind blow debris mall yard theme benjamin franklin break ice result city build regulation size baseball diamond content mona ask meet mary ann supermarket instrument poach catﬁsh stun shocking device beneficiary ann callahan make hotel reservation boss source ﬂew boston goal drive portland figure prototypical example thematic role semantic role help generalize different surface realization icate argument example agent realize subject sentence case theme subject consider possible realization thematic argument verb break john agentbroke window theme john agentbroke window themewith rock instrument rock instrumentbroke window theme window themebroke window themewas break john agent example suggest break possible argument agent theme instrument set thematic role argument take verb call thematic grid grid case frame thematic grid case frame follow possibility realization argument break agent theme agent theme instrument instrument theme theme turn verb allow thematic role realize syntactic position example verb like realize theme goal argument different way doris agentgave book themeto cary goal agentgave cary goalthe book theme multiple argument structure realization fact break agent instrument theme subject realize theme goal order call verb alternation ordiathesis alternation alternationverb alternation show dative alternation occur particular dative alternation mantic class verb include verb future have advance allocate chapter emantic role labeling owe send verbs forward hand mail verb throwing kick pass throw levin list english verb semantic class belong high level class divide speciﬁc class alternation participate list verb class incorporate online resource verbnet kipper link verb wordnet framenet entry semantic role problem thematic role represent meaning thematic role level like useful deal complication like diathesis alternation prove cult come standard set role equally difﬁcult produce formal deﬁnition role like agent theme instrument example researcher attempt deﬁne role set ﬁnd need fragment role like agent ortheme speciﬁc role levin port hovav summarize number case fact kind instrument intermediary instrument appear subject enable instrument shelly cut banana knife knife cut banana shelly eat slice banana fork fork eat sliced banana addition fragmentation problem case like reason generalize semantic role ﬁnite discrete list role let finally prove difﬁcult formally deﬁne thematic role consider agent role case agent animate volitional sentient causal individual noun phrase exhibit property problem lead alternative semantic role model use semantic role few role ﬁrst option deﬁne generalize semantic role abstract speciﬁc thematic role example proto proto proto agent proto patient generalize role express roughly agent like roughly patient like ing role deﬁne necessary sufﬁcient condition set heuristic feature accompany agent like patient like meaning argument display agent like property tionally involve event cause event change state ticipant sentient intentionally involve move great likelihood argument label proto patient like tie undergo change state causally affect participant stationary relative participant etc great likelihood argument label proto second direction instead deﬁne semantic role speciﬁc particular verb particular group semantically relate verb noun section describe commonly lexical resource use alternative version semantic role propbank use role verb speciﬁc semantic role framenet use semantic role ciﬁc general semantic idea call frame heproposition bank proposition bank proposition bank generally refer propbank resource propbank tence annotate semantic role english propbank label sentence penn treebank chinese propbank label sentence penn chinese treebank difﬁculty deﬁne universal set thematic role semantic role propbank deﬁne respect individual verb sense sense verb speciﬁc set role give number name general represent proto proto semantic role consistent deﬁne speciﬁcally verb nonetheless generalization benefactive instrument attribute end state start point benefactive instrument attribute end point slightly simpliﬁed propbank entry sense verbs agree andfall propbank entry call frame ﬁle note deﬁnition frame ﬁle role entity agree extent fall informal gloss intend read human formal deﬁnition agreer proposition entity agree group agree offer argm tmp usually john agree mary logical subject patient thing fall extent fall start point end point end state sale million million average junk bond note role fall normal subject falli proto propbank semantic role useful recover shallow semantic formation verbal argument consider verb increase incrementally causer increase thing increase increase ext mnr start point end point propbank semantic role labeling allow infer commonality event structure follow example case big fruit agent andthe price banana theme despite differ surface chapter emantic role labeling big fruit increase price banana price banana increase big fruit price banana increase propbank number non numbered argument call argm tmp argm loc etc represent modiﬁcation adjunct meaning relatively stable predicate list frame ﬁle datum label modiﬁer helpful training system detect temporal location directional modiﬁcation predicate argm include tmp yesterday evening loc museum san francisco dir bangkok mnr clearly enthusiasm prp cau response ruling rec adv miscellaneous prd secondary predication eat meat raw propbank focus verbs related project nombank meyer nombank add annotation noun predicate example noun agreement apple agreement ibm label apple ibm allow semantic role labeler assign label argument verbal nominal predicate framenet make inference semantic commonality different tence increase useful useful inference situation different verb verb noun example like extract similarity tence price banana increase price banana rise rise price banana note second example use different verb rise example use noun verb rise like system recognize price banana go go matter object verb increase nominal modiﬁer noun rise framenet project semantic role labeling project attempt framenet address kind problem baker fillmore fillmore baker ruppenhofer role propbank project speciﬁc individual verb role framenet project speciﬁc aframe frame consider following set word reservation ﬂight travel buy price cost fare rate meal plane individual lexical relation hyponymy synonymy word list result set relation rame net add complete account word relate clearly deﬁne respect coherent chunk common sense background information concern air travel holistic background knowledge unite word frame frame idea group word deﬁne respect ground information widespread artiﬁcial intelligence cognitive science frame related work like model johnson laird model script schank abelson script frame framenet background knowledge structure deﬁne set frame speciﬁc semantic role call frame element include set frame element cat use role word evoke frame proﬁle aspect frame element framenet dataset include set frame frame element lexical unit associate frame set label ple sentence example change position onascale frame deﬁne follow frame consist word indicate change item tion scale attribute starting point initial value end point final value semantic role frame element frame deﬁne fig note separate core role frame speciﬁc core role non core role like arg argument propbank express non core role general property time location core role attribute ttribute scalar property tem possess difference distance tem change position scale final state description present tem state change ttribute value independent predication final value position scale tem end initial state description present tem state change tribute value independent predication initial value initial position scale tem move away item entity position scale value range portion scale typically identiﬁe end point value ttribute ﬂuctuate non core role duration length time change take place speed rate change alue group roup tem change value attribute speciﬁed way figure frame element change position onascale frame framenet labeler guide ruppenhofer example sentence item attribute price difference item increase final state have day month item microsoft share value item colon cancer incidence group chapter emantic role labeling steady increase initial value final value item dividend difference item dividend increase note example sentence frame include target word like rise fall increase fact complete frame consist follow word verbs dwindle soar escalation shift advance edge mushroom swell explosion tumble climb explode plummet swing fall decline fall reach triple ﬂuctuation adverb decrease ﬂuctuate rise tumble gain increasingly diminish gain rocket growth dip grow shift noun hike double increase skyrocket decline increase drop jump slide decrease rise framenet code relationship frame allow frame inherit represent relation frame like causation eralization frame element different frame represent tance cause change ofposition onascale frame link change ofposition onascale frame cause relation add gent role causative example following agent raise item price soda difference frame allow understanding system extract common event semantic verbal nominal causative non causative usage framenet develop language include ish german japanese portuguese italian chinese semantic role label semantic role labeling shorten srl task automaticallysemantic role labeling ﬁnde semantic role argument predicate sentence rent approach semantic role labeling base supervise machine learning framenet propbank resource specify count icate deﬁne set role task provide training test set recall difference model semantic role framenet employ frame speciﬁc frame element role bank use small number numbered argument label prete verb speciﬁc label general argm label example blame program unable identify cognizer target evaluee reason san francisco examiner issue special edition yesterday target arg argm feature base algorithm semantic role label simpliﬁed feature base semantic role label algorithm sketch fig feature base algorithm early system like simmon begin parse broad coverage parser assign parse input emantic role label figure show parse parse traverse ﬁnd word predicate predicate algorithm examine node parse tree use supervised classiﬁcation decide semantic role play predicate give label training set propbank framenet feature vector extract node feature template describe subsection classiﬁer train predict semantic role constituent give feature number potential semantic role plus extra role non role constituent standard classiﬁcation algorithm finally test sentence label classiﬁer run relevant constituent function semantic rolelabel word return label tree parse parse word predicate inparse node inparse featurevector extract feature node predicate parse classify featurevector parse figure generic semantic role labeling algorithm lassify node siﬁer assign semantic role non role constituent train label datum framenet propbank sbj nnp nnp nnp san francisco examiner vbd target tmp issue noon yesterday figure parse tree propbank sentence show propbank argument label dotted line show path feature sbj constituent san francisco examiner instead train single stage classiﬁer fig node level ﬁcation task break multiple step small number constituent sentence argument give predicate system use simple heuristic prune unlikely constituent binary classiﬁcation node argument bele nclassiﬁcation constituent label argument previous chapter emantic role label separation identiﬁcation classiﬁcation lead well use ture different feature useful task computational ciency global optimization classiﬁcation algorithm fig classiﬁes argument separately cally make simplifying assumption argument predicate label independently assumption false interaction ment require global assignment label constituent example constituent framenet propbank require non overlapping signiﬁcantly semantic role constituent independent example propbank allow multiple identical argument constituent verb label role label system add fourth step deal global consistency label sentence example local classiﬁer return list possible label associate probability constituent second pass viterbi decode ranking approach choose good consensus label integer linear programming ilp common way choose solution conform well multiple constraint feature semantic role label system use generalization core set feature introduce gildea jurafsky common basic feature template demonstrate thenp sbj constituent san francisco examiner fig include governing predicate case verb issue predicate cial feature label deﬁne respect particular predicate phrase type constituent case sbj mantic role tend appear nps sorpp headword constituent examiner headword constituent compute standard head rule give appendix fig certain headword pronouns place strong constraint possible semantic role likely ﬁll headword speech constituent nnp path parse tree constituent predicate path mark dot line fig follow gildea jurafsky use simple linear representation path represent upward downward movement tree respectively path useful compact representation kind grammatical function relationship constituent predicate voice clause constituent appear case active contrast passive passive sentence tend strongly different linking semantic role surface form active one binary linear position constituent respect predicate orafter subcategorization predicate set expect argument appear verb phrase extract information structure rule expand immediate parent predicate vbd predicate fig name entity type emantic role label ﬁrst word word constituent follow feature vector represent ﬁrst example recall observation value example constituent parse tree bear semantic role issue examiner nnp active org examiner feature addition set gram inside constituent complex version path feature upward downward half particular node occur path possible use dependency parse instead constituency parse basis feature example dependency parse path instead constituency path neural algorithm semantic role label simple neural approach srl treat sequence label task like entity recognition bio approach let assume give predicate task detect labeling span recall bio tagging begin end tag possible role plus outside tag predconcatenate predicateb figure simple neural approach semantic role labeling input sentence follow sep extra input predicate case love encoder output concatenate indicator variable predicate word shi lin tagger goal compute high probability tag quence give input sequence word argmax fig show sketch standard algorithm input word map pretraine embedding token concatenate predicate embed pass feedforward network softmax output distribution srl label decode crf layer instead mlp layer bilstm output global inference practice provide chapter emantic role label evaluation semantic role label standard evaluation semantic role labeling require argument label assign exactly correct word sequence parse constituent compute precision recall measure identiﬁcation classiﬁcation evaluate separately common dataset evaluation carrera arquez pradhan selectional restriction turn section way represent fact relationship tween predicate argument selectional restriction semantic type con selectional restriction straint verb impose kind concept allow ﬁll argument role consider meaning associate follow example want eat someplace nearby possible parse semantic interpretation sentence sensible interpretation eatis intransitive phrase someplace nearby adjunct give location eat event nonsensical speaker godzilla interpretation eatis transitive phrase someplace nearby direct object theme eating like malaysian food following sentence want eat malaysian food know someplace nearby direct object sentence useful cue semantic fact theme ate event tend edible restriction place verb eaton ﬁller theme argument selectional restriction selectional restriction associate sense entire lexeme following example lexeme serve restaurant serve green lipped mussel airline serve denver example illustrate offering food sense serve ordinarily strict theme kind food example illustrate provide commercial service sense serve constrain theme type appropriate location selectional restriction vary widely speciﬁcity verb imagine example impose strict requirement agent role restrict human animate entity place semantic requirement theme role verb like diagonalize hand place speciﬁc constraint ﬁller theme role matrix argument adjective odorless restrict concept possess odor rehearsal ask musician imagine tennis game radon odorless gas detect human sense diagonalize matrix ﬁnd eigenvalue example illustrate set concept need represent selectional restriction matrix able possess odor etc open end distinguish selectional restriction feature represent lexical knowledge like part speech limited electional restriction represent selectional restriction way capture semantic selectional restriction use extend event representation appendix recall neo davidsonian representation event consist single variable stand event predicate denote kind event variable relation event role ignore issue thel structure thematic role deep event role semantic contribution verb like eatmight look like follow eating heme representation know ﬁller theme role associate eat event theme relation stipulate selectional restriction ymust edible simply add new term effect eating heme hing phrase like eat hamburger encounter semantic analyzer form following kind representation eating heme hing representation perfectly reasonable membership yin category hamburger consistent membership category ediblethe assume reasonable set fact knowledge base correspondingly representation phrase eat takeoff ill form membership event like category takeoff inconsistent membership category ediblethe approach adequately capture semantic selectional restriction problem direct use fol perform simple task enforce selectional restriction overkill far simple formalism job far computational cost second problem approach presuppose large logical knowledge base fact concept selectional restriction unfortunately common sense knowledge basis develop currently kind coverage necessary task practical approach state selectional restriction term wordnet synset logical concept predicate simply speciﬁes wordnet synset selectional restriction argument meaning tion form role ﬁller word hyponym subordinate synset ate hamburger example instance set selectional restriction theme role verb eatto synset ffood nutrient glossed asany substance metabolize animal energy build tissue luckily chain hypernyms hamburger show fig reveal hamburger food ﬁller role need match restriction synset exactly need synset superordinate apply approach theme role verbs imagine lift agonalize discuss early let restrict imagine stheme synsetfentityg tofphysical entityg diagonalize tofmatrixg arrangement correctly permit imagine hamburger andlift hamburger correctly rule diagonalize hamburger chapter emantic role labeling sense hamburger beefburger fried cake mince beef serve bun sandwich snack food dish nutriment nourishment nutrition food nutrient substance matter physical entity entity figure evidence wordnet hamburger edible selectional preference early implementation selectional restriction consider strict straint kind argument predicate katz fodor hirst example verb eatmight require theme argument food early word sense disambiguation system idea rule sense violate selectional restriction governing predicate quickly clear selectional restriction well represent preference strict constraint wilk wilk example selectional restriction violation like inedible argument eat occur form sentence example negate selectional restriction overstate fall apart people realize eat gold lunch hungry championship trial kulkarni ateglass stomach accompany water tea modern system selectional preference specify relation tween predicate possible argument soft constraint kind selectional association inﬂuential selectional association model resnik resnik deﬁne idea selectional preference strength generalselectional preference strengthamount information predicate tell semantic class ment example verb eattell lot semantic class direct object tend edible verb contrast tell direct object selectional preference strength deﬁne ence information distribution distribution expected semantic class likely direct object fall class tribution expected semantic class particular verb likely direct object speciﬁc verb vwill fall semantic class great difference distribution information verb give possible object difference distribution quantiﬁe relative entropy kullback leibler divergence kullback relative entropy leibler kullback leibler divergence electional restriction difference probability distribution pandq selectional preference divergence express formation bit verb vexpresse possible semantic class ment resnik deﬁne selectional association particular class verb theselectional association relative contribution class general selectional preference verb selectional association probabilistic measure strength ciation predicate class dominate argument predicate resnik estimate probability association parse corpus e time predicate occur argument word assume word partial observation wordnet concept contain word follow table resnik show sample high low selectional association verb wordnet semantic class direct object direct object direct object verb semantic class assoc semantic class assoc read write activity write write commerce entity method selectional preference conditional probability alternative selectional association verb wordnet class argument use conditional probability argument word give predicate verb directly model strength association verb predicate noun argument conditional probability model compute parse large pus billion word compute occurrence count give verb occur give noun give relation conditional probability argument noun give verb particular relation selectional preference metric pair word brockmann lapata keller lapata inverse probability find well performance case brockmann lapata chapter emantic role label simple approach use simple log occurrence frequency predicate argument log count conditional probability well extract preference syntactic subject object brockmann lapata evaluate selectional preference way evaluate model selectional preference use pseudoword gale pseudoword sch pseudoword artiﬁcial word create nate test word context banana confounder word door create banana door task system identify word original word evaluate selectional preference model example relationship verb direct object test corpus select verb tokens verb token drive select direct object car concatenate confounder word near neighbor noun frequency close original house car house use selectional preference model choose carandhouse preferred object drive compute model choose correct original ject car chamber jurafsky evaluation metric human preference test set argument pair rate degree plausibility usually magnitude estimation technique psychophysic subject rate plausibility argument proportional modulus item selectional preference model evaluate correlation human ence keller lapata primitive decomposition predicate way think semantic role discuss chapter help deﬁne role argument play decompositional way base ﬁnite list thematic role agent patient instrument proto agent patient etc idea decompose meaning set primitive semantic element feature call primitive decomposition orcomponential analysis componential analysis take focus particularly predicate consider example verb kill jim kill philodendron jim cause philodendron alive truth conditional propositional semantic perspective sentence meaning assume equivalence send meaning killa kill semantic primitive like cause alive set potential semantic primitive account verbal alternation discuss section lakoff dowty consider follow example john open door cause john open door door open open ummary door open open door decompositional approach assert single state like predicate ate open underlie example difference meaning example arise combination single predicate itive cause approach primitive decomposition explain similarity tween state action causative non causative predicate rely have large number predicate like open radical approach choose break predicate approach verbal predicate position play role early natural language system conceptual dency set primitive predicate show fig dependency primitive deﬁnition atran abstract transfer possession control entity ptran physical transfer object location mtran transfer mental concept entity entity mbuild creation new information entity propel application physical force object integral movement body animal ingest taking substance animal expel expulsion animal speak action produce sound attend action focus sense organ figure set conceptual dependency primitive example sentence cdrepresentation verb bring translate primitive atrans ptran indicate waiter physically convey check mary pass control note cdalso associate ﬁxed set thematic role primitive represent participant action waiter bring mary check atrans ject ject summary role abstract model role argument play event describe predicate role model semantic role base single ﬁnite list role semantic role model include verb semantic role list proto agent patient implement propbank frame role list implement framenet chapter emantic role label role labeling task assign semantic role label constituent sentence task generally treat supervise chine learning task model train propbank framenet rithm generally start parse sentence automatically tag parse tree node semantic role neural model map straight word end end semantic selectional restriction allow word particularly predicate post constraint semantic property argument word selectional preference model like selectional association simple conditional bility allow weight probability assign association predicate argument word class historical note idea semantic role date introduce modern linguistic gruber fillmore fillmore interested argument structure study lucien tesni ere groundbreake syntaxe structurale tesni ere term dependency introduce foundation lay dependency mar follow tesni ere terminology fillmore ﬁrst refer argument role actant fillmore quickly switch term case fillmore propose universal list semantic role case agent patient instrument etc take argument predicate verbs list lexicon case frame list obligatory optional case argument idea semantic role provide intermediate level semantic representation help map syntactic parse structure deep fully speciﬁed representation meaning quickly adopt natural language processing system extract case frame create machine tion wilk question answer hendrix speak language cessing nash webber dialogue system bobrow purpose semantic role labeler develop early one simmon ﬁrst parse sentence mean atn augment transition network parser verb set rule specify parse map mantic role rule mainly reference grammatical function subject object complement speciﬁc preposition check constituent internal feature animacy head noun later system assign role build parse tree dictionary verb speciﬁc case frame levin marcus case representation widely teach nlp course describe standard natural language processing ﬁrst edition winston textbook artiﬁcial intelligence fillmore propose model frame semantic later describe intuition follow idea frame semantic speaker aware bly complex situation type package connected expectation name frame schemas scenario script cultural narrative meme word language understand frame presuppose background fillmore note word frame air suite relate notion propose time minsky hyme goffman relate notion name like script schank abelson andschemata bobrow norman tannen comparison fillmore inﬂuence semantic ﬁeld theorist visit yale lab take notice list slot ﬁller early information extraction system like dejong schank abelson fillmore draw insight begin framenet corpus annotation project time beth levin draw early case frame dictionary levin develop book summarize set verb class deﬁne share argument realization levin verbnet project build work kipper lead soon propbank semantic role label corpus create martha palmer colleague palmer combination rich linguistic annotation corpus base approach stantiate framenet propbank lead revival automatic approach semantic role labeling ﬁrst framenet gildea jurafsky propbank datum gildea palmer inter alia problem ﬁrst address handwritten rule generally recast supervised machine learning enable large consistent database popular feature role labeling deﬁne gildea jurafsky surdeanu xue palmer pradhan che zhao use dependency constituency parse introduce share task surdeanu survey palmer arquez use neural approach semantic role labeling pioneer lobert apply crf convolutional net early work like foland martin focus dependency feature later work eschew syntactic feature altogether zhou introduce use stacked layer bilstm architecture show augment bilstm architecture highway network replace crf decode possible apply wide variety global constraint srl decoding semantic role label scheme work single sentence cusing object verbal nominal case nombank predicate case verbal nominal predicate implicit ment appear contextual sentence mustimplicit argument infer sentence house new owner sale ﬁnalize day ago thesale second sentence reasonable reader infer house mention prior sentence e argument implicit argument detection shorten isrl isrl introduce gerber chai ruppenhofer recent neural model avoid need huge label training set unsupervised approach semantic role labeling attempt induce set semantic role cluster argument task pioneer riloff schmelzenbach swier stevenson grenager manning titov klementiev lang lapata woodsend lapata titov dam recent innovation frame labeling include connotation frame mark rich information argument predicate connotation frames mark chapter emantic role labeling sentiment writer reader argument example verb survive inhe survive bombing express writer sympathy subject heand negative sentiment bombing chapter detail selectional preference widely study selectional tion model resnik resnik method include cluster rooth discriminative learning bergsma topic el eaghdha ritter constraint express level word class agirre martinez selectional preference successfully integrate semantic role labeling erk zapirain exerciseschapter sentiment affect connotation day able measure power word maya angelou chapter turn tool interpret affective meaning extend affective study sentiment analysis appendix use word affective follow tradition affective computing picard mean emotion sentiment sonality mood attitude affective meaning closely relate subjectivity subjectivity study speaker writer evaluation opinion emotion speculation wiebe affective meaning deﬁne inﬂuential typology tive state come scherer deﬁne class affective state factor like cognitive realization time course fig emotion relatively brief episode response evaluation external internal event major signiﬁcance angry sad joyful fearful ashamed proud elated desperate mood diffuse affect state pronounce change subjective feeling low intensity relatively long duration apparent cause cheerful gloomy irritable listless depressed buoyant interpersonal stance affective stance take person ciﬁc interaction color interpersonal exchange situation distant cold warm supportive contemptuous friendly attitude relatively enduring affectively colored belief preference disposition object person like love hating valuing desire personality trait emotionally laden stable personality disposition havior tendency typical person nervous anxious reckless morose hostile jealous figure scherer typology affective state scherer design extractor kind affective state appendix introduce sentiment analysis task extract positive negative orientation writer express text correspond scherer typology extraction attitude ﬁgure people like dislike rich text like consumer review book movie newspaper editorial public sentiment blog tweet detect emotion andmood useful detect student fuse engage certain interact tutorial system caller help line frustrate blog post tweet indicate sion detect emotion like fear novel example help trace group situation fear change chapter exicon sentiment affect connotation detect different interpersonal stance useful extract mation human human conversation goal detect stance like friendliness awkwardness interview friendly conversation example summarize meeting ﬁnding part conversation people especially excited engaged conversational hot spot help meeting tion detect personality user user extrovert extent open experience help improve tional agent work well match user personality tion mairesse walker affect important generation recognition synthesize affect important conversational agent domain include literacy tutor child storybook computer game appendix introduce use naive baye classiﬁcation classify document sentiment classiﬁer successfully apply task word training set input classiﬁer determine affect status text chapter focus alternative model instead word feature focus certain word one carry particularly strong cue affect sentiment list word affective lexicon ment lexicon lexicon presuppose fact semantic word affective meaning orconnotation word connotation different meaning connotation different ﬁeld use mean aspect word meaning relate writer reader emotion sentiment opinion evaluation addition ability help determine affective status text connotation lexicon useful feature kind affective task tional social science analysis section introduce basic theory emotion sentiment lexicon special case emotion lexicon mention useful lexicon survey way building lexicon human labeling semi supervised supervise finally talk detect affect particular entity introduce connotation frame deﬁne emotion important affective class emotion scherer deﬁne emotion relatively brief episode response evaluation external internal event major signiﬁcance detect emotion potential improve number language processing task emotion recognition help dialogue system like tutoring system detect student unhappy bored hesitant conﬁdent automatically detect emotion review customer response anger dissatisfaction trust help business recognize speciﬁc problem area one go emotion play role medical nlp task like help diagnose depression suicidal intent detect emotion express character novel play role understand different social group view society different time computational model emotion nlp mainly base lie theory emotion study ﬁeld affective science family emotion view ﬁxed atomic unit limit ber generate call basic emotion tomkin basic efine emotion plutchik model date darwin know family theory emotion propose ekman ekman universally present culture surprise happiness anger fear disgust sadness atomic theory plutchik wheel emotion consist basic emotion oppose pair joy sadness anger fear trust disgust andanticipation surprise emotion derive show fig figure plutchik wheel emotion second class emotion theory widely nlp view emotion space dimension russell model include dimension valence andarousal add dominance deﬁne valence pleasantness stimulus arousal level alertness activeness energy provoke stimulus dominance degree control dominance exert stimulus emotion sentiment view special case second view emotion point space particular valence dimension measure pleasant unpleasant word directly measure sentiment lexicon base model affect affective meaning word erally ﬁxed irrespective linguistic context word dialect culture speaker contrast model affective science send emotion rich process involve cognition barrett appraisal theory example emotion complex process person consider event congruent goal take account variable like agency certainty urgency novelty control associate event moor computational model nlp take account rich theory emotion likely play important role future chapter exicon sentiment affect connotation available sentiment affect lexicon wide variety affect lexicon create release basic lexicon label word dimension semantic variability generally call sentiment valence simple lexicon dimension represent binary fashion wordlist positive word wordlist negative word old general inquirer stone draw content analysis earlygeneral inquirer work cognitive psychology word mean osgood eral inquirer lexicon positive word lexicon negative word lexicon discuss mpqa subjectivity lexicon wilson positive negative word draw prior lexicon plus bootstrapped list subjective word phrase riloff wiebe entry lexicon hand label sentiment label reliability strongly subjective weakly subjective polarity lexicon liu give positive negative word draw product review label bootstrapping method wordnet positive admire amazing assure celebration charm eager enthusiastic excellent fancy tastic frolic graceful happy joy luck majesty mercy nice patience perfect proud rejoice relief respect satisfactorily sensational super terriﬁc thank vivid wise derful z negative abominable anger anxious bad catastrophe cheap complaint condescending deceit defective disappointment embarrass fake fear ﬁlthy fool guilt hate idiot inﬂict lazy miserable mourn nervous objection pest plot reject scream silly terrible unfriendly vile wicked figure word consistent sentiment general inquirer stone mpqa subjectivity lexicon wilson polarity lexicon liu slightly general sentiment lexicon lexicon assign word value affective dimension nrc valence arousal dominance lexicon mohammad assign valence arousal inance score word example show fig valence arousal dominance vacation enrage powerful delightful party authority whistle organize saxophone consolation effortless discourage torture nap weak figure value sample word emotional dimension mohammad nrc word emotion association lexicon call emolex emolex mad turney use plutchik basic emotion deﬁne lexicon include word include word prior lexicon frequent noun verb adverb adjective value lexicon sample reate affect lexicon human labeling word anger anticipation disgust fear joy sadness surprise trust positive negative reward worry tenderness sweetheart suddenly thirst garbage small set word nrc emotion affect intensity lexicon mohammad contain real value score association anger fear joy sadness fig show example anger fear joy sadness outrage horror superb sad violence anguish cheer guilt coup pestilence rainbow unkind oust stress gesture difﬁcultie suspicious fail warm beggar nurture conﬁdent hardship sing figure sample emotional intensity word anger fear joy sadness mohammad liwc linguistic inquiry word count widely set liwc icon contain word pennebaker design capture aspect lexical meaning relevant social psychological task addition sentiment relate lexicon like one negative emotion bad weird hate lem tough positive emotion love nice sweet liwc include lexicon category like anger sadness cognitive mechanism perception tentative hibition show fig hand build affective lexicon general inquirer clude additional lexicon dimension like strong weak active passive overstate understated lexicon category like pleasure pain virtue vice motivation cognitive orientation useful feature task distinction concrete concrete word like banana orbathrobe andabstract word like belief andalthough abstract lexicon brysbaert crowdsource assign rating concreteness word assign banana bathrobe bagel belief word like brisk create affect lexicon human labeling early method build affect lexicon common use human label word commonly crowdsourcing crowdsourcing break task small piece distribute large number chapter exicon sentiment affect connotation positive negative emotion emotion insight inhibition family negate appreciat anger aware avoid brother comfort bore believe careful cousin great cry decid hesitat daughter happy despair feel limit family interest fail ﬁgur oppos father joy fear know prevent grandf perfect griev know reluctan grandm nobod hate mean safe husband safe panic notice stop mom terriﬁc suffer recogni stubborn mother value terrify sense wait niece wow violent think wary wife figure sample lexical category liwc pennebaker mean previous letter word preﬁx word preﬁx include category tator let look methodological choice crowdsourced emotion lexicon nrc emotion lexicon emolex mohammad turney label emotion step ensure annotator judge correct sense word ﬁrst answer multiple choice synonym question prime correct sense word require annotator read potentially confusing sense deﬁnition create automatically headword associate thesaurus category sense question macquarie dictionary headword random distractor category example word close meaning related startle word startle annotator ask rate associate word emotion joy fear anger etc association rate scale weakly moderately strongly associate outli rating remove term assign class choose jority annotator tie break choose strong intensity level map binary label word weak map moderate strong map nrc lexicon mohammad build select word emoticon prior lexicon annotate crowd source bad scaling louviere kiritchenko mohammad well well bad scaling bad scaling annotator give item usually ask item thebest high bad low term property set word describe end scale take prior literature valence example rater ask word associate ness pleasure positiveness satisfaction contentedness hopefulness unhappiness annoyance negativeness dissatisfaction emi supervise induction affect lexicon melancholy despair word list option word associate piness pleasure positiveness satisfaction contentedness ness unhappiness annoyance negativeness dissatisfaction melancholy despair word list option score word lexicon proportion time item choose good high minus proportion time item choose bad low agreement annotation evaluate half reliability split corpus half compute correlation thesplit half reliability annotation half semi supervised induction affect lexicon common way learn sentiment lexicon start set seed word deﬁne pole semantic axis word like good orbad ﬁnd way label word wby similarity seed set summarize family seed base semi supervised lexicon induction algorithm axis base graph base semantic axis method know lexicon induction method turney littman algorithm give seed word like good orbad word wto label measure similar good different bad describe slight extension algorithm base compute semantic axis ﬁrst step choose seed word hand method deal fact affect word different different contexts start single large seed lexicon rely induction algorithm ﬁnetune domain choose different seed word different genre hellrich suggest modeling affect different historical time period start large modern affect dictionary well small seedset tune stable time example second approach hamilton deﬁne set seed word general sentiment analysis different set twitter set sentiment ﬁnancial text domain positive seed negative seed general good lovely excellent fortunate ant delightful perfect loved love happybad horrible poor unfortunate pleasant disgusting evil hate hate unhappy twitter love love love awesome nice amazing good fantastic correct happyhate hate hate terrible nasty awful bad horrible wrong sad finance successful excellent proﬁt beneﬁcial improve improve success gain positivenegligent loss volatile wrong loss damage bad litigation failure negative second step compute embedding pole word embedding shelf embedding compute chapter exicon sentiment affect connotation speciﬁc corpus example ﬁnancial corpus ﬁnance lexicon goal ﬁnetune shelf embedding corpus fine tuning cially important speciﬁc genre text datum train good embedding ﬁnetune begin shelf embedding like continue train small target corpus embedding pole word create embed represent pole take centroid embedding seed word recall centroid multidimensional version mean give set embedding positive seed word embedding negative seed word pole centroid nnx mmx semantic axis deﬁne pole compute subtract tor vaxis vaxis semantic axis vector direction positive sentiment finally compute cosine similarity angle vector direction positive sentiment direction embed high cosine mean wis align dictionary word sentiment score sufﬁcient need group word positive negative lexicon use threshold method discrete lexicon label propagation alternative family method deﬁne lexicon propagate sentiment label graph idea suggest early work hatzivassiloglou mckeown describe simple sentprop sentiment propagation algorithm hamilton step graph give word embedding build weighted lexical graph connect word knear neighbor accord cosine larity weight edge word wiandwjare set seed set choose positive negative seed word polarity seed set perform random walk graph start seed set random walk start node emi supervise induction affect lexicon choose node probability proportional edge ability word polarity score seed set proportional probability random walk seed set landing word fig word score walk positive negative seed set result positive negative raw label score combine value positive polarity score helpful standardize score zero mean unit variance corpus conﬁdence score sentiment score inﬂuence seed set like know score word change different seed set use bootstrap sampling conﬁdence region compute propagation btime random subset positive negative seed set example choose seed word time standard deviation bootstrap sample polarity score give conﬁdence measure idolize love adore appreciate like ﬁnd dislike notice disapprove abhor hate loathe despise uncover idolize love adore appreciate like ﬁnd dislike notice disapprove abhor hate loathe despise uncover figure intuition entprop algorithm run random walk seed word assign polarity score show color green red base frequency random walk visit method core semisupervised algorithm metric measure similarity seed word turney littman hamilton proache embed cosine distance metric word label positive basically embedding high cosine positive seed low cosine negative seed method choose kind distance metric embed cosine example hatzivassiloglou mckeown algorithm use syntactic cue adjective consider similar frequently conjoin rarely conjoin base intuition adjective conjoin word andtend polarity positive adjective generally coordinate positive negative negative fair legitimate corrupt brutal positive adjective coordinate negative fair brutal corrupt legitimate contrast adjective conjoin butare likely opposite chapter exicon sentiment affect connotation fair brutal cue opposite polarity come morphological negation adjective root differ morphological negative equate inadequate thoughtful thoughtless tend opposite polarity method ﬁnding word similar polarity seed word use thesaurus like wordnet kim hovy liu word synonym presumably share polarity word antonym probably opposite polarity seed lexicon build lexicon update follow possibly iterate add synonyms positive word antonyms like ﬁne negative word add synonyms negative word awful antonyms like evil positive word extension algorithm assign polarity wordnet sense call wordnet baccianella fig show example sentiwordnet synset pos neg obj agreeable pleasing deserving esteem compute estimate cause sharp stinging pain critical importance consequence angle degree have experience rapid onset short severe course figure example sentiwordnet baccianella note difference sense homonymous word purely objective positive acute positive negative neutral acute algorithm polarity assign entire synset word positive lexicon build synset associate positive word negative lexicon synset associate negative word classiﬁer train datum wordnet gloss decide sense deﬁne positive negative neutral step involve random walk algorithm assign score wordnet synset degree positivity negativity neutrality summary semisupervise algorithm use human deﬁned set seed word pole dimension use similarity metric like embed cosine coordination morphology thesaurus structure score word similar positive seed dissimilar negative seed supervised learning word sentiment semi supervised method require minimal human supervision form seed set supervision signal exist world use signal score associate online review web contain enormous number online review restaurant movie book product text review upervised learning word sentiment movie review excerpt imdb great movie ﬁlm wonderful experience surreal zany witty slapstick time terriﬁc performance probably bad movie see story go interesting stuff restaurant review excerpt yelp service impeccable food cook season perfectly watermelon perfectly square grill octopus mouthwatering take water get entree starter receive silverware napkin request book review excerpt goodread go try stop deceive eye catch title want like book disappointed book hilarious recommend look satirical read romantic twist narrator keep butt product review excerpt amazon lid blender probably like good enable pour take lid perfect pitcher work fantastic hate blender nearly impossible frozen fruit ice turn smoothie add ton liquid wish spout figure excerpt review review website scale star imdb scale star associate review score value range star star score fig show sample extract restaurant book movie review use review score supervision positive word likely appear star review negative word star review instead binary polarity kind supervision allow assign word complex representation polarity distribution star score star system represent sentiment word tuple number score represent word association polarity level association raw count likelihood function count class cfrom example compute imdb likelihood word like ing occur star review divide number time ing occur star review imdb dataset total ber word occur star review imdb estimate slight modiﬁcation weighting normalize likelihood illuminating visualization potts count pottsscore divide imdb estimate sum hood category give potts score word disappointing associate vector element potts score word wand category ccan show variant pointwise mutual information pmi log term exercise chapter exicon sentiment affect connotation potts diagram potts visualization word score represent pott diagram prior sentiment word distribution rating category fig show potts diagram positive negative scalar tive note curve strongly positive scalar shape letter strongly negative scalar look like reverse contrast weakly tive negative scalar hump shape maximum mean weakly negative word like disappointing mean weakly itive word like good shape offer illuminate typology affective meaning overviewdatamethodscategorizationscale inductionlooke aheadexample attenuator imdb token token token tripadvisor token imdb token token token tripadvisor token imdb token token token tripadvisor token scalarsnegative figure potts diagrams potts positive negative scalar adjective e shape reverse shape strongly positive negative adjective hump shape weakly polarize adjective fig show potts diagram emphasize attenuate adverb note emphatic tend shape likely occur tive review shape likely occur strongly positive tive attenuator hump shape emphasize middle scale downplay extreme diagram typology lexical sentiment play role model sentiment compositionality addition function like posterior likelihood normalize likelihood function count word occur sentiment label introduce page ing idea like normalize count writer log odd ratio informative dirichlet prior thing want word polarity distinguish word likely category text example want know word associate star review versus associate star review difference relate ment want ﬁnd word democratic republican member congress word menu expensive upervised learning word sentiment overviewdatamethodscategorizationscale inductionlooke aheadexample attenuator imdb token token token tripadvisor token imdb token token token tripadvisor token imdb token token token tripadvisor token scalarsnegative figure pott diagram potts emphatic attenuate adverb cheap restaurant give class document ﬁnd word associate gory measure difference frequency word wmore frequent class aor class instead difference frequency compute ratio frequency compute log odd ratio log ratio odd word sort word whichever ation measure pick range word overrepresente category ato word overrepresente category problem simple log likelihood log odd method phasize difference rare word frequent word rare word occur differently corpora tiny count statistical ﬂuctuation zero occurrence corpus compare non zero occurrence frequent word different count large section walk detail solution problem log odd ratio informative dirichlet prior method monroe particularly useful method ﬁnding word statistically overrepresente particular category text compare base idea large corpus prior estimate expect frequency word let start goal assume want know word horrible occur corpus ior corpus compute log likelihood ratio log likelihood ratio mean frequency word win corpus nito mean total number word corpus instead let compute log odd ratio horrible high odd ior log odd chapter exicon sentiment affect connotation dirichlet intuition use large background corpus prior estimate expect frequency word wto simply add count corpus numerator denominator essentially shrink count prior like ask large difference iand jgiven expect give frequency estimate large background corpus method estimate difference frequency word win corpora iandjvia prior modiﬁed log odd ratio estimate niis size corpus nji size corpus wis count word win corpus wis count word win corpus scale size background corpus awis scale count word win background corpus addition monroe use estimate variance log odd ratio ﬁnal statistic word score log odd ratio monroe method modiﬁes commonly log odd ratio way use score log odd ratio control variance word frequency use count background corpus provide prior count word fig show method apply dataset restaurant review yelp compare word star review word star review jurafsky large difference obvious sentiment word star review negative sentiment word like bad bad awful star review positive sentiment word like great good amazing illuminate difference star review use logical negation star review use emphatic emphasize universality highly star review use ﬁrst person plural star review use second person star review talk people manager waiter customer star review talk dessert property expensive restaurant like course atmosphere jurafsky sing lexicon sentiment recognition class word star review class word star review negative bad rude terrible horrible bad awful disgusting bland tasteless gross mediocre overpriced bad poorpositive great good delicious amazing favorite perfect excellent awesome friendly fantastic fresh wonderful credible sweet negation universalsvery highly perfectly deﬁnitely lutely pro pro pro article past verb ask tell say charge wait left tookadvice try recommend sequencer conjunct noun manager waitress waiter customer customer attitude waste poisoning money bill minutesnoun atmosphere dessert chocolate wine course menu irreali modalswould auxiliarie comp prep die city mouth figure word associate star ﬁve star restaurant review yelp dataset review monroe method jurafsky lexicon sentiment recognition appendix introduce naive baye algorithm sentiment analysis lexicon focus chapter far number way improve sentiment detection simple case lexicon sufﬁcient training datum build supervise sentiment analyzer expensive human assign sentiment document train supervised classiﬁer situation lexicon rule base algorithm tion simple version use ratio positive negative word document positive negative word lexicon decide larity word document classiﬁe positive threshold document classiﬁe positive ratio great sentiment lexicon include positive negative weight word simple sentiment algorithm wcount wcount sentiment supervise training datum available count compute sentiment icon weight normalize way ture classiﬁer lexical non lexical feature return algorithm section chapter exicon sentiment affect connotation lexicon affect recognition detection emotion kind affective meaning describe scherer generalize algorithm describe detect sentiment common algorithm involve supervised classiﬁcation training set label affective meaning detect classiﬁer build feature extract training set sentiment analysis training set large test set sufﬁciently similar training set simply word bigram feature powerful classiﬁer like logistic regression svm excellent algorithm performance hard beat treat affective meaning classiﬁcation text sample simple document classiﬁcation modiﬁcation nonetheless necessary large dataset example schwartz study personality gender age million word facebook post subset gram length word phrase subject include feature gram gram keep sufﬁciently high pmi pmi great length number word weight feature include raw count training set normalize probability log probability schwartz example turn feature count phrase likelihood normalize subject total word use subject training datum sparser similar test set lexicon discuss play helpful role combination word gram possible value lexicon feature simple indicator function value feature fltake value particular text word relevant lexicon notation appendix feature value deﬁne particular output class cand document alternatively value feature flfor particular lexicon lcan total number word token document occur lexica word associate score weight count multiply weight wcount exicon method entity affect count alternatively log normalize writer deﬁne lexicon feature supervised classiﬁer predict desire affective category text document classiﬁer train examine lexicon feature associate class classiﬁer like logistic regression feature weight give indication associate feature class lexicon base method entity centric affect want affect score entire document particular entity text entity centric method field tsvetkov combines affect lexicon contextual embedding assign affect score entity text context affect people relabel valence arousal nance dimension sentiment agency power algorithm ﬁrst train classiﬁer map embedding score word win training corpus use shelf pretraine encoder like bert extract contextual embed efor instance word additional ﬁnetuning average eembedding instance wto obtain single embed vector training point use nrc lexicon score train regression model word wto predict score word average embed give entity mention min text assign affect score follow use pretraine contextual embedding min context feed embed regression model score entity result tuple give entity mention score resentation entity complete document run coreference resolution average score mention fig show score algorithm character movie dark knight run wikipedia plot summary text gold coreference connotation frame lexicon describe far deﬁne word point affective space connotation frame contrast lexicon incorporate rich kind gram connotation frame matical structure combine affective lexicon frame semantic lexicon chapter basic insight connotation frame lexicon predicate like verb express connotation verb argument rashkin rashkin consider sentence like country violate sovereignty country chapter exicon sentiment affect connotation power scoreweaklyracheldentgordanbatmanjokerpowerfully sentiment scorenegativejokerdentgordanrachelbatmanpositive agency scoredulldentgordanrachelbatmanjokerscaryfigure power sentiment agency score acter dark night learn sion model elmo embedding score generally align character archetype antagonist low sentiment score ment result effective removal industry article metoo movement portray man like weinstein erful speculate corpora train elmo bert portray powerful corpus traditional power role invert embedding extract elmo bert perform bad dom biased power ture datum train idence exist performance bert mask embedding bedding generally capture power poorly pare unmasked embedding table outperform unmasked embedding task outperform frequency baseline setting form field likely capture affect information unmasked embedding table qualitative document level analysis finally qualitatively analyze method capture affect dimension analyze single document detail conduct ysis domain expect entity fulﬁll traditional power role entity al know follow bamman analyze wikipedia plot summary movie dark knight batman joker antagonist jim gordan law enforcement ofﬁcer ally batman consider batman bruce wayne scoreweakly rachel joker dent gordan batmanpowerfully sentiment scorenegative joker gordan batman dent rachel positive agency scoredull rachel dent gordanbatman joker scaryfigure power sentiment agency score char acter inthe dark nighta learn asp withelmo embedding score reﬂect pat tern regression model great separationbetween dent ally batman turn evil andrachel dawes primary love interest facil itate extract example sentence score eachinstance entity narrative separatelyand average instance obtain entityscore maximize databy capture mention entity form reference resolution hand addition ally base result asthe use wikipedia datum train elmomodel peters use elmo embed ding result refer ence entity score compare toone polar opposite pair identiﬁe asp boththe regression model asp similar pat tern batman high power rachel haslow power additionally joker associatedwith negative sentiment high est agency plot summary themovie progress joker take aggre sive action character dynamic reﬂecte score high power high agency low sentiment character primary plot driver general asp show great separationbetween character regression model wehypothesize occur asp isolatesthe dimension interest regression proach capture confound average metric evaluation find signiﬁcant change result sce nario compute score average embedding ratherthan average score separately compute embed de reduce computationally complexity figure power dominance sentiment valence agency arousal character movie dark knight compute embedding train nrc lexicon note protagonist batman antagonist joker high power agency score differ sentiment love interest rachel low power agency high sentiment teenager survive boston marathon bombing verb violate author express sympathy country portray country victim express antagonism agent country contrast verb survive author express bombing negative experience subject sentence teenager sympathetic character aspect connotation inherent meaning verbs violate andsurvive show fig writer asympathetic victimthere issome typeof hardship frame survive writer asympathetic victim frame violate figure connotation frame survive andviolate survive writer reader positive sentiment subject negative sentiment direct object violate writer reader positive sentiment instead direct object connotation frame lexicon rashkin rashkin express connotative aspect predicate ment include effect bad happen value valuable mental state distress event connotation frame mark power differential argument verb implore mean theme argument great power agent agency argument wait low agency fig show visualization sap connotation frame build hand sap learn supervised learning rashkin example hand label ummary implore tribunal princess wait formal notation connotation frame power agency ﬁrst example show relative power differential imply verb implore agent position power theme bunal contrast demand tribunal mercy imply agent authority theme second example show low level agency imply verb wait interactive demo website ﬁnding appendix see section connotation frame offer new insight complement viate know bechdel test bechdel particular ﬁnd high agency woman lens connotation frame rare modern ﬁlm movie snow white accidentally pass bechdel test movie strong female character entirely free deeply ingrain bias social norm connotation frame power agency create new connotation relation power andagency example figure sion exist connotation frame amt crowdworker annotate verb placeholder avoid gender bias text xrescue example task show appendix figure deﬁne tat construct follow power differential verbs imply thority level agent theme relative movie lexicon demo available home movie agency sample verb connotation frame high annotator agreement size indicative verb frequency corpus big frequent color difference legibility example agent inate theme denote power agent imply level control theme alternatively agent or theme denote power writer imply theme important authoritative amt crowdsourcing bel transitive verb power differential annotator verb inter annotator agreement krippendorff agency agency attribute agent verb denote action describe imply agent powerful decisive capable push forward storyline example person describe perience thing active cisive describe ing thing amt worker label tive verb imply high moderate low agency inter annotator agreement denote high agency agency low agency asagency pairwise agreement hard constraint power agency tively despite agreement reach moderate label count ing high low label show notator rarely strongly disagree contribute factor low score include subtlety choose neutral figure connotation frame sap show verb implore imply agent low power theme contrast verb like demand show low level agency subject wait figure sap ing datum supervise classiﬁer individual relation improve accuracy global constraint relation summary kind affective state distinguish include emotion mood attitude include sentiment interpersonal stance personality represent ﬁxed atomic unit call basic tion point space deﬁne dimension like valence andarousal word connotational aspect relate affective state connotational aspect word meaning represent lexicon affective lexicon build hand crowd source label affective content word lexicon build semi supervised bootstrappe seed word similarity metric like embed cosine lexicon learn fully supervise manner convenient training signal find world rating assign user review site word assign weight lexicon function word count training text ratio metric like log odd ratio informative dirichlet prior affect detect like sentiment standard supervise text classiﬁcation technique word bigram text feature additional feature draw count word lexicon lexicon detect affect rule base classiﬁer pick simple majority sentiment base count word lexicon frame express rich relation affective meaning icate encode chapter exicon sentiment affect connotation historical note idea formally represent subjective meaning word begin good pioneering study ﬁrst propose vector space model meaning describe chapter osgood participant rate word scale run factor analysis rating signiﬁcant factor uncover evaluative dimension distinguish pair like good bad valuable worthless pleasant unpleasant work inﬂuence development early dictionary sentiment affective meaning ﬁeld ofcontent analysis stone wiebe begin inﬂuential line work detect subjectivity text subjectivity begin task identify subjective sentence subjective acter describe text hold private state belief attitude learn sentiment lexicon polarity lexicon hatzivassiloglou mckeown show useful feature subjectivity detection vassiloglou wiebe wiebe term sentiment introduce das chen describe task measure market sentiment look word stock trading message board paper das chen propose use sentiment lexicon list word lexicon create hand word assign weight accord discriminate particular class buy versus sell maximize class variation minimize class variation term sentiment use lexicon catch quickly inter alia turney pang ﬁrst show power word sentiment lexicon wang manning semi supervised method describe extend sentiment tionarie draw early idea synonyms antonyms tend occur sentence miller charles justeson katz riloff herd semi supervised method learn cue affective ing rely information extraction technique like autoslog pattern extractor riloff wiebe graph base algorithm sentiment ﬁrst geste hatzivassiloglou mckeown graph propagation standard method zhu ghahramani zhu zhou velikovich crowdsourcing improve precision ﬁltere result semi supervised lexicon learning riloff shepherd fast recent work focus way learn embedding directly encode timent property ensifi algorithm rothe learn transform embed space focus sentiment mation exercise relationship word wand category cin pott score variant pointwise mutual information pmi log resolution entity link stigand patriotic archbishop canterbury find advisable find say duck find mouse reply crossly course know know ﬁnd thing say duck ally frog worm question archbishop ﬁnd lewis carroll alice wonderland important component language processing know talk text consider following passage victoria chen cfo megabuck banking see pay jump million year old company president widely know come megabuck rival lotsabuck underline phrase passage writer refer person name victoria chen linguistic expression like herorvictoria chen mention orreferring expression discourse entity refer mention victoria chen referent distinguish refer expression referent referent italicize referring expression refer discourse entity say corefer victoria chen corefer andshecorefer coreference important component natural language processing logue system tell user ﬂight united cathay paciﬁc know ﬂight user mean ond question answer system use wikipedia answer question marie curie know shewa sentence bear saw machine translation system translate language like spanish pronoun drop use coreference previous sentence decide spanish sentence encanta conocimiento dice translate love knowledge say love knowledge say example come actual news article ısabout female professor mistranslate machine translation inaccurate coreference resolution schiebinger natural language processing system human interpret linguistic sion respect discourse model karttunen discourse modeldiscourse model fig mental model understander build incrementally terprete text contain representation entity refer text property entity relation referent ﬁrst mention discourse representation evoke evoked model subsequent mention representation access model access convenient shorthand speak refer expression refer referent say sherefer victoria chen reader mind mean speaker perform act refer victoria chen utter chapter oreference resolution entity link vdiscourse figure mention evoke access discourse entity discourse model reference text entity previously introduce discourse call anaphora refer expression say anaphora anaphor passage pronoun sheandherand anaphor nite year old anaphoric anaphor corefer prior mention case victoria chen call antecedent antecedent ring expression antecedent entity single mention text like lotsabuck call singleton singleton chapter focus task coreference resolution coreferencecoreference resolution resolution task determine mention corefer mean refer entity discourse model discourse entity set coreferre expression call coreference chain acluster chain cluster example processing coreference resolution algorithm need ﬁnd coreference chain correspond entity discourse model fig chen year old sheg banking company megabucksg payg note mention nest example mention heris syntactically mention pay refer completely different discourse entity coreference resolution comprise task form jointly identify mention cluster ence chain discourse entity say mention corefere associate course entity like decide real world entity associate discourse entity example mention washington refer state capital city person george washington pretation sentence course different task ofentity link grishman entity resolution task mapping entity link discourse entity real world usually operationalize entity follow common nlp usage anaphor mean mention antecedent narrow usage mean mention like pronoun interpretation depend antecedent narrow interpretation repeat name anaphor linguistic nlp differ use term reference ﬁeld formal semantic use word reference andcoreference describe relation mention real world entity contrast follow functional linguistics tradition mention refer adiscourse entity webber relation discourse entity real world individual require additional step link linking resolution mapping ontology list entity world like gazeteer appendix common ontology task wikipedia wikipedia page act unique particular entity entity link task wikiﬁcation mihalcea csomai task cide wikipedia page correspond individual refer mention entity link ontology example ontology gene link mention gene text disambiguate gene ontology section introduce task coreference resolution tail survey variety architecture resolution introduce architecture task entity link turn algorithm mention important task touch brieﬂy end chapter famous winograd schema problem call ﬁrst point terry winograd dissertation entity coreference resolution problem design difﬁcult solve resolution method describe chapter kind real world knowledge require kind challenge task natural language processing example consider task determine correct antecedent pronoun follow example city council deny demonstrator permit fear violence advocate violence determine correct antecedent pronoun require understand second clause intend explanation ﬁrst clause city council likely demonstrator fear violence demonstrator likely advocate violence solve winograd schema problem require ﬁnde way represent discover necessary real world knowledge problem discuss chapter related task event ence decide event mention buyand acquisition inevent coreference sentence corpus refer event amd agree buy markham ontario base ati billion cash stock company announce monday acquisition turn amd world large provider graphic chip event mention hard detect entity mention bal nominal detect mention pair mention rank model entity apply event complex kind coreference discourse deixis webber discourse deixis anaphor refer discourse segment hard delimit categorize like example adapt webber accord soleil beau open restaurant thatturne lie thatwa false strike funny way describe situation referent speech act chapter proposition manner description algorithm chapter difﬁcult type non nominal antecedent kolhatkar chapter oreference resolution entity link coreference phenomena linguistic background offer linguistic background reference phenomena introduce type refer expression deﬁnite indeﬁnite nps pronouns name describe evoke access entity discourse model talk linguistic feature anaphor antecedent relation like number gender agreement property verb semantics type refer expression indeﬁnite noun phrase common form indeﬁnite reference glish mark determiner mark tiﬁer determiner indeﬁnite reference generally duce discourse context entity new hearer mrs martin kind send mrs goddard beautiful goose go round day bring walnut see beautiful cauliﬂower today deﬁnite noun phrase deﬁnite reference nps use english article refer entity identiﬁable hearer entity identiﬁable hearer mention previously text represent discourse model concern white stallion sell ofﬁcer pedigree ofthe white stallion fully establish alternatively entity identiﬁable contain hearer set belief world uniqueness object imply description case evoke representation referent discourse model read new york times see car key use common half deﬁnite nps newswire text non anaphoric ﬁrst time entity mention poesio vieira bean riloff pronoun form deﬁnite reference pronominalization tie extremely salient discourse discuss emma smile chat cheerfully shecould pronoun participate cataphora mention cataphora referent shesawit dorothy think emerald city day pronouns sheanditboth occur referent introduce pronoun appear quantiﬁed context consider bind bind dancer bring herleft arm forward relevant reading herdoe refer woman context instead behave like variable bind quantiﬁed expression dancer concern bind interpretation pronoun oreference phenomenon linguistic background language pronoun appear clitic attach word like spanish example ancora recasen mart intenci reconocer gran prestigio que tiene marat unir con esta gran carrera aim recognize great prestige marathon join jit great race demonstrative pronoun demonstrative pronoun thisandthat appear ther determiner instance ingredient spice buy copy thoreau walden buy ﬁve year ago tattered well condition note ambiguous colloquial speak english indeﬁnite deﬁnite zero anaphora instead pronoun language include nese japanese italian possible anaphor lexical realization call zero anaphor zero pronoun follow italian zero anaphor japanese example poesio john iwent visit friend way ibought wine giovanni far visita degli amici ficompr del vino john yujin houmon sita tochu fiwain tta chinese example nervous ago calm zero anaphor complicate task mention detection language name name people location organization refer new old entity discourse miss woodhouse certainly justice business machine seek patent compensation amazon ibm previously sue company information status way refer expression evoke new referent discourse introduce new information access old entity model old tion call information status orinformation structure entity beinformation status discourse new ordiscourse old common distinguish discourse new discourse old kind entity informationally prince new nps brand new nps introduce entity discourse new new like fruit orsome walnut unused nps introduce entity discourse new hearer old like hong kong marie curie orthe new york times old nps call evoke nps introduce entity course model discourse old hearer old like itin go new restaurant chapter oreference resolution entity link inferrable introduce entity hearer old discourse old hearer infer existence reasoning base entity discourse consider follow example go superb restaurant yesterday chef open mix ﬂour butter water knead dough shiny chef northe dough discourse model base ﬁrst sentence example reader bridge inferencebridge inference entity add discourse model associate restaurant ingredient base world knowledge restaurant chef dough result mix ﬂour liquid haviland clark webber baldwin nissim hou form give strong clue information status talk entity position give new dimension extent give new ent give salient discourse easy hearer mind predictable hearer versus new non salient discourse unpredictable chafe prince gundel referent accessible ariel accessible salient hearer mind easy mind refer linguistic material example pronoun referent high degree activation salience discourse contrast salience salient entity like new referent introduce discourse need introduce long explicit refer expression help hearer recover referent entity ﬁrst introduce discourse mention likely name title role appositive restrictive relative clause introduction protagonist victoria chen cfo megabuck banking entity discuss discourse salient hearer mention average typically short informative example shorten example chen deﬁnite description year old pronoun sheorher hawkin change length monotonic sensitive discourse structure grosz reichman fox complication non referring expression noun phrase nominal refer expression bear confusing superﬁcial resemblance example early computational work reference resolution karttunen point npa car follow example create discourse referent janet car refer anaphoric itorthe car itis toyota car red summarize common type structure count tion coreference task complicate task mention detection usually refer entity introduce sentence ongoing discourse deﬁnite noun phrase refer oreference phenomenon linguistic background appositive appositional structure noun phrase appear head noun phrase describe head english appear commas like unit ual appear apposition united orcfo megabuck banking apposition victoria chen victoria chen cfo megabuck banking see united unit ual match fare appositional nps refer expression instead function kind supplementary parenthetical description head nonetheless useful link phrase entity describe dataset like ontonote mark appositional relationship predicative prenominal nps predicative attributive nps describe ertie head noun united unit ual unit ual describe property united refer distinct entity mark mention coreference task example nps million company president attributive describe property pay andthe year old example show chinese example predicate 中国最大的城市 china big city mention pay jump million year old company president shanghai china big city expletive use pronoun like itin english corresponding pronoun language referential expletive orpleonastic case include expletive rain idiom like hit particular syntactic situation like cleft cleft extraposition itwa emma goldman found mother earth herring hang wall generic kind expression refer entity itly evoke text generic reference consider love mango tasty refer particular mango set mangos instead class mango general pronoun youcan generically july san francisco youhave wear jacket linguistic property coreference relation see linguistic property individual referring expression turn property antecedent anaphor pair understand property helpful design novel feature perform error analysis number agreement refer expression referent generally agree number english singular ral youis unspeciﬁed number plural antecedent like chef generally corefer singular anaphor like algorithm enforce number agreement strictly semantically plural entity ferre itorthey ibm announce new machine translation product yesterday work chapter oreference resolution entity link second singular common singular describe singular individual useful gender neutral recently increase singular old english person agreement english distinguishe ﬁrst second person pronoun antecedent agree pronoun person person pronoun person antecedent noun phrase phenomenon like quotation cause exception example sheare coreferent vote nader align value say gender noun class agreement language noun ical gender noun pronouns generally agree grammatical gender antecedent english occur person singular pronoun distinguish male female nonpersonal grammatical gender non binary pronoun like zeorhirmay occur recent text know gender associate text complex require world knowledge individual example maryam theorem exciting maryam theorem maryam theorem exciting theorem maryam bind theory constraint bind theory syntactic straint relation mention antecedent sentence chomsky oversimplify bit reﬂexive pronoun like reﬂexive selfcorefer subject immediate clause contain nonreﬂexive corefer subject janet buy bottle ﬁsh sauce janet janet buy bottle ﬁsh sauce janet recency entity introduce recent utterance tend salient introduce utterance pronoun itis likely refer jim map doctor map doctor find old map captain chest jim find old map hide shelf describe island grammatical role entity mention subject position salient object position turn salient mention oblique position ﬁrst sentence express roughly propositional content preferred referent pronoun vary subject billy bone jim hawkins billy bone go bar jim hawkins call glass rum billy jim hawkins go bar billy bone call glass rum jim bind pronoun example shakespeare comedy error man meet doth salute acquaint friend word gender generally language noun class like european language language like bantu language chinese large number noun oreference task dataset verb semantic verb semantically emphasize argument ase interpretation subsequent pronoun compare john telephoned bill lose laptop john criticize bill lose laptop example differ verb ﬁrst sentence typically resolve john resolve bill partly link implicit causality saliency implicit cause criticizing event object implicit cause telephone event subject verb entity implicit cause salient selectional restriction kind semantic knowledge play role referent preference example selectional restriction verb place argument chapter help eliminate referent eat soup new bowl cook hour possible referent soup bowl verb eat require direct object denote edible constraint rule outbowl possible referent coreference task dataset formulate task coreference resolution follow give text ﬁnd entity coreference link evaluate task pare link system create human create gold coreference annotation let return coreference example superscript number coreference chain cluster subscript letter individual mention ter victoria cfo megabuck see ajump million year calso president widely know dcame cfrom rival assume example entirety article chain pay lotsabuck singleton mention chen year old sheg banking company megabucksg payg coreference evaluation campaign input system raw text article system detect mention link cluster solve task require deal pronominal anaphora ﬁgure refer victoria chen ﬁltere non referential pronoun like pleonastic init year deal deﬁnite noun phrase ﬁgure year old coreferent victoria chen company megabuck need deal name realize megabuck asmegabuck banking chapter oreference resolution entity link exactly count mention link annotate differ task task dataset dataset example coreference dataset label singleton make task simple resolver achieve high score corpora singleton singleton constitute majority mention running text hard distinguish non referential nps task use gold mention detection system give human label mention boundary task cluster gold mention eliminate need detect segment mention run text coreference usually evaluate conll score combine metric muc ceaf section give detail let mention characteristic popular coreference dataset ontonote pradhan pradhan conll share task base pradhan ontonote contain hand annotate chinese english coreference dataset roughly million word consist newswire magazine article broadcast news broadcast conversation web datum conversational speech data word annotated arabic newswire important distinguishing characteristic ontonote label singleton simplify coreference task singleton resent entity way similar coreference dataset refer expression nps coreferent mark mention generic pleonastic pronoun mark appositive clause mark separate mention include mention richard godown president industrial biotechnology association mention entire phrase prenominal modiﬁer annotate separate entity proper noun wheat entity wheat ﬁeld uni entity policy adjective like american inamerican policy number corpora mark rich discourse phenomenon isnote corpus annotate portion ontonote information status include bridge example hou litbank coreference corpus bamman contain coreference annotation token different literary novel clude singleton quantiﬁed negate noun phrase ancora erence corpus recasen mart contain word spanish ancora catalan ancora news datum include label complex phenomenon like discourse deixis language arrau corpus uryupina contain word english mark nps mean singleton cluster available arrau include diverse genre like dialog train datum ﬁction pear story label bridge ence discourse deixis generic ambiguous anaphoric relation mention detection ﬁrst stage coreference mention detection ﬁnde span text thatmention detection constitute mention mention detection algorithm usually liberal propose candidate mention emphasize recall ﬁltere later example system run parser name entity tagger text extract span apossessive pronoun name entity sample text repeat victoria chen cfo megabuck banking see pay jump ention detection million year old company president widely known come megabuck rival lotsabuck result following list potential mention victoria chen million cfo megabuck bank year old megabuck megabuck bank company lotsabuck company president pay recent mention detection system generous span base algorithm describe section ﬁrst extract literally gram span word course recall section nps overwhelming majority random gram span refer expression mention detection system need eventually ﬁlter tic expletive pronoun like itabove appositive like cfo megabuck banking inc predicate nominal like company president million ﬁltering rule early rule base system design regular expression deal pleonastic like follow rule lappin leass use dictionary cognitive verb believe know ipate capture pleonastic itin think ketchup modal adjective necessary possible certain important likely rule modern system modaladjective modaladjective cogv appears mean follow mention detection rule design speciﬁcally particular uation campaign ontonote example mention embed large mention numeric quantity annotate rarely erential ontonote task like conll pradhan common ﬁrst pass rule base mention detection algorithm lee nps possessive pronoun name entity numeric quantity dollar mention embed large mention adjectival form nation stop word like pleonastic itbase regular expression pattern rule base system generally insufﬁcient deal detection modern system incorporate sort learn mention tion component referentiality classiﬁer anaphoricity classiﬁer detect anaphor discourse new classiﬁer detect mention discourse new potential antecedent future anaphor ananaphoricity detector example draw positive training examplesanaphoricity detector span label anaphoric refer expression hand label dataset like ontonote arrau ancora name entity mark negative training example anaphoricity classiﬁer use feature candidate mention head word surround word deﬁniteness animacy length position sentence discourse ﬁrst propose early work cardie section chapter oreference resolution entity link referentiality anaphoricity detector run ﬁlter tion classiﬁe anaphoric referential pass coreference system end result ﬁltering mention detection system example follow ﬁltered set potential mention victoria chen pay megabuck bank year old megabuck company lotsabuck turn hard ﬁltering mention base anaphoricity referentiality classiﬁer lead poor performance anaphoricity classiﬁer threshold set high mention ﬁltere recall suffer classiﬁer threshold set low pleonastic non referential mention include precision suffer modern approach instead perform mention detection anaphoricity coreference jointly single end end model deni baldridge rahman example mention detection lee system base single end end neural network compute score mention referential score mention ence combine decision train score single end end loss describe method detail section despite advance correctly detect referential mention unsolved problem system incorrectly mark pleonastic pronoun like itand non referential nps coreferent large source error modern coreference resolution system kummerfeld klein martschat strube martschat strube wiseman lee mention referentiality anaphoricity detection important open area investigation source knowledge turn helpful especially combination unsupervised semisupervise algorithm igate expense label dataset early work example bean riloff learn pattern characterize anaphoric non anaphoric nps tracting generalize ﬁrst nps text guarantee non anaphoric chang look head noun appear frequently training datum appear gold mention help ﬁnd non referential nps bergsma use web count semisupervise way augment standard feature anaphoricity detection english important task itis common ambiguous quarter half itexample non anaphoric consider follow example advance anaphoric hollywood non anaphoric theitinmake non anaphoric idiom bergsma turn context example pattern like advance hollywood use google gram enumerate word replace itin pattern non anaphoric contexts tend itin wildcard position anaphoric contexts occur nps example advance frequent datum system try avoid mention detection anaphoricity detection altogether dataset like ontonote label singleton alternative ﬁltere non referential mention run coreference resolution simply delete candidate mention corefere mention likely work explicitly model referentiality solve problem detect singleton important task like entity rchitecture coreference algorithm asmake advance hollywood occur gram contexts feature supervised anaphoricity classiﬁer architecture coreference algorithms modern system coreference base supervise neural machine learning supervise hand label dataset like ontonote section overview architecture modern system categorization distinguish algorithm base coreference sion way entity base represent entity discourse model mention base consider mention independently useranke model directly compare potential antecedent detail state art algorithm section mention pair architecture begin mention pair architecture simple inﬂuential mention pair coreference architecture introduce feature complex algorithm architecture perform well mention pair mention pair chitecture base classiﬁer suggest give pair mention candidate anaphor candidate antecedent make binary classiﬁcation decision coreferre let consider task classiﬁer pronoun shein example assume slightly simpliﬁed set potential antecedent fig victoria chenmegabuck bankingherher paythe year figure pair mention like potential antecedent mention like victoria chen orher mention pair classiﬁer assign probability coreference link prior mention victoria chen megabuck banking etc binary classiﬁer compute probability mention antecedent want probability high actual antecedent victoria chen year old low non antecedent megabuck banking pay early classiﬁer hand build feature section recent classiﬁer use neural representation learning section training need heuristic select training sample pair mention document coreferent select pair lead massive overabundance negative sample common heuristic soon choose close antecedent positive example pair negative example formally anaphor mention miwe create positive instance mji close antecedent chapter oreference resolution entity link negative instance mkbetween mjandmi anaphor choose positive example negative example similarly anaphor company choose company megabuck positive example company pany year old company pay company negative example classiﬁer train apply test sentence clustering step mention iin document classiﬁer consider prior mention clustering soon classiﬁer run right leave mention mention ﬁrst antecedent probability link antecedent probably antecedent select clustering classiﬁer run antecedent probable precede mention choose antecedent transitive closure pairwise relation take cluster mention pair model advantage simplicity main problem classiﬁer directly compare candidate antecedent train decide likely antecedent fact well second ignore discourse model look mention entity classiﬁer decision completely locally pair able account mention entity model address ﬂaw mention rank architecture mention rank model directly compare candidate antecedent choose highest scoring antecedent anaphor early formulation mention classiﬁer decide mention antecedent denis baldridge suppose iis fact anaphoric antecedent choose model need run separate anaphoricity classiﬁer instead turn well jointly learn anaphoricity detection coreference single loss rahman modern mention rank system ith mention anaphor associated random variable yirange value special dummy mention mean idoe antecedent discourse new start new coref chain non anaphoric victoria chenmegabuck bankingherher paythe year theseshould highall theseshould year figure candidate anaphoric mention like mention rank system assign bility distribution previous mention plus special dummy mention test time give mention ithe model compute softmax antecedent plus give probability candidate antecedent lassifier hand feature fig show example computation single candidate anaphor antecedent classiﬁe anaphor transitive closure run pairwise decision complete clustering training tricky mention rank model mention pair model anaphor know possible gold antecedent use training instead good antecedent mention latent mention cluster legal gold antecedent choose early work heuristic choose antecedent example choose close antecedent gold antecedent non antecedent window sentence negative example denis baldridge kind way model latent antecedent exist fernande chang durrett klein simple way credit legal antecedent sum loss function optimize likelihood correct antecedent gold clustering lee detail section mention rank model implement hand build feature neural representation learning incorporate hand build ture explore direction section section entity base model mention pair mention rank model decision tion contrast entity base model link mention previous mention previous discourse entity cluster mention mention rank model turn entity rank model simply have classiﬁer decision cluster mention individual mention rahman traditional feature base model extract feature cluster size cluster useful feature shape list type mention cluster sequence token cluster compose fvictoria year oldgwould shape kuhn base model include mention pair classiﬁer use feature aggregate mention pair probability example compute average probability erence mention pair cluster clark man neural model learn representation cluster automatically example rnn sequence cluster mention encode state ing cluster representation wiseman learn distribute resentation pair cluster pool learn representation mention pair clark manning entity base model expressive use level information practice lead large gain performance ranking model commonly classiﬁer hand build feature feature base classiﬁer use hand design feature logistic regression svm random forest classiﬁer coreference resolution classiﬁer chapter oreference resolution entity link form neural one nonetheless useful build lightweight system compute datum sparse feature useful error analysis neural system give anaphor mention potential antecedent mention feature base classiﬁer use type feature feature anaphor feature candidate antecedent iii feature relationship pair entity base model additional use additional class feature mention antecedent entity cluster feature relation anaphor mention antecedent entity cluster feature anaphor antecedent mention word victoria word embedding antecedent anaphor head word victoria head word head embed antecedent anaphor attribute perthe number gender animacy person name entity type attribute antecedent anaphor length length word antecedent anaphor mention type type tecedent anaphor feature antecedent entity entity shape shape list type mention antecedent entity cluster sequence entity attribute number gender animacy person name entity type attribute antecedent entity ant cluster size number mention antecedent cluster feature pair mention sentence distance number sentence antecedent anaphor mention distance number mention antecedent anaphor anaphor relation antecedent cosine cosine antecedent anaphor embedding feature pair entity exact string match true string mention antecedent anaphor cluster identical head word match true mention antecedent cluster headword mention anaphor cluster word inclusion word anaphor cluster include antecedent cluster figure feature base coreference sample feature value anaphor potential antecedent victoria chen figure show selection commonly feature show value compute potential anaphor potential antecedent victoria chen example sentence repeat victoria chen cfo megabuck banking see pay jump million year old company president widely known shecame megabuck rival lotsabuck feature prior work find particularly useful exact string match entity headword agreement mention distance pronoun exact attribute match nominal proper name word inclusion cosine lexical feature like head word common use word appear time neural mention algorithm crucial feature base system use conjunction feature iment suggest move individual feature classiﬁer conjunction multiple feature increase point lee speciﬁc tion design hand durrett klein pair feature conjoin bengtson roth feature conjunction learn decision tree random forest classiﬁer cardie lee feature neural model neural system use tual word embedding beneﬁt shallow feature like string match mention type feature like mention length distance mention genre complement neural contextual embed model neural mention rank algorithm section describe neural coref algorithm lee simpliﬁed extend bit draw joshi amention rank algorithm consider possible span text ment assign mention score span prune mention base score assign coreference link remain mention formally give document dwith tword model consider span bigrams trigram gram etc practice consider span maximum length task assign span ian antecedent random variable range value previous span special dummy token choose dummy token mean idoes antecedent iis new start new coreference chain iis non anaphoric pair span iand system assign score erence link span iand span system learn distribution antecedent span score factor deﬁne span ii mention span ji mention ji antecedent dummy antecedent score ﬁxed way dummy score positive model predict highest scoring antecedent score negative abstain compute span representation compute function score span ior pair span need way represent span coref family algorithm represent span try capture word token ﬁrst word word important word ﬁrst run paragraph subdocument encoder like bert generate embedding hifor token span iis represent vector githat concatenation encoder chapter oreference resolution entity link embed ﬁrst start token span encoder output end token span vector attention base representation hstart goal attention vector represent word token likely syntactic head word span see prior section head word useful feature match head word good indicator coreference attention representation compute usual system learn weight vector compute dot product hidden state httransforme ffn attention score normalize distribution softmax pend start attention distribution create vector attention weighted sum embedding etof word span start fig show computation span representation mention score encoding encoder generalelectricsaidthepostalservicecontactedthecompanyspan head hatt span representation mention score electricelectric say thethe postal serviceservice contact thethe company figure computation span representation mention score bert version coref model lee joshi model consider span maximum width ﬁgure show small subset bigram trigram span compute mention antecedent score mandc know compute vector gifor represent span detail scoring function compute feedforward network inference time mention score mis ﬁlter good neural mention algorithm compute antecedent score high scoring mention antecedent score input representation span iandj wise similarity span element wise tiplication fig show computation score sfor possible antecedent company example sentence fig figure computation score sfor possible antecedent pany example sentence fig figure lee give set mention joint distribution antecedent ment compute forward pass transitive closure antecedent create ﬁnal clustering document fig show example prediction model show attention weight lee ﬁnd correlate traditional semantic head note model get second example wrong presumably attendant andpilot likely nearby word embedding figure sample prediction lee model cluster example show correct example mistake bold parenthesize span tion predicted cluster red color word indicate attention weight figure adapt lee learning training single gold antecedent mention instead coreference labeling give entire cluster coreferent mention mention latent antecedent use loss function mize sum coreference probability legal antecedent give mention iwith possible antecedent let gold set mention gold cluster contain set mention occur set mention gold cluster occur chapter oreference resolution entity link want maximize mention iis gold cluster gold turn probability loss function use cross entropy loss function deﬁne chapter take probability sum mention ﬁnal loss function training entity link entity link task associate mention text representation entity link real world entity ontology knowledge base grishman natural follow coreference resolution coreference resolution task associate textual mention corefer entity entity link take step identify entity especially important nlp task link knowledge base sort potential knowledge basis focus section wikipedia widely ontology nlp task usage unique wikipedia page act unique particular entity task decide wikipedia page correspond individual refer text mention wikiﬁcation mihalcea csomai wikiﬁcation early system mihalcea csomai cucerzan milne witten entity link roughly stage mention tion andmention disambiguation algorithm simple classic baseline use anchor dictionary information wikipedia graph structure ferragina scaiella modern neural algorithm focus mainly application entity link question lot literature context link base anchor dictionary web graph simple baseline introduce agme linker ferragina scaiella wikipedia draw early algorithm mihalcea csomai cucerzan milne witten wikiﬁcation algorithms deﬁne set entity set wikipedia page refer wikipedia page unique entity agme ﬁrst create catalog entity wikipedia page remove disambiguation meta page index standard engine like lucene page algorithm compute link count total number link wikipedia page point count derive wikipedia dump finally algorithm require anchor dictionary anchor dictionary list wikipedia page anchor text hyperlinke span text anchor text page point example web page stanford university point page anchor text like stanford orstanford university ntity link compute wikipedia anchor dictionary include wikipedia page title anchor text wikipedia page point anchor string compute total frequency freq wikipedia include non anchor use number time aoccur link link probability linkprob cleanup ﬁnal anchor dictionary require example remove anchor string compose number single character rare unlikely useful entity low linkprob mention detection give question text try link agme detect mention query anchor dictionary token sequence word large set sequence prune simple heuristic example pruning substring small linkprob question ada lovelace bear rise anchor ada lovelace possibly ada substring span likelovelace prune have low linkprob span like bear low linkprob anchor dictionary mention disambiguation mention span unambiguous point entity wikipedia page entity link span ambiguous match anchor multiple wikipedia entity page agme algorithm use factor disambiguate ambiguous span refer prior probability andrelatedness coherence ﬁrst factor probability span refer particular entity page probability anchor apoint ratio number link ewith anchor text ato total number occurrence aas anchor count let factor work link entity following question chinese dynasty come yuan common association span yuan anchor dictionary chinese currency probability high rare wikipedia association yuan include common chinese language speak thailand correct entity case chinese dynasty chose base wrong disambiguation miss correct link yuandynasty help sort case agme use second factor relatedness entity entity input question example fact question contain span chinese dynasty high probability link pagedynastie inchinese history ought help match yuandynasty let work give question candidate anchor span adetecte assign relatedness score possible entity relatedness score link weighted average relatedness eand entity entity consider relate extent wikipedia page share link formally relatedness entity aandbis compute chapter oreference resolution entity link set wikipedia page point xandwis set pedia page collection vote give anchor bto candidate annotation average possible entity relatedness weight prior probability total relatedness score sum vote anchor detect relatedness score combine relatedness prior choose entity high relatedness ﬁnde entity small value set choose entity high prior result step single entity assign span agme algorithm step prune spurious anchor entity pair assign score average link probability coherence coherence coherence finally pair prune score threshold lis set hold set neural graph base link recent entity link model base encoder encode candidate mention span encode entity compute dot product coding allow embedding entity knowledge base precompute cache let sketch elq link algorithm give question qand set candidate entity wikipedia associate wikipedia text output tuple entity mention start mention end fig show encode wikipedia entity text wikipedia encode mention span text question compute similarity describe entity mention detection dimensional embedding question token algorithm run question bert normal way bert compute likelihood span entity mention way similar span base algorithm see reader compute score jbee start end mention ntity link figure sketch inference process elq algorithm entity link question candidate question mention span candidate entity separately encode score entity span dot product wstartandwendare vector learn training trainable embed wmention compute score token tion smention mention probability compute combine score ismention entity link link mention entity compute embedding entity set wikipedia entity text entity wikipedia page title ﬁrst token wikipedia page description run bert take output clstoken bert cls entity representation xei bert mention span link entity computing entity eand span dot product similarity span encode average token embedding entity encode iqt finally softmax distribution entity span train elq mention detection entity link algorithm fully vise mean unlike anchor dictionary algorithm section chapter oreference resolution entity link require dataset entity boundary mark link label dataset webquestionssp yih extension webquestion berant dataset derive google search question tion entity span question mark link sorokin gurevych result entity label sion webqsp eland graphq give training set elq mention detection entity link phase train jointly optimize sum loss mention detection loss binary cross entropy loss lthe length passage nthe number candidate gold mention span entity link loss egis gold entity mention evaluation coreference resolution evaluate coreference algorithms model theoretically compare set esischain cluster hproduce system set gold reference chain cluster rfrom human labeling report precision recall wide variety method comparison fact common metric evaluate coreference algorithm link base muc vilain blanc recasen hovy luo metric mention base bagga baldwin entity base ceaf metric luo link base entity aware lea metric moosavi strube let explore metric muc measure vilain measure base number coreference link pair mention common hand precision number common link divide number link recall number common link divide number link make muc bias system produce large chain few entity ignore singleton involve link mention base link base mention reference chain compute precision recall weighted sum nmention document compute precision recall entire task give mention letrbe reference chain include hthe hypothesis chain set correct mention precision mention jhj recall mention jrj total precision weight sum precision mention weight weight total recall weight sum recall mention weight weight equivalently precision correct mention hypothesis chain contain entity mention hypothesis chain contain entity recall correct mention hypothesis chain contain entity mention reference chain contain entity inograd schema problem weight wifor entity set different value produce different version algorithm follow proposal denis baldridge conll coreference competition score base average muc ceaf pradhan common evaluation campaign report average metric luo pradhan detailed description entire set metric reference implementation attempt reimplement scratch pradhan alternative metric propose deal particular coreference main task example consider task resolve mention name entity person organization geopolitical entity useful formation extraction knowledge base completion hypothesis chain rectly contain pronoun refer entity version link wrong useful task instead want metric weight mention informative name informative chen metric consider hypothesis match gold chain contain variant nec metric agarwal winograd schema problem early ﬁeld researcher note case coreference difﬁcult require world knowledge sophisticated reasoning solve problem famously point winograd follow example city council deny demonstrator permit fear violence advocate violence winograd notice antecedent reader prefer noun continuation city council demonstrator suggest require understand second clause intend explanation ﬁrst clause cultural frame suggest city council likely demonstrator fear violence demonstrator likely advocate violence attempt ﬁeld nlp focus method involve world knowledge common sense reasoning levesque propose challenge task call winograd schema challenge problem challenge taskwinograd schema coreference problem design easily disambiguate human reader hopefully solvable simple technique selectional restriction basic word association method problem frame pair statement differ single word phrase coreference question trophy suitcase large question large answer trophy quickly follow levesque rahman competition ijcai conference davis natural language inference version problem call wnli wang chapter oreference resolution entity link trophy suitcase small question small answer suitcase problem following characteristic problem party pronoun preferentially refer party grammatically refer question ask party pronoun refer word question change human preferred answer change party kind world knowledge need solve problem vary trophy suitcase example knowledge physical world big object small object original winograd sentence stereotype social actor like politician protester example like following knowledge human action like turn taking thanking bill pass gameboy john turn turn answer bill john joan sure thank susan help give receive give receive help answer susan joan winograd schema design require common sense soning large percentage original set problem solve train language model ﬁne tune winograd schema sentence kocijan large pretraine language model encode enormous world common sense knowledge current trend propose new dataset increasingly difﬁcult winograd like coreference resolution problem like ref emami example like marcus undoubtedly fast jarrett right prime gap big end likely combination language modeling edge prove fruitful knowledge base model overﬁt lexical idiosyncracie winograd schema training set trichelair gender bias coreference aspect language processing coreference model exhibit gender bias zhao rudinger webster ple winobias dataset zhao use variant winograd schema paradigm test extent coreference algorithm bias ing gendere pronoun antecedent consistent cultural stereotype summarize chapter embedding replicate societal bias training test associate man historically sterotypical male occupation like doctor woman stereotypical female occupation like secretary caliskan garg winobias sentence contain mention correspond male stereotypically female occupation gendere pronoun link sentence disambiguate gender pronoun biased model distract cue example ummary secretary call physician iand tell iabout new patient pro stereotypical secretary call physician iand tell iabout new patient anti stereotypical zhao consider coreference system bias rate link pronoun consistent gender stereotypical occupation physician link pronoun inconsistent gender stereotypical occupation herwith physician coreference tem architecture rule base feature base machine learn end end neural signiﬁcant bias perform average bad anti stereotypical case possible source bias female entity signiﬁcantly derrepresente ontonote dataset train coreference system zhao propose way overcome bias generate second gender swap dataset male entity ontonote replace female one vice versa retrain coreference system combined inal swap ontonote datum debiased glove embedding basi result coreference system long exhibit bias winobia dataset signiﬁcantly impact ontonote coreference accuracy follow paper zhao bias exist elmo contextualize word vector representation coref system use show retrain elmo datum augmentation reduce remove bias coreference system winobia webster introduce dataset gap task gendere pronoun resolution tool develop improve coreference algorithm gendere pronoun gap gender balanced label corpus sentence gendere ambiguous pronoun contrast gendered noun english ontonote training datum feminine example create draw naturally occur sentence wikipedia page create hard resolve case name entity gender ambiguous pronoun refer person like following fujisawa join mari motohashi rink team skip move karuizawa kitami shehad spend junior day webster modern coreference algorithm perform icantly bad resolve feminine pronoun masculine pronoun gap kurita show system base bert contextualize word sentation show similar bias summary chapter introduce task coreference resolution task link mention text corefer refer discourse entity discourse model result set coreference chain call cluster orentitie mention deﬁnite nps orindeﬁnite nps pronouns include zero pronoun orname chapter oreference resolution entity link surface form entity mention link information status new old orinferrable accessible orsalient entity nps refer expression pleonastic itinit rain corpora human label coreference annotation supervised learning include ontonote english chinese bic arrau english ancora spanish catalan mention detection start noun name entity use anaphoricity classiﬁer orreferentiality classiﬁer ﬁlter non mention common architecture coreference mention pair mention rank andentity base use feature base neural siﬁer modern coreference system tend end end perform mention tection coreference single end end architecture algorithm learn representation text span head learn pare anaphor span candidate antecedent span entity link task associate mention text tion real world entity ontology coreference system evaluate compare gold entity label precision recall metric like muc blanc orlea winograd schema challenge problem difﬁcult coreference lem require world knowledge sophisticated reasoning solve coreference system exhibit gender bias evaluate dataset like winobia gap historical note coreference natural language processing wood winograd discourse model entity centric foundation coreference formulate karttunen cole ence play role linguistic semantic heim kamp bonnie webber dissertation follow work webber explore model computational aspect provide fundamental insight entity represent discourse model way license subsequent reference example provide continue lenge theory reference day hobbs tree search algorithm ﬁrst longhobbs algorithm series syntax base method identify reference robustly naturally ring text input hobbs algorithm pronoun resolve syntactic constituency parse sentence include rent sentence detail algorithm depend grammar understand simpliﬁed version kehler search list nps current prior sentence simpliﬁed hobbs algorithm search nps following order current sentence right left start ﬁrst left pronoun previous sentence left right iii sentence prior left right simple algorithm present originally hobbs note current sentence leave right start ﬁrst noun group right pronoun cataphora ﬁrst noun group agree pronoun respect number gender person choose antecedent kehler lappin leass inﬂuential entity base system weight combine syntactic feature extend soon kennedy raev system avoid need syntactic parse approximately contemporaneously center grosz apply pronominal anaphora resolution brennan wide variety work follow focus center use coreference kameyama genio walker eugenio strube hahn kehler tetreault iida kehler rohde ing integrate coherence drive theory pronoun interpretation chapter use center measure discourse coherence coreference competition darpa sponsor muc ence provide early label coreference dataset corpora set tone late work choose focus exclusively simple case identity coreference ignore difﬁcult case like bridge metonymy draw community supervise machine learning metric like muc metric vilain later ace uation produce label coreference corpora english chinese arabic widely model training evaluation darpa work inﬂuence community supervise learning ning connolly aone bennett mccarthy lehnert soon lay set basic feature extend cardie series machine learning model follow year focus separately pronominal anaphora resolution kehler bergsma lin coreference cardie wagstaff cardie deﬁnite reference poesio vieira vieira poesio separate anaphoricity detection bean riloff bean riloff cardie singleton detection marneffe mention pair mention rank approach pioneer yang iida propose pairwise rank method extend denis baldridge propose rank max prior mention idea mention detection anaphoricity coreference jointly single end end model grow early proposal use dummy antecedent mention rank allow non referential choice coreference classiﬁer deni baldridge joint system combine anaphoricity classiﬁer probability coreference probability denis baldridge rank model rahman posal train model jointly single objective simple rule base system coreference return prominence partly ability encode entity base feature high precision way zhou haghighi klein raghunathan lee lee hajishirzi end suffer inability deal semantic necessary correctly handle case common noun coreference return supervise learning lead number advance mention rank model extend neural architecture example chapter oreference resolution entity link inforcement learning directly optimize coreference evaluation model clark manning end end coreference way span extraction lee zhang neural model design advantage global entity level information clark man wiseman lee coreference relate task entity link discuss chapter coreference help entity link give possible surface form help link right wikipedia page conversely entity link help improve coreference resolution consider example hajishirzi michael eisner donald tsang grand opening hong kong eisner president welcome fan park integrate entity link coreference help draw encyclopedic edge like fact donald tsang president help disambiguate tionthe president ponzetto strube ratinov roth show attribute extract wikipedia page build rich model entity mention coreference recent research show link coreference jointly hajishirzi zheng jointly name entity tag durrett klein coreference task introduce involve simplifying assumption relationship anaphor antecedent identity coreferre mention refer identical discourse referent real text tionship complex different aspect discourse referent neutralize refocus example recasen show example metonymy capital city washington metonymically metonymy refer example recasen strict interpretation policy require notify foreign dictator certain coup plot washington reject bid cross border ashgh abad nowruz persian new year south celebrate new year north regular day france president elect term seven year united states heis elect term year linguistic discussion complication coreference jovsky van deemter kibble poesio fauconni turner versley barker offer useful compact history machine learning model erence resolution excellent book length survey anaphora erence resolution cover different time period hirst early work mitkov poesio andy kehler write discourse chapter ﬁrst edition book starting point second edition chapter remnant andy lovely prose edition coreference ter exerciseschapter coherence wild wandering reverie nay dream shall ﬁnd reﬂect imagination run altogether ture connection uphold different idea succeed loose free conversation transcribe immediately transcribe immediately observe connect transition david hume enquiry concern human understanding orson welle movie citizen kane groundbreake way notably structure story life ﬁctional medium magnate charle foster kane movie proceed chronological order kane life instead ﬁlm begin kane death famously murmur rosebud structure ﬂashback life insert scene reporter investigate death novel idea structure movie linearly follow structure real timeline apparent century cinematography inﬁnite possibility impact different kind coherent narrative structure coherent structure fact movie work art like movie language normally consist isolated unrelated sentence instead collocated structured coherent group sentence refer coherent structured group sentence discourse use word discourse herence refer relationship sentence make real discourse coherence different random assemblage sentence chapter ing example discourse news article conversation thread social medium wikipedia page favorite novel make discourse coherent create text take random tence different source paste coherent discourse certainly real discourse exhibit local local ence andglobal coherence let consider way real discourse global locally coherent sentence clause real discourse relate nearby sentence systematic way consider example hobb john take train paris istanbul like spinach sequence incoherent unclear reader second sentence follow ﬁrst like spinach train trip fact reader effort try ﬁgure discourse coherent french spinach shortage fact hearer try identify connection suggest human discourse comprehension involve need establish kind coherence contrast following coherent example jane take train paris istanbul attend chapter iscourse coherence second sentence give reason jane action ﬁrst sentence ture relationship like reason hold text unit call coherence relation coherent discourse structure coherence relation coherence relation introduce section second way discourse locally coherent virtue coherent discourse entity salient discourse focus forth multiple entity call entity base coherence consider follow incoherent passage salient entity wildly swing john jenny piano store living room jenny piano john want buy piano living room jenny want buy piano go piano store nearby living room second ﬂoor ﬁnd like piano buy hard ﬂoor entity base coherence model measure kind coherence track salient entity discourse example center theory grosz thecentering theory inﬂuential theory entity base coherence keep track entity discourse model salient point salient entity likely pronominalize appear prominent syntactic position like subject object center theory transition sentence maintain salient entity consider coherent one repeatedly shift entity entity grid model coherence barzilay lapata commonly entity grid model realize intuition center theory framework entity base coherence introduce section finally discourse locally coherent topically coherent nearbytopically coherent sentence generally topic use similar ulary discuss topic topically coherent discourse draw single semantic ﬁeld topic tend exhibit surface property know lexical cohesion halliday hasan sharing identical lexical cohesion cally relate word nearby sentence example fact word house chimney garret closet window belong semantic ﬁeld appear sentence share identical word shingle cue tie discourse winter build chimney shingle side house tight shingled plaster house garret closet large window addition local coherence adjacent nearby sentence course exhibit global coherence genre text associate particular conventional discourse structure academic article section describe methodology result story follow conventional plotline motif persuasive essay particular claim try argue essay express claim structured set premise support argument demolish potential counterargument introduce version kind global coherence care local global coherence discourse herence property write text coherence detection play oherence relation task require measure quality text example coherence help pedagogical task like essay grading essay quality measurement try grade write human essay somasundaran feng lai tetreault coherence help summarization know coherence relationship sentence help know select tion finally detect incoherent text play role mental health task like measure symptom schizophrenia kind disorder language ditman kuperberg elvev bedi iter coherence relation recall introduction difference passage jane take train paris istanbul like spinach jane take train paris istanbul attend conference reason coherent reader form connection tween sentence second sentence provide potential reason ﬁrst sentence link hard form connection text span discourse speciﬁed set coherence relation relation section describe commonly model coherence relation associated corpora rhetorical structure theory rst penn discourse treebank pdtb rhetorical structure theory commonly model discourse organization rhetorical structure theory rst mann thompson rst relation deﬁne rst span text generally nucleus satellite nucleus unit nucleus satellite central writer purpose interpretable independently satellite central generally interpretable respect nucleus symmetric relation hold nucleus example rst coherence relation deﬁnition adapt rst treebank manual carlson marcu reason nucleus action carry animate agent satellite reason nucleus nuc jane take train paris istanbul satshe attend conference elaboration satellite give additional information detail situation present nucleus nuc dorothy kansas satshe live midst great kansas prairie evidence satellite give additional information detail situation present nucleus information present goal convince reader accept information present nucleus nuc kevin sathis car park chapter iscourse coherence attribution satellite give source attribution instance report speech nucleus satanalyst estimate nuc sale store decline quarter list multinuclear relation series nucleus give contrast explicit comparison nuc billy bone mate nuc long john quartermaster rst relation traditionally represent graphically asymmetric satellite relation represent arrow satellite nucleus kevin car park outsideevidence talk coherence large text consider chical structure coherence relation figure show rhetorical ture paragraph marcu text scientiﬁc american magazine distant percent far sun earth slim atmospheric blanket mar experience frigid weather condition surface temperature typically average degree celsius degree fahrenheit equator dip degree near pole midday sun tropical latitude warm thaw ice occasion liquid water form way evaporate instantly low atmospheric pressure title evidence background distant orbit percent far sun earth slim atmospheric mar experience frigid weather elaboration additional surface temperature typically average degree celsius degree list dip degree near contrast midday sun tropical latitude warm thaw ice occasion explanation argumentative liquid water form way evaporate low atmospheric pressure figure discourse tree scientiﬁc american text marcu note asymmetric relation represent curved arrow satellite nucleus leave fig tree correspond text span sentence clause phrase call elementary discourse unit oredu rst unit edu refer discourse segment unit correspond arbitrary span text determine boundary edu important task extract coherence relation roughly speak think oherence relation segment analogous constituent sentence syntax section generally draw parse algorithm infer discourse ture corpora discourse coherence model rst discourse treebank carlson large available discourse corpus sist english language document select penn treebank rst parse large set distinct relation group class rst treebank exist spanish german basque dutch brazilian portuguese braud see example coherence clearly coherence relation play role summarization information extraction example nucleus text presumably express important information satellite drop summary penn discourse treebank pdtb penn discourse treebank pdtb second commonly dataset pdtb embody model coherence relation miltsakaki prasad prasad pdtb labeling lexically ground instead ask annotator directly tag coherence relation text span give list discourse connective word signal discourse relation like discourse connective ora result text word mark coherence relation text span connective span annotate fig phrase result signal causal relationship pdtb call ﬁrst sentence italic sentence bold jewelry display department store cluttered uninspired merchandise fake result marketer faux gem steadily lose space department store fashionable rival cosmetic maker july environmental protection agency impose gradual ban virtually use asbestos implicit result remain use cancer cause asbestos outlaw coherence relation mark explicit discourse connective pdtb annotate pair neighboring sentence explicit signal like annotator ﬁrst choose word phrase signal case result label sense example biguous discourse connective annotator mark ausal emporal sense ﬁnal dataset contain roughly explicit relation implicit relation fig show example major semantic class fig show tagset unlike rst discourse treebank integrate pairwise coherence relation global tree structure span entire discourse pdtb annotate span pair level make commitment respect high level discourse structure treebank similar method language show example chinese discourse treebank zhou xue chinese small percentage explicit discourse connective english discourse relation mark explicit chapter iscourse coherence class type example temporal synchronous parishioner michael angel stop chat church door member implicit tower ﬁve man woman pull rhythmically rope attach ﬁve bell ﬁrst sound contingency reason unlike ruder breeden appear position agenda implicit mer white house aide work closely congress savvy way washington comparison contrast want removal perceive barrier investment japan deny real barrier expansion conjunction actor stand outside character clear odd literally stand head figure high level semantic distinction pdtb sense hierarchy temporal comparison juxtaposition opposition precedence succession contrast juxtaposition opposition expectation contra expectation concession contingency expansion reason result cause justiﬁcation hypothetical general unreal present past factual present speciﬁcation equivalence generalization condition relevance implicit conjunction disjunction choose tive figure pdtb sense hierarchy level type subtype type subtype type commonly implicit argument classiﬁcation type italic rare implicit labeling compare english annotator label corpus directly mapping pair sentence sense tag start lexical discourse tor conn为 设立了图们江发展基金 order promote development tumen river region south korea donate million dollar establish tuman river development fund discourse treebank share task multilingual course parse xue discourse structure parse give sequence sentence automatically determine coherence relation task call discourse parsing thoughdiscourse parsing pdtb assign label leaf span build iscourse structure parse tree rst edu segmentation rst parse rst parsing generally stage ﬁrst stage edu segmentation extract start end edu output stage labeling like following rambo say acre property san fernando valley price million late actor erroll flynn live edus roughly correspond clause early model edu segmentation ﬁrst run syntactic parser post process output modern system generally use neural sequence model supervise gold edu segmentation dataset like rst discourse treebank fig show example architecture simpliﬁed algorithm lukasik predict token break input sentence pass encoder pass linear layer softmax produce sequence indicate start edu layersoftmaxedu break figure predict edu segment beginning encoded text rst parse tool build rst coherence structure discourse long base syntactic parse algorithm like shift reduce parsing marcu modern rst parser eisenstein draw neural syntactic parser see chapter representation learning build representation span train parser choose correct shift reduce action base gold parse training set describe shift reduce parser parser state sist stack queue produce structure take series action state action include push ﬁrst edu queue stack create single node subtree merge subtree stack lis coherence relation label dis nuclearity direction pop root operation remove ﬁnal tree stack fig show action parser take build structure fig chapter iscourse coherence elabelab american telephone telegraph say lay technician effective nov worker install maintain repair private branch exchange large intracompany telephone network figure example rst discourse tree edus attr andelab discourse relation label arrow indicate nuclearitie discourse relation rst discourse parsing study adopt discrete syntax feature propose statistical model feed neural network model braud braud approach model syntax tree explicit way require discrete syntax parse output input rst parsing approach suffer error propagation problem syntax tree produce supervise syntax parsing model error propagate discourse parse model problem extremely input discourse parsing different distribution training datum supervise syntax parser recently zhang suggest alternative method extract syntax feature afﬁne dependency parser dozat manning method give competitive performance relation extraction actually represent syntax tree implicitly reduce error propagation problem work investigate implicit syntax feature extraction approach rst parsing dition propose transition base neural model task able incorporate feature ﬂexibly exploit hierarchical directional lstms lstms encode text enhance transition base model dynamic oracle base propose model study effectiveness propose implicit syntax feature conduct experiment standard rst course treebank carlson evaluate performance propose base baseline ﬁnde model able achieve strong performance apply dynamic oracle evaluate effectiveness implicit syntax feature extract afﬁne dency parser result implicit syntax feature effective give well performance explicit tree lstm code release public apache license summary mainly follow contribution work propose base neural rst discourse parse model dynamic oracle compare different syntactic integration approach propose rest paper organize follow section describe propose model include transition base neural model dynamic oracle strategy implicit syntax feature extraction approach section present experiment evaluate model section show related work finally section draw conclusion transition base discourse parse follow eisenstein exploit transition base framework rst discourse parsing framework conceptually simple ﬂexible support arbitrary feature widely number nlp task zhu dyer zhang addition transition base model formalize certain task predict sequence action essential similar sequence sequence model propose recently bahdanau following ﬁrst describe transition system rst discourse parsing introduce neural network model encoder decoder part respectively thirdly present propose dynamic oracle strategy aim enhance transition base model introduce integration method implicit syntax feature finally describe training method neural network model transition base system transition base framework convert structural learning problem sequence action tion key point transition system transition system consist part state action state store partially parse result action control state transition figure example rst discourse tree show edus figure stack queue action relation table example transition base system rst discourse parsing initial state state ﬁnal state represent result kind action transition system remove ﬁrst edu queue stack form single node subtree merge subtree stack lis discourse relation label relation nuclearity nuclear satellite root pop tree stack mark decoding complete stack hold subtree queue give rst tree show figure generate follow action sequence attr elab elab table show decode process detail way naturally convert rst discourse parse predict sequence transition action line include state step action refer tree encoder decoder previous transition base rst discourse parse study exploit statistical model design discrete feature sagae heilman sagae wang work propose transition base neural model rst discourse parsing follow encoder decoder framework give input sequence edus encoder compute input tation decoder predict step action condition encoder output encoder follow hierarchical lstms encode source edu input ﬁrst layer represent sequencial word inside edus second layer represent sequencial edus give input sentence ﬁrst represent word form pos tag concatenate neural embedding way input vector ﬁrst layer lstm emb apply lstm directly obtain lstm second layer lstm build sequential edus ﬁrst obtain suitable tion edu compose span word inside certain sentence assume edu word apply ﬁrst layer lstm obtain tion calculate edu representation average pooling shw edu representation ready apply second layer lstm directly result lstm figure parse example fig shift reduce parser figure use encoder decoder architecture encoder represent input span word edus hierarchical bilstm ﬁrst bilstm layer represent word inside edu second represent edu sequence give input sentence word sente usual static embedding combination character embedding tag contextual embedding result input word representation sequence result word level bilstm sequence hwvalue bilstm edu span bilstm output representation represent average pooling shw second layer use input compute ﬁnal representation sequence edu representation bilstm decoder feedforward network wthat output action obase concatenation subtree stack plus ﬁrst edu queue representation edu queue directly encoder hide vector represent partial tree compute average pool encoder output edus tree ihe iscourse structure parse training ﬁrst map rst gold parse tree sequence oracle action use standard cross entropy loss train system action state sand oracle action ﬁrst compute decoder output apply softmax probability compute cross entropy loss lce rst discourse parser evaluate test section rst discourse bank gold edus end end rst pareval metric marcu standard ﬁrst transform gold rst tree right branch nary tree report metric tree label span label nucleus relation metric computing micro average span document marcu morey pdtb discourse parse pdtb discourse parsing task detect pdtb coherence relation span call shallow discourse parsing task involvesshallow discourse parsingﬂat relationship text span tree rst parsing set subtask pdtb discourse parsing lay lin ﬁrst complete system separate task explicit task implicit task connective find discourse connective disambiguate non discourse use find span connective label relationship span assign relation adjacent pair sentence system propose task take pair adjacent sentence input assign coherence relation sense label output setup low lin assume gold sentence span boundary assign adjacent span second level pdtb tag remove rare tag show italic fig simple strong algorithm task represent span bert embedding layer hide state correspond position cls token pass single layer tanh feedforward network softmax sense classiﬁcation nie task address task ing discourse connective non discourse use example pitler nenkova point word andi discourse connective link clause elaboration expansion relation non discourse conjunction selling pick previous buyer bail position aggressive short seller anticipate decline move favorite color blue chapter iscourse coherence similarly discourse connective indicate temporal relation simply non discourse adverb meaning modifying asbestos ﬁber crocidolite unusually resilient enter lung brief exposure cause symptom decade later researcher say form asbestos kent cigarette ﬁlter cause high percentage cancer death group worker expose year ago researcher report determine word discourse connective special case word sense disambiguation early work disambiguation show pdtb high level sense class disambiguate high accuracy syntactic feature gold parse tree pitler nenkova recent work perform task end end word input bilstm crf bio output conn conn task pdtb span identiﬁed sequence model ﬁnd rst edus bilstm sequence model pretraine contextual embed bert input muller simple heuristic pretty line ﬁnding span relation completely single sentence span adjacent sentence argument sentence biran mckeown centering entity base coherence second way discourse coherent virtue entity idea point discourse entity salient discourse coherent continue discuss entity appear early functional guistic psychology discourse chafe kintsch van dijk soon way computational model section introduce model kind entity base coherence center theory grosz entity base entity grid model barzilay lapata center center theory grosz theory discourse salience andcentering theory discourse coherence model discourse salience center propose give point discourse entity discourse model salient center model discourse coherence center propose discourse adjacent sentence continue maintain salient entity coherent shift forth multiple entity continue shift technical term theory follow text grosz exactly propositional content different salience help understand main center intuition john go favorite music store buy piano frequent store year excited ﬁnally buy piano arrive store close enter entity coherence john go favorite music store buy piano store john frequent year excited ﬁnally buy piano close john arrive text differ entity john store realize sentence discourse intuitively coherent grosz point discourse clearly individual john describe action feeling discourse contrast focus ﬁrst john store john store lack aboutness ﬁrst discourse center theory realize intuition maintain representation utterance backward look center ofun denote rep look centerresent current salient entity focus discourse interpret forward look center ofun denote setforward look center potential future salient entity discourse entity evoke unany serve salient entity following utterance set forward look center rank accord factor like discourse salience grammatical role example subject high rank object high rank grammatical role highest rank forward look center preferred center cpis kind prediction entity talk utterance talk entity entity salient instead use algorithm centering present brennan deﬁne intersentential relationship pair utterance unand depend relationship show fig undeﬁned continue smooth shift retain rough shift figure center transition rule brennan follow rule algorithm rule element realize pronoun utterance realize pronoun rule transition state order continue prefer retain prefer smooth shift prefer rough shift rule capture intuition pronominalization include zero anaphora common way mark discourse salience multiple pronoun utterance realize entity previous utterance pronoun realize backward center pronoun rule capture intuition discourse continue center tity coherent one repeatedly shift center transition table base factor backward look center cbi discourse entity prefer new utterance hold continue relation speaker talk entity go continue talk chapter iscourse coherence entity retain relation speaker intend shift new entity future utterance place current entity low rank shift relation speaker shift new salient entity let walk start repeat show representation utterance process john go favorite music store buy piano excited ﬁnally buy piano arrive store close day close john arrive grammatical role hierarchy order sentence music store piano john undeﬁned sentence pianog john john result continue transition continue complete example leave exercise reader entity grid model centering embody particular theory entity mention lead ence salient entity appear subject position pronominalize discourse salient mean continue mention entity way entity grid model barzilay lapata alternative way entity grid capture entity base coherence instead have theory entity grid model machine learn induce pattern entity mention discourse coherent model base entity grid dimensional array sent distribution entity mention sentence row represent tence column represent discourse entity version entity grid model focus nominal mention cell represent possible appearance entity sentence value represent entity appear grammatical role grammatical role subject object send implementation barzilay lapata subject passive represent lead representation characteristic thematic role fig barzilay lapata show grid text show fig row sentence second column entity trial show trial appear ﬁrst sentence direct object sentence oblique appear middle sentence column entity microsoft show appear ject sentence appear object preposition entity appear multiple time record highest rank grammatical tion compute entity grid require extract entity enter entity coherence computational linguistics volume number pattern encode feature vector appropriate perform relate ranking classiﬁcation task entity grid discourse representation text represent entity grid distribution discourse entity text sentence follow miltsakaki kukich assume unit analysis traditional sentence main clause accompany subordinate adjunct clause row grid correspond sentence column correspond discourse entity discourse entity mean class coreferent noun phrase explain section coreferent entity identiﬁed occurrence discourse entity text correspond grid cell contain information presence absence sequence sentence addition entity present give sentence grid cell contain information syntactic role information express way constituent label thematic role information grammatical relation ﬁgure prominently entity base theory local coherence section serve logical point departure grid cell correspond string set category reﬂecte entity question subject object entity absent sentence signal gap grammatical role information extract output broad coverage dependency parser lin briscoe carroll state art statistical parser collin charniak discuss information compute experiment section table illustrate fragment entity grid construct text table text contain sentence grid column length consider instance grid column entity trial trial present sentence oand sentence note grid table take coreference resolution account entity appear different linguistic form example microsoft corp microsoft company map single entry grid column introduce microsoft table table fragment entity grid noun phrase represent head noun grid cell correspond grammatical role subject object trial microsoft evidence competitor market product brand case netscape software tactic government suit earning sxo xso soo figure entity grid text fig entity list head noun cell represent entity appear subject object absent figure barzilay lapata barzilay lapata model local coherence table summary augment syntactic annotation grid computation justice conduct anti trust microsoft increasingly attempt crush ois accuse try forcefully buy competitive unseat establish srevolve pressure merge browser sclaim commonplace good economically ﬁle civil curb violation sherman scontinue increase noun attest different grammatical role sentence default role high grammatical ranking subject rank high object turn rank high rest example entity microsoft mention twice sentence grammatical role microsoft corp company represent sin grid table entity grid feature vector coherent text exhibit certain regularity reﬂecte grid topology regularity formalize center theory constraint transition local focus adjacent sentence grid coherent text likely dense column column gap microsoft table sparse column consist gap market andearning table expect entity correspond dense column subject object characteristic pronounce low coherence text inspire center theory analysis revolve pattern local entity transition local entity transition sequence represent entity occurrence syntactic role nadjacent sentence local transition easily obtain grid continuous subsequence column transition certain probability give grid instance probability transition grid table compute ratio frequency divide total number transition length text view distribution deﬁne transition type step represent text ﬁxed set transition sequence standard feature vector notation grid render jof document dicorrespond feature vector mis number predeﬁne entity transition grid algorithm experiment section furthermore allow eration large number transition potentially uncover novel entity distribution pattern relevant coherence assessment coherence relate task note considerable latitude available specify transition type include feature vector transition give length frequent transition document collection example figure discourse entity mark annotate grammatical tion figure barzilay lapata resolution cluster discourse entity chapter parse sentence grammatical role result grid column dense like column microsoft dicate entity mention text sparse column like column earning indicate entity mention rarely entity grid model coherence measure pattern local entity sition example department subject sentence tione sentence transition transition sequence gnwhich extract continuous cell column transition probability probability grid fig occur time total transition length fig show distribution transition length text fig show ﬁrst document computational linguistics volume number introduce grid construction linguistic dimension central research issue develop entity base model coherence determine source linguistic knowledge essential accurate prediction encode succinctly discourse representation previous approach tend agree feature entity distribution relate local coherence disagreement lie way feature model study alternative encoding mere duplication previous fort poesio focus linguistic aspect parameterization interested automatically construct model account putational learn issue consider alternative representation exploration parameter space guide consideration linguistic importance parameter accuracy automatic computation size result feature space linguistic focus property entity bution tightly link local coherence time allow multiple interpretation encoding process computational consideration prevent consider discourse representation compute reliably ing tool instance experiment granularity utterance sentence versus clause available clause separator introduce substantial noise grid construction finally exclude representation explode size feature space increase datum require train model entity traction accurate computation entity class key compute ingful entity grid previous implementation entity base model class erent noun extract manually miltsakaki kukich karamani poesio option model obvious solution identify entity class employ automatic coreference resolution tool determine noun phrase refer entity document current approach recast coreference resolution classiﬁcation task pair nps classiﬁe coreferre base constraint learn annotated corpus separate clustering mechanism coordinate possibly contradictory pairwise classiﬁcation construct partition set nps experiment employ cardie coreference resolution system system decide nps coreferent exploit wealth lexical grammatical semantic positional feature train muc data set yield state art performance measure table example feature vector document representation transition length give syntactic category figure feature vector represent document transition length document text fig figure barzilay lapata transition probability feature machine learning model model text classiﬁer train produce human label coherence score example human label text coherent herent datum expensive gather barzilay lapata introduce simplify innovation coherence model train self supervision train distinguish natural original order sentence discourse chapter iscourse coherence modiﬁed order randomized order turn evaluation section evaluate neural entity base coherence entity base coherence model neural model introduce section generally evaluate way human rate coherence document train classiﬁer predict human rating categorial high low high mid low continuous good evaluation use end task mind like essay grading human rater correct deﬁnition ﬁnal label alternatively expensive human label end task mind use natural text self supervision self supervision pair natural discourse pseudo document create change ordering naturally order discourse coherent random permutation lin successful coherence algorithm fer original ordering self supervision implement way sentence order crimination task barzilay lapata compare document random permutation sentence model consider correct original mute test pair rank original document higher give kdocument compute npermutation result knpair original document permutation use training testing sentence insertion task chen document remove nsentence create copy document sinserte position task decide ndocument original ordering distinguish original position sfrom position insertion hard discrimination compare document differ sentence finally sentence order reconstruction task lapata document randomize sentence train model correct order give kdocument compute npermutation result inknpair original document permutation use training testing reordering course hard task simple classiﬁcation representation learning model local coherence kind local coherence topical semantic ﬁeld coherence discourse cohere talk topic subtopic draw semantic ﬁeld ﬁeld pioneer series unsupervised model kind coherence use lexical cohesion halliday hasan lexical cohesion sharing identical semantically relate word nearby sentence morris hirst compute lexical chain word like pine bush tree trunk occur discourse relate roget thesaurus category link category show number density chain correlate topic structure texttile algorithm hearst texttile compute cosine neighboring text span normalize dot product vector raw word count show sentence paragraph epresentation learning model local coherence subtopic high cosine sentence neighboring subtopic early model lsa coherence method foltz ﬁrst use embedding model coherence sentence sine lsa sentence embed compute embedding sentence sby sum embedding word deﬁne overall coherence text average similarity pair adjacent sentence coherence modern neural representation learn coherence model begin draw intuition early unsupervised model learn tence representation measure change neighboring tence new model draw idea pioneer barzilay lapata self supervision unlike coherence relation model train hand label representation rst pdtb model train distinguish natural discourse unnatural discourse form scramble order sentence representation learning discover feature matter ordering aspect coherence present model local coherence discriminator lcd like early model lcd compute coherence text erage coherence score consecutive pair sentence unlike early unsupervised model lcd self supervise model train discriminate consecutive sentence pair training document assume ent construct incoherent pair consecutive pair positive example negative incoherent partner sentence sii sentence uniformly sample document fig describe architecture model take sentence pair return score high score coherent pair give input sentence pair sandt model compute sentence embedding sentence embedding algorithm concatenate feature pair concatenation vector difference absolute value difference element wise product pass layer feedforward network output coherence score model train coherence score higher real pair negative pair formally training objective corpus cof documents consist list sentence expectation respect negative sample distribution ditione give sentence sithe algorithms sample negative sentence chapter lsa embedding compute apply svd document matrix cell weight log frequency normalize entropy ﬁrst dimension chapter iscourse coherence figure architecture lcd model document coherence show computation score pair sentence sandt figure uniformly sentence document lis loss function take score positive pair negative pair goal encourage high low fig use margin loss margin parameter useful baseline algorithm high performance measure perplexity train rnn language model datum compute log likelihood sentence siin way give precede context conditional log likelihood context marginal log hood difference value tell precede context improve predictability predictability measure coherence training model predict long context consecutive pair tence result strong discourse representation example language model train contrastive sentence objective predict text distance sentence improve performance discourse ence task iter language model style model generally evaluate method tion evaluate rst pdtb coherence relation task global coherence discourse cohere globally level pair tence consider story example narrative structure story old kind global coherence study inﬂuential morphology folktale propp model discourse structure russian folktale kind plot grammar model include set character category call dramatis personae like hero villain donor helper set event call function like villain commit kidnapping donor test hero lobal coherence pursue occur particular order component propp show plot fairy tale study represent sequence function different tale choose different subset function order lakoff show propp model amount discourse grammar story recent computational work layson demonstrate proppian function induce corpora folktale text detect event similar action story bamman show generalization dramatis personae induce movie plot summary wikipedia model induce latent personae feature like action character take villain gle action villain foil arrest descriptive word villain evil section introduce kind global discourse structure widely study computationally ﬁrst structure argument way people attempt convince persuasive essay offer claim support premise second somewhat related structure scientiﬁc paper way author present goal result relationship prior work paper argumentation structure ﬁrst type global discourse structure structure argument analyze people argumentation computationally call argumentation mining mining study argument date aristotle rhetoric describe component good argument pathos appeal emotion pathos listener ethos appeal speaker personal character logo logical ethos logo structure argument discourse structure study argumentation focus logo particularly building training annotated dataset persuasive essay argument reed stab gurevych peldszus stede habernal gurevych musi corpora ple include annotation argumentative component like claim central claim component argument controversial needs support premise premise reason give author persuade reader support attack claim premise argumentative relation themargumentative relation like support attack consider follow example persuasive essay stab gurevych ﬁrst sentence present claim bold present premise support claim give premise support premise museum art gallery provide well understanding art internet museum art gallery tail description term background history author provide see artwork online watch eye picture online texture dimensional structure art important study example argumentative relation support support support fig show structure complex ment argumentation mining clearly relate rhetorical structure kind coherence relation argument tend local chapter iscourse coherence stab gurevych parse argumentation structure cloning example illustrate know argumentative relation important separate argument paragraph example show argument component frequently exhibit precede text unit relevant argument helpful recognize argument component type example precede course connector like consequently signal subsequent claim discourse marker like furthermore indicate premise formally precede token argument component start token tiare deﬁne tokens cover argument component sentence body paragraph illustrate contra argument argumentative attack relation admittedly cloning bemisused formilitary purpose claim example premise premise premise paragraph begin claim attack stance author support bypremise second sentence sentence include premise defend stance author premise attack claim premise support premise paragraph conclusion restate major claim marize main aspect essay sum permit cloning bear risk like misuse military purpose claim strongly believe technology beneﬁcial humanity majorclaim likely thistechnologybear important cure significantly improve lifecondition claim conclusion essay start attack claim follow restatement major claim sentence include claim summarize portant point author argumentation figure show entire argumentation structure example essay figure structure example essay arrow indicate argumentative denote argumentative support relation circlehead attack relation dashedline indicate relation encode stance attribute claim denote figure argumentation structure persuasive essay arrow indicate argumentation relation ther support arrowhead attack circlehead denote premise figure stab gurevych sive essay single main claim premise spread text local coherence coherence relation algorithm detect argumentation structure include classiﬁer distinguish claim premise non argumentation relation siﬁer decide span support attack relation peldszus stede main focus computational work preliminary effort annotate detect rich semantic relationship park cardie hidey detect tation scheme large scale structure argument like argument example argumentation scheme orargument cause effect orargument consequence feng hirst important line research study argument structure feature associate success persuasiveness argument habernal gurevych tan hidey aristotle logo related discourse structure aristotle ethos pathos technique particularly relevant detection mechanism sort persuasion example scholar investigate linguistic realization persuasion feature study social scientist like reciprocity people return favor social proof people follow choice authority people inﬂuence power scarcity people value thing scarce bring persuasive argument cialdini rosenthal mckeown show feature combine argumentation structure predict inﬂuence social medium althoff find linguistic model reciprocity authority predict success online request semisupervised model yang detect mention scarcity commitment social identity predict success peer peer lending form stede schneider comprehensive survey argument ummary structure scientiﬁc discourse scientiﬁc paper speciﬁc global structure course paper author indicate scientiﬁc goal develop method tion provide evidence solution compare prior work popular annotation scheme model rhetorical goal argumentative ingmodel teufel teufel inform theargumentative zone idea scientiﬁc paper try knowledge claim new piece knowledge add repository ﬁeld myers sentence scientiﬁc paper assign tag fig show shorten example label sentence category description example aim statement speciﬁc research goal hypothesis current aim process examine role training play tagging process ownmethod new knowledge claim work order useful purpose follow extension ownresult measurable objective outcome curve generally upward trend lie far backoff error rate use work work use framework allocation transfer control whittaker gapweak lack solution ﬁeld problem produce experimental evidence suggest simple model lead overestimate support work support current work support current similar describe rie merialdo broadly similar conclusion antisupport clash result theory periority result challenge claim figure example label argumentative zone labelset teufel teufel teufel develop label corpora scientiﬁc article computational linguistic chemistry sion train standard sentence classiﬁcation architecture assign label summary chapter introduce local global model discourse coherence discourse arbitrary collection sentence coherent factor discourse coherent coherence relation sentence entity base coherence topical coherence set coherence relation andrhetorical relation pose relation rhetorical structure theory rst hold span text structure tree shift reduce parse algorithm generally assign structure penn discourse treebank pdtb label relation pair span label generally assign sequence model base coherence capture intuition discourse entity continue mention entity sentence sentence tere theory family model describe salience model chapter iscourse coherence discourse entity coherence achieve virtue keep discourse entity salient discourse entity grid model give way compute entity realization transition lead coherence different genre different type global coherence persuasive essay claim premise extract ﬁeld argument mining scientiﬁc article structure relate aim method result comparison historical note coherence relation arise independent development number ar include hobb idea coherence relation play inferential role hearer investigation mann thompson discourse structure large text approach coherence relation tion include segment discourse representation theory sdrt asher sdrt caride baldridge linguistic discourse model polanyi scha polanyi polanyi wolf gibson argue coherence structure include crossed bracketing impossible represent tree propose graph representation instead compendium relation propose literature find hovy rst parsing ﬁrst propose marcu early work rule base focus discourse marker marcu creation rst discourse treebank carlson carlson marcu enable wide variety machine learning algorithm begin shift reduce parser marcu decision tree choose action continue wide variety machine learn parse method soricut marcu sagae hernault feng hirst surdeanu joty chunker sporleder lapata subba eugenio integrate sophisticated semantic information rst parsing eisenstein ﬁrst apply neural model rst parse neural model lead modern set neural rst model braud inter alia neural segmenter wang neural pdtb parsing model eisenstein qin qin barzilay lapata pioneer idea self supervision ence train coherence model distinguish true ordering sentence random permutation ﬁrst apply paradigm neural representation neural self supervise model follow jurafsky logeswaran lai tetreault iter aspect global coherence global topic structure text way topic shift course document barzilay lee introduce hmm model capture topic coherence later work expand intuition soricut marcu elsner louis nenkova jurafsky relationship explicit implicit discourse connective fruitful research marcu echihabi ﬁrst propose use sen historical note tence explicit relation help provide training datum implicit relation remove explicit relation try predict way ing performance implicit connective idea reﬁne sporleder lascaride pitler rutherford xue tionship way create discourse aware representation dissent algorithm nie create task predict explicit discourse marker sentence representation learn good task function powerful sentence representation discourse task idea entity base coherence arise multiple ﬁeld functional linguistic chafe psychology discourse processing kintsch van dijk roughly contemporaneous work grosz sidner joshi colleague grosz address focus attention conversational participant maintain discourse unfold deﬁne level focus entity relevant entire discourse say global focus entity locally focus central particular utterance say immediate focus sidner scribe method track immediate discourse foci use resolve pronoun demonstrative noun phrase distinction rent discourse focus potential focus predecessor forward look center center theory respectively root center approach lie paper joshi kuhn joshi weinstein address relationship immediate focus inference require integrate current utterance discourse model grosz integrate work prior work sidner grosz lead manuscript center widely circulate remain unpublished grosz collection center paper appear walker karamanis poesio deep exploration center parameterization history section chapter use center coreference grid model entity base coherence ﬁrst propose barzilay lapata draw early work lapata barzilay extend barzilay lapata additional feature elsner charniak feng lin model project entity global graph discourse guinaudeau strube mesgar strube convolutional model capture long range entity dependency nguyen joty theory discourse coherence algorithm ing discourse level linguistic phenomenon include verb phrase ellipsis ping asher kehler tense interpretation lascaride asher kehler kehler extensive investigation relationship coherence relation discourse connective find knott dale useful survey discourse processing structure include stede webber andy kehler write discourse chapter ﬁrst edition book starting point second edition chapter remnant andy lovely prose edition coherence chapter iscourse coherence exercise finish center theory processing utterance process algorithm mark coherent select editorial column favorite newspaper determine discourse structure sentence portion problem encounter help superﬁcial cue speaker include discourse connective structure conversation intricate complex joint activity conversation ture true conversation conversation people conversation people language model understand structure human conversation important social science linguistic task cept introduce study human conversation useful tool analyze human llm conversation draft initial stub chapter introduce different kind conversational structure annotate computationally property human conversation conversational phenomenon place human converse conversation human machine different consider go conversation human travel agent human client excerpt fig need travel day want travel need meeting ﬂye city seattle time like leave pittsburgh hmm think option non stop right non stop today ﬁrst depart pgh arrive seattle time second ﬂight depart pgh arrive seattle ﬂight depart pgh arrive seattle ﬂight night depart arrive seattle air ﬂight say return yeah end day non stop act actually day week friday hmm consider stay extra day til sunday sunday figure phone conversation human travel agent human client passage frame overlap chapter onversation structure turn dialogue sequence turn single contribution turn speaker dialogue game turn turn turn fig turn consist sentence like short single word long multiple sentence turn structure important implication spoken dialogue human know stop talk client interrupt system perform role know stop talk user make correction issue come llm system know start talk example time conversation speaker start turn immediately speaker ﬁnishe long pause people usually predict person ﬁnish talk speak language model detect user speak process utterance respond task call endpointe endpointing point detection challenging noise people pause middle turn speech act key insight conversation originally philosopher wittgenstein work fully austin utterance dialogue kind action perform speaker action monly call speech act ordialogue act taxonomy consist speech act major class bach harnish constative commit speaker case answer claim conﬁrme deny disagree state directive attempt speaker addressee advise ing forbid invite order request commissive commit speaker future course action promising planning vowing bet oppose acknowledgment express speaker attitude hearer respect cial action apologize greeting thank accept acknowledgment user ask person dialogue system turn sic issue irective ask question require answer way issue irective sense system say day want travel system politely e user answer contrast user state constraint like need travel issue onstative user thank system issue cknowledgment speech act express important component intention speaker writer say say ground dialogue series independent speech act collective act perform speaker hearer like collective act important participant establish agree call common groundcommon ground stalnaker speaker ground utterance ropertie human conversation ing mean acknowledge hearer understand speaker clark people need ground non linguistic action reason elevator ton light press acknowledge elevator call essentially ground action push button norman grounding important hearer need indicate speaker hasnotsucceede perform action hearer problem standing indicate problem speaker mutual derstanding eventually achieve closure achieve clark schaefer introduce idea joint linguistic act contribution phase call presentation contribution tance ﬁrst phase speaker present hearer utterance perform sort speech act acceptance phase hearer ground utterance indicate speaker understanding achieve method hearer use ground speaker utterance clark schaefer discuss continuum method order weak strong continued attention show continue attend remain satisﬁed presentation contribution start relevant contribution acknowledgment nod say continuer like huh yeah like ment likethat great demonstration demonstrate understand mean example reformulate paraphrasing utterance rative completion utterance display display verbatim presentation example kind ground occur travel agent conversation ground explicitly say agent ground repeat person say utterance agent repeat demonstrate understanding client notice client answer question agent begin question imply new question addition old question indicate client agent successfully understand answer question particular fragment example acknowledgment example fragment want boston baltimore huh word huh continuer call acknowledgment continuer kenor abackchannel continuer short optional utterance acknowledge backchannel content utterance require acknowledgment yngve jefferson schegloff ward tsukahara subdialogue dialogue structure conversation structure consider example local structure speech act discuss ﬁeld conversation analysis sack analysis question set expectation answer proposal follow acceptance orrejection ompliment nice jacket rise downplayer old thing pair call adjacency pair adjacency chapter onversation structure compose ﬁrst pair second pair schegloff expectation help system decide action dialogue act follow immediately second pair part separate sequence jefferson sequence dialogue example utterance correction subdialogue subdialogue litman litman allen chu carroll carberry act actually day week friday hmm consider stay extra day til sunday sunday question prior discourse agent look return ﬂight agent answer question realize consider stay til sunday mean client probably like change plan ﬁnding return ﬂight sequence clariﬁcation question form logue request response especially common dialogue system speech recognition error cause system ask ﬁcation repetition like follow user go unknown word system let go user go hong kong system ﬂight addition sequence question presequence like presequence lowing example user start question system capability train reservation make request user train reservation system yes user great like reserve seat train new york initiative conversation completely control participant ple reporter interview chef ask question chef respond reporter case conversational initiative carbonell initiative nickerson normal human human dialogue common initiative shift forth participant answer question ask conversation new rection ask question respond ask clarify say lead conversation sort way interaction mixed initiative carbonell mixed initiative norm human human conversation difﬁcult dialogue system primitive dialogue system tend use system initiative system ask question user answer user initiative like simple search engine user speciﬁes query system passively respond modern large language model base dialogue system come close mixed tive completely natural initiative switch get right important goal modern ialog act corpora inference implicature inference important dialogue understanding consider client response repeat day want travel need meeting notice client fact answer agent question client merely mention meeting certain time license agent infer client mention meeting inform agent travel date speaker expect hearer draw certain inference word speaker communicate information present utter word kind example point grice theory conversational implicature mean implicature lar class licensed inference grice propose enable hearer draw inference conversation guide set maxim general heuristic play guide role interpretation conversational utterance maxim maxim relevance say speaker attempt relevant relevance utter random speech act client mention meeting agent reason relevance mention meeting agent know precondition have meeting web conferencing place meeting hold maybe meeting reason travel people like arrive day meeting agent infer ﬂight subtle characteristic human conversation turn speech act ing dialogue structure initiative implicature reason ﬁcult build dialogue system carry natural conversation human challenge active area dialogue system research dialog act corpora idea speech act grounding combine single kind action call dialogue act tag represent interactive function sentence dialogue act tag dialog act analyze human human conversation human llm conversation nature participant type dialogue task base task base inﬂuence development dialogue act tagset figure show domain speciﬁc tagset task people scheduling meeting tag speciﬁc domain scheduling ugg proposal particular date meet ccept eject acceptance rejection proposal date tag general function like larify request user clarify ambiguous proposal figure show tagset restaurant recommendation system fig show tag label sample dialogue system young example show content dialogue act slot ﬁller communicate number general domain independent dialogue act tagset damsl dialogue act markup layer architecture inspire chapter onversation structure tag example thank thank greet hello dan introduce bye alright bye request look suggest thirteenth seventeenth june reject friday book day accept saturday sound ﬁne request good day week init want appointment givereason meeting afternoon feedback okay deliberate let check calendar confirm okay wonderful clarify okay mean tuesday digress meet lunch eat lot ice cream motivate visit subsidiary munich garbage oop figure high level dialogue act meeting scheduling task system jekat tag sys user description hello open dialogue info inform info request request value give reqalt request alternative confirm explicitly conﬁrm confreq implicitly conﬁrm request value select implicitly conﬁrm request value affirm afﬁrm info negate negate correct value deny deny bye close dialogue figure dialogue act restaurant recommendation system young sys user column indicate act valid system output user input respectively work clark schaefer allwood allwood utterance tag type function forward look function like speech act function backward look function like ground ing look interlocutor previous utterance allen core walker carletta core ialog act corpora utterance dialogue act look eat find type restaurant look restaurant type food restaurant food like italian near italian near museum roma nice italian restaurant near roma type restaurant food italian near museum reasonably price moderate yes roma moderate price roma pricerange moderate phone number number roma roma phone thank goodbye bye figure sample dialogue system young dialogue act fig forward look function statement claim speaker info request question speaker check question conﬁrme information influence addressee bach directive open option weak suggestion listing option action directive actual command influence speaker austin commissive offer speaker offer subject conﬁrmation commit speaker commit conventional opening greeting closing farewell thanking thanking respond thank backward look function damsl focus relationship terance previous utterance speaker include accept jecte proposal damsl focus task orient dialogue ing repair act backward look function agreement speaker response previous proposal accept accept proposal accept accept proposal maybe accept reject proposal reject reject proposal reject reject proposal hold put response usually subdialogue answer answer question understanding speaker understand previous signal non speaker understand signal speaker understand ack demonstrate continuer assessment repeat rephrase demonstrate repetition reformulation completion demonstrate collaborative completion fig show labeling part sample conversation version chapter onversation structure damsl forward backward tag assert need travel info req ack day want travel assert answer need meeting info req ack ﬂye city assert answer seattle info req ack time like leave pittsburgh check hold hmm think option non stop accept ack right assert non stops today info req assert open option ﬁrst depart pgh arrive seattle time second ﬂight depart pgh arrive seattle ﬂight depart pgh arrive seattle accept ack ﬂight night check ack assert ack depart arrive seattle air ﬂight ack figure potential damsl labeling beginning conversational fragment fig abadi agarwal barham brevdo chen citro corrado davis dean devin ghemawat fellow harp irving ard jia jozefowicz kaiser kudlur levenberg man monga moore murray olah schuster shlens steiner sutskever talwar tucker vanhoucke van egas vinyal den wattenberg wicke zheng flow large scale machine learning heterogeneous system software available abney schapire singer boost ply tag attachment emnlp vlc agarwal subramanian nenkova roth evaluation name entity ence workshop computational model reference anaphora coreference aggarwal zhai survey text classiﬁcation gorithm aggarwal zhai eds mining text datum springer agichtein gravano snowball extract relation large plain text collection ceeding acm tional conference digital brarie agirre banea cardie cer diab gonzalez agirre guo lopez gazpio itxalar mihalcea rigau uria wiebe task semantic textual similarity english ish pilot interpretability agirre diab cer gonzalez agirre task pilot mantic textual similarity agirre martinez learn class class selectional preference conll ahia kumar gonen sai mortensen smith tsvetkov language cost tokenization era commercial language model emnlp aho ullman theory parsing translation compile volume prentice cover sandwich proof mcmillan breiman theorem annal probability allen general ory action time artiﬁcial telligence allen core draft damsl dialog act markup eral layer unpublished manuscript allen hunnicut klatt text speech mitalk system cambridge versity press allwood activity base approach pragmatic burg paper theoretical tic allwood nivre ahls semantic matic linguistic feedback nal semantic althoff danescu mizil jurafsky ask favor case study success altruistic request icwsm kwak ahn semaxis lightweight framework characterize speciﬁc word semantic timent acl anastasopoulos neubig cross lingual bedding speak english acl anthropic release note system prompt com release system prompt antoniak mimno evaluate stability embed base word similarity tacl aone bennett uating automate manual sition anaphora resolution gy acl ardila branson davis kohler meyer henretty morais saunders tyer weber common voice massively multilingual speech corpus lrec ariel accessibility ory overview sanders schilperoord spooren ed text representation linguistic psycholinguistic aspect morphology typology historical retrospect state art prospect ford arora lewis fan kahn reason lic private datum retrieval base system tacl artetxe schwenk massively multilingual sentence bedding zero shot cross lingual transfer tacl asher reference abstract object discourse study guistic philosophy slap kluwer asher lascarides ic conversation cambridge versity press atal hanauer speech analysis synthesis prediction speech wave jasa austin thing word harvard university press kiros hinton layer normalization neurips workshop baayen word frequency distribution springer baayen piepenbrock guliker celex lexical database release rom linguistic data consortium university pennsylvania utor babbage passage life philosopher longman baccianella esuli tiani sentiwordnet enhance lexical resource ment analysis opinion mining lrec bach harnish tic communication speech act mit press backus syntax semantic propose ternational algebraic language zurich acm gamm conference information processing ing international conference information processing paris unesco backus transcript tion answer session wexelblat history ming language page demic press bibliography bada eckert evans cia shipley sitnikov baumgartner cohen spoor blake hunter concept annotation craft corpus bmc bioinformatics baevski zhou mohamed auli framework self supervise learning speech representation neurip volume bagga baldwin algorithm score coreference chain lrec workshop tic coreference bahdanau cho gio neural machine tion jointly learn align translate iclr bahdanau chorowski serdyuk brakel gio end end base large vocabulary speech recognition icassp bahl mercer speech assignment tistical decision algorithm ing ieee international symposium information theory bahl jelinek mercer maximum hood approach continuous speech recognition ieee transaction pattern analysis machine ligence bai jones ndousse askell chen dassarma drain fort ganguli henighan joseph vath kernion conerly showk elhage dodd hernandez hume johnston kravec lovitt nanda olsson amodei brown clark dlish olah mann plan train helpful harmless assistant ment learn human bajaj campos craswell deng ando xiaodong liu majumder mcnamara tra nguye rosenberg song stoica tiwary wang marco human generate machine reading comprehension dataset neurips baker fillmore lowe berkeley framenet project cole acl baker dragon tem overview ieee tion assp baker stochastic modeling automatic speech derstande reddy recognition academic press baldridge asher hunter annotation robust parsing discourse structure unrestricted text zeitschrift sprachwissenschaft bamman lewke soor annotated dataset coreference english literature lrec bamman smith learn latent sona ﬁlm character acl bamman popat shen annotated dataset ary entity naacl hlt banerjee lavie teor automatic metric evaluation improved tion human judgment ceeding acl workshop trinsic extrinsic evaluation measure rization banko cafarella soderland broadhead etzioni open information extraction web ijcai chen haddow heaﬁeld hoang espl gomis forcada kamran kirefu koehn ortiz jas pla sempere ram sarr ıa strelec thompson waites gin zaragoza paracrawl web scale acquisition parallel corpora acl bar hillel present tus automatic translation guage alt advance computer academic press barker nominal provide criterion identity rathert alexiadou ed semantic nominalization language framework mouton barrett mesquita ochsner gross experience emotion annual view psychology barzilay lapata ele local coherence base approach acl barzilay lapata ele local coherence base approach computational guistic barzilay lee catch drift probabilistic content el application generation summarization hlt naacl eagon inequality application tistical estimation probabilistic function markov process model ecology bulletin american mathematical society baum petrie tical inference probabilistic tion ﬁnite state markov chain annal mathematical statistic bazell spondence fallacy structural guistic hamp householder austerlitz ed study member english department istanbul university reprint reading linguistics university chicago press bean riloff corpus base identiﬁcation anaphoric noun phrase acl bean riloff pervise learning contextual role knowledge coreference tion hlt naacl beckman ayer guideline tobi belling unpublished manuscript ohio state university http research phonetic beckman hirschberg tobi annotation tion manuscript ohio state versity bedi carrillo cecchi slezak sigman mota ribeiro javitt copelli corcoran mate analysis free speech dict psychosis onset high risk youth npj schizophrenia bejˇcek haji haji kettnerov kol mikulov ırovsk nedoluzhko panevov pol akov zik anov prague pendency treebank technical report institute formal plied linguistic charles university prague lindat clarin ital library institute formal applied linguistic charles university prague bellegarda latent mantic analysis framework span language modeling rospeech bellegarda exploit tent semantic information cal language modeling proceeding ieee bellman dynamic ming princeton university press bellman eye cane autobiography world entiﬁc singapore bender benderrule name language study matter blog post bender friedman mcmillan major guide write datum statement natural language processing edu data bender koller climb nlu ing form understanding age datum acl bengio courville cent representation ing review new tive ieee transaction pattern analysis machine intelligence bengio ducharme cent neural probabilistic language model neurips bengio ducharme vincent jauvin neural abilistic language model jmlr bengio lamblin popovici larochelle greedy layer wise training deep work neurip bengio schwenk sen ecal morin gauvain neural probabilistic language el innovation machine learning springer bengtson roth derstande value feature coreference resolution emnlp bennett elfner syntax prosody interface annual review linguistic bentivogli cettolo federico federmann machine translation human evaluation vestigation evaluation base post editing relation rect assessment icslt berant chou frostig liang semantic parsing freebase question answer pair emnlp berg kirkpatrick burkett klein empirical tigation statistical signiﬁcance nlp emnlp berger della pietra della pietra maximum tropy approach natural language processing computational tic lin strap path base pronoun lution cole acl bergsma lin goebel discriminative learning selectional preference bele text emnlp bergsma lin goebel distributional identiﬁcation non referential pronoun acl bethard cleartk timeml minimalist approach tempeval bhat bhat shrivastava sharma join hand exploit monolingual bank parsing code mix datum eacl bianchi suzgun tanasio rottger jurafsky hashimoto zou safety tune llama lesson improve safety large language model follow tion iclr bickel referential density discourse syntactic typology language bickmore trinh olafsson asadi le cruz patient consumer safety risk conversational assistant medical information observational study siri alexa google tant journal medical internet search bikel miller schwartz weischedel nymble high performance learn ﬁnder anlp biran mckeown pdtb discourse parse tagging task tagger approach sigdial bird klein loper natural language processing python bisani ney strap estimate conﬁdence val asr performance evaluation icassp bishop pattern tion machine learning springer bisk holtzman thomason andreas bengio chai lapata lazaridou nisnevich pinto turian experience ground language emnlp bizer lehmann kobilarov auer becker cyganiak hellmann dbpedia crystallization point web datum web semantic science service agent world wide web kuhn learn structured perceptron coreference resolution latent antecedent non local feature acl black taylor chatr generic speech synthesis system cole black abney flickinger gdaniec grishman rison hindle ingria linek klavans liberman marcus roukos torini strzalkowski procedure quantitatively e syntactic coverage english grammar speech natural guage workshop blei dan latent dirichlet tion jmlr blodgett barocas daum iii wallach language technology power critical survey bias nlp acl blodgett green demographic dialectal variation social medium case study african american english emnlp blodgett racial disparity natural language processing case study cial media african american glish fat workshop kdd bloomﬁeld introduction study language henry holt company bloomﬁeld language versity chicago press bobrow kaplan kay norman thompson winograd gus frame drive dialog system artiﬁcial telligence bobrow norman principle memory schemata bobrow collins ed representation understanding academic press boersma weenink praat phonetic puter version computer program retrieve bojanowski grave joulin mikolov enrich word vector subword information tacl bollacker evans paritosh sturge taylor freebase collaboratively create graph database structure man knowledge sigmod bibliography bolukbasi chang zou saligrama kalai man computer programmer woman homemaker debiase word embedding neurip bommasani hudson adeli altman arora von arx bernstein bohg bosselut skill brynjolfsson buch card castellon terji chen creel davis demszky ahue doumbouya durmus ermon etchemendy yarajh fei fei finn gale gillespie goel goodman grossman guha hashimoto henderson witt hong hsu huang icard jain rafsky kalluri karamcheti keeling khani tab koh krass krishna kuditipudi mar ladhak lee lee leskovec levent malik ning mirchandani mitchell munyikwa nair narayan narayanan newman nie niebles nilforoshan nyarko ogut orr itriou park piech lance potts raghunathan reich ren rong roohani ruiz ryan sadigh sagawa thanam shih srinivasan tamkin taori thomas tram wang wang xie yasunaga zaharia zhang zhang zhang zhang zheng zhou liang tie risk foundation model arxiv booth probabilistic representation formal language ieee conference record tenth annual symposium ing automata theory borge analytical guage john wilkins inquisition university texas press trans ruth simms bostrom durrett byte pair encoding suboptimal guage model pretraine emnlp bourlard morgan connectionist speech recognition hybrid approach kluwer bradley terry rank analysis incomplete block design method paired comparison biometrika tnt statistical speech tagger anlp brent popat och dean large guage model machine tion emnlp conll braud coavoux søgaard cross lingual rst discourse parsing eacl essai emantique science des signiﬁcation hachette brennan friedman pollard center proach pronouns acl brin extract pattern relation world wide web proceeding world wide web database international workshop number lnc springer brockmann lapata evaluate combine proache selectional preference acquisition eacl broschart tongan differently linguistic typology brown cocke della pietra della pietra linek lafferty mercer roossin tical approach machine tion computational linguistic brown della pietra della pietra mercer mathematic statistical chine translation parameter mation computational linguistic brown mann ryder subbiah kaplan wal neelakantan shyam sastry askell wal herbert oss krueger henighan child ramesh ziegler ter hesse chen sigler litwin gray chess clark berner dlish radford sutskever amodei language el shot learner neurip volume brysbaert warriner kuperman ness rating thousand erally know english word mas behavior research method zheng open source mandarin speech pus speech recognition line cocosda proceeding buchholz marsi share task multilingual dency parsing conll hirst evaluate wordnet base sure lexical semantic ness computational linguistic bullinaria levy extract semantic representation word occurrence statistic computational study behavior search method bullinaria levy extract semantic sentation word occurrence statistic stop list stemming svd behavior research method caliskan bryson narayanan semantic rive automatically language corpora contain human like bias science callison burch osborne koehn evaluate role bleu machine translation research eacl canavan graff perlen callhome english speech guistic data consortium cao kang wang sun instruction mining tion datum selection tune large language model conference language modeling carbonell cai artiﬁcial intelligence proach computer assist tion ieee transaction machine system cardie case base approach knowledge acquisition domain speciﬁc sentence analysis aaai cardie domain speciﬁc knowledge acquisition tual sentence analysis sis university massachusetts amherst available sci technical report cardie wagstaff noun phrase coreference ing emnlp vlc carletta dahlb reithinger walker standard dialogue coding natural guage processing technical port dagstuhl seminar port dagstuhl seminar number carlini tramer lace jagielski herbert oss lee roberts brown song erlingsson extract training datum large language model usenix curity symposium usenix security carlson reference kind english thesis sity massachusetts amherst ward carlson marcu course tag manual technical report isi isi carlson marcu okurowski build discourse tag corpus framework rhetorical structure theory sigdial carreras arquez introduction share task semantic role labeling conll chafe givenness trastiveness deﬁniteness subject topic point view topic demic press chamber navytime event time order raw text chamber cassidy ell bethard dense event order multi pass chitecture tacl chamber jurafsky improve use pseudo word evaluate selectional ence acl chamber jurafsky template base information tion template acl chan jaitly vinyal listen tend spell neural network large vocabulary conversational speech recognition icassp chandioux erationnel pour duction automatique des bulletins destin grand public meta chang manning sutime library recognize normalizing time expression lrec chang samdani roth constrain tent variable model coreference resolution emnlp chang samdani zovskaya sammon roth illinois coref system share task conll chaplot salakhutdinov knowledge base word sense disambiguation topic model aaai charniak statistical ing context free grammar word statistic aaai hendrickson cobson perkowitz equation speech ging aaai che guo qin liu multilingual dependency base syntactic mantic parsing conll chen tically aware coreference evaluation metric ijcnlp chen fisch weston borde read pedia answer open domain tion acl chen manning fast accurate dependency parser ing neural network emnlp chen snyder lay incremental text ing online hierarchical ranking emnlp conll chen wang zhang zhou liu chen liu wang zhao wei neural codec guage model zero shot text speech synthesizer ieee trans taslp chen goodman empirical study smooth technique language modeling computer speech language chen shi qiu huang adversarial multi criterion learning chinese word tation acl cheng dong pata long short term memory network machine ing emnlp cheng durmus sky marked personas natural language prompt sure stereotype language model acl chiang hierarchical base model statistical machine translation acl chinchor hirschman lewis evaluate message understanding system sis message ing conference computational guistic chiticariu danilevsky reiss zhu temt declarative text ing enterprise naacl hlt ume chiticariu reiss rule base information traction dead long live base information extraction tem emnlp nichols name entity recognition rectional lstm cnns tacl cho van merri cehre bahdanau bougares schwenk bengio learn phrase representation ing rnn encoder decoder tical machine translation emnlp choe charniak parse language modeling emnlp choi palmer te transition base dependency parsing acl choi palmer transition base semantic role beling predicate argument cluster proceeding acl workshop relational el semantic choi tetreault stent depend dependency parser comparison base evaluation tool acl chomsky model description language ire transaction information ory chomsky cal structure linguistic theory plenum chomsky syntactic ture mouton chomsky formal tie grammar luce bush galanter ed book mathematical psychology volume wiley chomsky lecture ernment binding foris chorowski bahdanau cho bengio end end continuous speech recognition attention base recurrent result neurips deep learning representation learn workshop chou lee juang minimum error rate ing base good string model icassp christodoulopoulos ter steedman decade unsupervised pos duction far come emnlp chu liu short arborescence direct graph science sinica chu carroll carberry collaborative response generation planning dialogue computational linguistic bibliography church stochastic part program noun phrase parser unrestricted text anlp church stochastic part program noun phrase parser unrestricted text icassp church unix poet slide elsnet summer school unpublished paper church gale comparison enhance ture delete estimation od estimate probability english bigrams computer speech language cialdini inﬂuence psychology persuasion morrow cieri miller walker fisher corpus resource generation text lrec clark principle trast constraint language quisition macwhinney mechanism language tion lea clark language cambridge university press clark fox tree spontaneous speaking cognition clark schaefer contribute discourse cognitive science clark yallop troduction phonetic ogy edition blackwell clark choi collins garrette kwiatkowski nikolaev palomaki tydi benchmark information seek question answering typologically diverse language tacl clark luong manning electra training text encoder tor generator iclr clark manning entity centric coreference resolution model stacking acl clark manning deep reinforcement learning mention rank coreference clark manning improve coreference resolution learn entity level distribute resentation acl clark curran osborne bootstrappe pos tagger unlabelled datum conll cmu carnegie mellon nounce dictionary carnegie mellon kosaraju ian chen jun kaiser plappert tworek hilton nakano hesse man training veriﬁer solve math word problem arxiv preprint coccaro jurafsky ward well integration tic predictor statistical language modeling icslp coenen reif yuan kim pearce egas berg visualize e geometry bert neurips coleman introduce speech language processing bridge university press collin head drive tical model natural language parsing thesis university pennsylvania philadelphia collobert weston fast semantic extraction novel neural network architecture acl collobert weston uniﬁed architecture natural language processing deep neural network multitask learning icml collobert weston bottou karlen kavukcuoglu kuksa natural language processing scratch jmlr comrie language universal linguistic typology tion blackwell conneau khandelwal goyal chaudhary zek guzm grave ott zettlemoyer stoyanov unsupervised cross lingual representation learning scale acl conneau khanuja zhang axelrod dalmia riesa rivera bapna fleur shot learning evaluation universal tion speech ieee slt connolly burger day machine learn proach anaphoric reference ceeding international ference new method guage processing nemlap cooley tukey algorithm machine culation complex fouri rie mathematic computation cooper liberman borst sion audible visible pattern basis research ception speech proceeding ofthe national academy sciences cordier factor analysis correspondence cole core ishizaki moore nakatani reithinger traum tutiya port workshop course resource initiative cal report chiba corpus project chiba japan costa juss cross elebi elbayad heaﬁeld fernan kalbassi lam licht maillard sun wang wenzek blood akula barrault gonzalez hansanti hoffman jarrett sadagopan rowe spruit tran andrews ayan bhosale edunov fan gao goswami guzm koehn mourachko ropers saleem schwenk wang nllb team guage leave scale center machine translation arxiv cover thomas element information theory wiley covington tal algorithm dependency ing proceeding annual acm southeast conference cox analysis binary datum chapman hall london craven kumlien construct biological knowledge basis extract information text source crawford trouble bias keynote neurips croft typology sal cambridge university press crosbie shutova duction head essential anism pattern matching context learn arxiv preprint cross huang base constituency parse structure label system provably optimal dynamic oracle emnlp cruse mean guage introduction semantic pragmatic oxford university press second edition cucerzan large scale name entity disambiguation base wikipedia datum emnlp conll cui yuan ding yao zhu xie xie lin liu sun ultrafeedback boost guage model scale icml dahl sainath hinton improve deep neural network lvcsr rectiﬁed linear unit dropout icassp dahl deng acero context dependent pre trained deep neural network large vocabulary speech nition ieee transaction dio speech language ing dahl magesh suzgun large legal tion proﬁle legal hallucination large language model journal legal analysis dai semi supervised sequence learning neurip das chen hoo amazon sentiment ing small talk web efa barcelona meeting david selfridge eye ear computer proceeding ire institute radio engineer davidson bhattacharya weber racial bias hate speech abusive language tion dataset workshop abusive language online davy expand zon historical linguistic million word corpus ical american english corpora davy pedia corpus million cle billion word adapt wikipedia davy corpus contemporary american glish coca billion word davis morgenstern ortiz ﬁrst winograd schema challenge magazine davis biddulph ashek automatic recognition spoken digit jasa davis mermelstein comparison parametric sentation monosyllabic word recognition continuously speak sentence ieee transaction assp deerwester dumais furnas harshman landauer lochbaum andl streeter computer mation retrieval latent tic structure patent deerwester dumais landauer furnas harshman indexing tent semantic analysis jasis copet synnaeve adi high ﬁdelity ral audio compression tmlr dejong overview frump system lehnert ringle ed strategy natural language processing lea dene design ation mechanical speech ognizer university college don journal british tion radio engineer appear ion paper fry deng hinton kingsbury new type deep neural network learn speech nition related application overview icassp deng byrne hmm word phrase alignment tistical machine translation emnlp deni baldridge joint determination anaphoricity coreference resolution integer programming naacl hlt deni baldridge cialize model rank coreference resolution emnlp deni baldridge global joint model coreference tion name entity classiﬁcation procesamiento del lenguaje ral derose grammatical egory disambiguation statistical optimization computational guistic devlin chang lee toutanova bert training deep bidirectional former language understanding naacl hlt eugenio center theory italian pronominal system cole eugenio discourse function italian subject tering approach cole dia oliva antonialli gome fight hate speech silence drag queen ﬁcial intelligence content tion risk lgbtq voice online sexuality culture abercrombie bergman spruit hovy boureau rieser ipate safety issue sational framework tooling arxiv dinan fan williams banek kiela weston queen powerful igate gender bias dialogue eration emnlp ditman kuperberg building coherence work explore breakdown link clause boundary schizophrenia journal guistic dixon sorensen thain vasserman measure mitigate unintended bias text classiﬁcation aaai acm conference ethic ety dixon maxey nal analog synthesis continuous speech diphone method segment assembly ieee tion audio electroacoustic bethard moen improve implicit semantic role labeling predict semantic frame argument ijcnlp doddington automatic uation machine translation quality gram occurrence tic hlt dodge gururangan card schwartz smith work improve ing experimental result emnlp dodge sap marasovi agnew ilharco eveld mitchell gardner document large webtext corpora case study sal clean crawl corpus emnlp dong lapata guage logical form neural tention acl dorr machine translation vergence formal description propose solution computational linguistic dostert experiment machine translation language fourteen essay mit press doumbouya jurafsky manning tversky ral network psychologically sible deep learning tiable tversky similarity arxiv preprint dowty word meaning montague grammar bibliography dozat manning deep biafﬁne attention neural pendency parsing iclr dozat manning simple accurate semantic dependency parsing acl dozat manning stanford graph base ral dependency parser conll share task proceeding conll share task gual parsing raw text versal dependency dror baumer bogomolov reichart ity analysis natural language cessing testing signiﬁcance multiple dataset tacl dror peled cohen shlomov reichart cal signiﬁcance testing natural language processing volume synthesis lecture human guage technology morgan claypool dryer haspelmath ed world atlas language structure online max planck stitute evolutionary ogy leipzig available online durrett klein easy victory uphill battle erence resolution emnlp durrett klein joint model entity analysis ence type link tacl earley efﬁcient free parse algorithm thesis carnegie mellon university pittsburgh earley efﬁcient free parse algorithm cacm edmond optimum ing journal research national bureau standards edunov ott auli grangier understand translation scale emnlp efron tibshirani introduction bootstrap crc press egghe untangle herdan law heap law mathematical informetric argument jasist eisner new probabilistic model dependency parsing exploration cole ekman basic emotion dalgleish power ed handbook cognition tion nanda olsson henighan joseph mann askell bai chen erly dassarma drain ganguli hatﬁeld dodds hernandez jones kernion lovitt ndousse amodei brown clark kaplan candlish olah mathematical framework circuit white paper elman find structure time cognitive science elsner austerweil niak uniﬁed local global model discourse ence naacl hlt elsner charniak coreference inspire coherence modeling acl elsner charniak tend entity grid speciﬁc feature acl elvev foltz weinberger goldberg quantify incoherence speech automated methodology novel application nia schizophrenia research emami jelinek ral syntactic language model chine learning emami trichelair trischler suleman schulz cheung knowref coreference corpus remove der number cue cult pronominal anaphora tion acl erk simple base model selectional ence acl ethayarajh contextual contextualize word tion compare geometry bert elmo ding emnlp ethayarajh duvenaud hirst derstande linear word analogy acl ethayarajh duvenaud hirst understand desirable word embed tion acl ethayarajh jurafsky utility eye user critique nlp leaderboard emnlp ethayarajh zhang hzad stanford human ence dataset etzioni cafarella downey popescu shaked land weld unsupervised name entity extraction web mental study artiﬁcial intelligence evans word class world language booij lehmann mugdan ed morphology handbook tion word formation mouton fader soderland etzioni identify relation open information extraction emnlp fan bhosale schwenk kishky goyal baine celebi wenzek chaudhary goyal birch liptchinsky edunov auli joulin english centric multilingual chine translation jmlr fant speech tion research ing vetenskaps akad stockholm sweden fant acoustic theory speech production mouton fant glottal ﬂow model interaction journal ic fant speech acoustic phonetic kluwer fast chen bernstein empath understand topic signal large scale text chi fauconnier turner way think conceptual blending mind hide complexity basic book feldman ballard connectionist model property cognitive science fellbaum wordnet electronic lexical database mit press feng hirst fye argument scheme acl feng hirst linear time discourse parser constraint editing acl feng lin hirst impact deep hierarchical course structure evaluation text coherence cole fernande dos santo milidi latent ture perceptron feature tion unrestricted coreference olution conll ferragina scaiella fast accurate annotation short text wikipedia page ieee software ferro gerber mani heim wilson tide standard annotation temporal expression technical port mitre ferrucci introduction watson ibm nal research development field tsvetkov centric contextual affective analysis acl fillmore proposal cerne english preposition dinneen annual round ble volume monograph series language linguistic georgetown university press fillmore case case bach harms ed universal linguistic theory holt rinehart winston fillmore frame mantic understanding quaderni semantica fillmore valency mantic role concept deep structure case agel eichinger eroms wig heringer lobin ed dependenz und valenz ein internationale handbuch der forschung chapter walter gruyter fillmore acl time achievement award ter language computational linguistic fillmore baker frame approach semantic sis heine narrog ed oxford handbook linguistic analysis oxford sity press fillmore johnson petruck background framenet international journal lexicography finkelstein gabrilovich tia rivlin solan man ruppin place search context concept ited acm transaction tion system finlayson inferring propp function semantically annotated text journal ican folklore firth synopsis tic theory study linguistic analysis philological ciety reprint palmer select paper firth longman harlow fitt unisyn lexicon project speech analysis synthesis perception springer flanagan ishizaka shipley synthesis speech dynamic model vocal cord vocal tract bell tem technical journal foland martin nlp task amr parsing lstm base current neural network foland martin dependency base tic role label convolutional neural network sem foltz kintsch dauer measurement textual coherence latent tic analysis discourse process nekoto marivate matsila fasubaa kolawole bohungbe akinola muhammad kabongo osei freshia niyongabo ogayo ahia essa adeyemi selinga okegbemi tinus tajudeen degila ogueji siminyu kreutzer webster ali orife ezeani dangana kamper elsahar duru kioko murhabazi van biljon whitenack fuluchi emezue dossou sibanda bassey olabiyi ramkilowan faderin bashir ipatory research low resource machine translation case study african language finding emnlp author use forall symbol represent masakhane community fox discourse structure anaphora write sational english cambridge francis ˇcera frequency analysis english age houghton mifﬂin boston franz brent gram belong https gram belong friedman hendry value sensitive design shape technology moral imagination mit press friedman hendry borning survey value sensitive design method foundation trend computer interaction duration sity physical correlate tic stress jasa fry theoretical pect mechanical speech tion journal british tion radio engineer appear ion paper dene furnas landauer gomez dumais vocabulary problem system communication nication acm gabow galil spencer tarjan efﬁcient rithm ﬁnde minimum ning tree undirected direct graph combinatorica gaddy stern klein go neural constituency parser analysis naacl hlt gage new algorithm datum compression users journal gale church wrong add oostdijk haan ed corpus base research guage rodopi gale church program align sentence bilingual corpora acl gale church program align sentence bilingual corpora computational linguistic gale church yarowsky sense discourse hlt gale church yarowsky work tistical method word sense ambiguation aaai fall symposium probabilistic approach ural language gao hoppe thite derman foster nabeshima black phang presser golding leahy pile dataset diverse text language ing arxiv preprint garg schiebinger jurafsky zou word embedding quantify year gender ethnic stereotype proceeding national academy sciences garside claws tagging system garside leech sampson ed computational analysis english bibliography garside leech mcenery corpus annotation man gebru morgenstern chione vaughan lach daum iii ford datasheet dataset arxiv gehman gururangan sap choi smith altoxicityprompt evaluate ral toxic degeneration language model finding emnlp gemmeke ellis man jansen lawrence moore plakal ritter audio set ontology human label dataset audio event icassp gerber chai yond nombank study implicit argument nominal predicate acl ger schmidhuber cummin learn continual prediction lstm neural computation geva schuster berant levy transformer forward layer key value orie emnlp gil syntactic category cross linguistic variation sal grammar ogel comrie ed approach typology word class mouton gildea jurafsky tomatic labeling semantic role acl gildea jurafsky automatic labeling semantic role computational linguistic gildea palmer necessity syntactic parsing predicate argument recognition acl gile kuhn williams dynamic recurrent neural network theory cation ieee trans neural netw learn syst gillick cox statistical issue comparison speech recognition algorithm icassp girard justesse langue franc les diff erente signiﬁcation des mots qui passent pour synonime laurent paris giuliano pretation word association statistical association methodsfor mechanized documentation symposium proceeding ington usa march gov nistpub legacy gladkova drozd suoka analogy base tection morphological mantic relation word ding work naacl student research workshop glenberg son symbol grounding meaning comparison dimensional embodied theory meaning journal memory language godfrey holliman daniel switchboard telephone speech corpus search development icassp goel byrne minimum baye risk automatic speech nition computer speech guage goffman frame analysis essay organization ence harvard university press gonen goldberg stick pig debiase method cover systematic gender bias word embedding remove naacl hlt goodfellow bengio courville deep ing mit press goodman bit progress language modeling extend version technical report machine learning apply statistic group microsoft research redmond gould panda thumb penguin group goyal gao chaudhary chen wenzek ishnan ranzato guzm fan uation benchmark low resource multilingual machine tion tacl graff broadcast news speech language model corpus proceeding darpa speech recognition workshop grave sequence tion recurrent neural network icassp grave generate quence recurrent neural work arxiv grave fern andez gomez schmidhuber nectionist temporal classiﬁcation label unsegmented sequencedata recurrent neural network icml grave fern andez wicki bunke ber unconstrained line handwriting recognition rent neural network neurip grave jaitly end end speech recognition recurrent neural network icml grave mohame hinton speech recognition deep recurrent neural network icassp grave schmidhuber framewise phoneme classiﬁcation bidirectional lstm neural network architecture ral network grave wayne helka neural ture chine arxiv gray vector quantization ieee transaction assp green wolf sky laughery ball automatic question swerer proceeding western joint computer conference greenberg tative approach cal typology language tional journal american tic greenberg ellis insight speak guage glean phonetic scription switchboard corpus icslp greene rubin automatic grammatical tagging english department tic brown university providence rhode island greenwald mcghee schwartz ing individual difference implicit cognition implicit association test journal personality cial psychology grenager manning unsupervised discovery cal verb lexicon emnlp grice logic tion cole morgan ed speech act syntax mantic volume academic press grice note logic conversation cole syntax tic volume academic press grishman sundheim design evaluation grosz representation use focus system derstande dialog gin kaufmann grosz representation use focus dialogue derstande thesis sity california berkeley grosz joshi stein provide uniﬁed count deﬁnite noun phrase glish acl grosz joshi stein center framework model local coherence discourse computational tic gruber study lexical relation thesis mit friedrich kuhn apply occam razor transformer base dency parsing work sary iwpt guinaudeau strube graph base local coherence ing acl gundel hedberg zacharski cognitive status form refer expression discourse language gururangan marasovi swayamdipta agy downey smith stop pretraine adapt language model domain task acl gusﬁeld algorithm string tree sequence bridge university press guyon elisseeff introduction variable feature selection jmlr haber poesio sesse polyseme sense similarity predication acceptability contextualise embed tance sem habernal gurevych argument ing analyze predict vincingness web argument bidirectional lstm acl habernal gurevych argumentation mining generate web discourse tional linguistic haghighi klein simple coreference resolution rich syntactic semantic feature emnlp hajishirzi zilles weld zettlemoyer jointcoreference resolution entity link multi pass sieve emnlp hajiˇc build tactically annotate corpus prague dependency treebank karolinum hajiˇc morphological tagging datum dictionary naacl hajiˇc ciaramita son kawahara mart arquez meyers nivre pad stran surdeanu xue zhang share task syntactic semantic cie multiple language conll hakkani oﬂazer statistical ical disambiguation agglutinative language journal computer humanity halliday hasan cohesion english longman glish language series title hamilton clark leskovec jurafsky induce domain speciﬁc sentiment lexicon unlabeled corpora emnlp hamilton leskovec jurafsky diachronic word embedding reveal statistical law semantic change acl hannun sequence modeling ctc distill hannun maas sky pass large vocabulary continuous speech recognition directional current dnn arxiv preprint harris study building block speech jasa harris morpheme utterance language harris distributional ture word harris string analysis sentence structure mouton hague hashimoto srivastava namkoong liang fairness demographic repeat loss minimization icml hastie tibshirani friedman element statistical learning springer hatzivassiloglou mckeown predict semantic tation adjective acl hatzivassiloglou wiebe effect adjective tion gradability sentence jectivity cole clark new acquire new mation process sion journal verbal learning verbal behaviour hawkin deﬁniteness indeﬁniteness study ence grammaticality prediction croom helm ltd hayashi yamamoto oue yoshimura watanabe toda takeda zhang tan espnet tts uniﬁed reproducible able open source end end text speech toolkit icassp lee lewis moyer deep semantic role beling work acl liu liu lyu zhao xiao liu wang liu wang dureader chinese machine reading comprehension dataset real world application workshop machine reading question answering heaﬁeld kenlm fast small language model query workshop statistical machine translation heaﬁeld pouzyrevsky clark koehn able modiﬁed kneser ney language model estimation acl heap information trieval computational ical aspect academic press hearst automatic sition hyponyms large text corpora cole hearst automatic sition hyponyms large text corpora cole hearst texttile menting text multi paragraph subtopic passage computational linguistic hearst automatic ery wordnet relation baum wordnet electronic lexical database mit press heim semantic deﬁnite indeﬁnite noun phrase thesis university massachusetts amherst heinz stevens property voiceless tive consonant jasa hellrich buechel hahn model word emotion historical language quantity beat suppose stability seed word lection joint sighum shop computational linguistic cultural heritage social ence humanity literature bibliography hellrich hahn bad company neighborhood neural embed space consider henderson description base parse connectionist network thesis university vania philadelphia henderson induce history representation broad coverage statistical parsing hlt henderson discriminative training neural network cal parser acl henderson romoff brunskill jurafsky pineau tematic reporting energy carbon footprint machine ing journal machine learning research henderson jurafsky hashimoto lemley liang foundation model fair use jmlr henderson sinha gontier fried lowe pineau ical challenge data drive logue system aaai acm ethic society conference hendrickx kim kozareva nakov pad pennacchiotti romano szpakowicz task multi way classiﬁcation semantic relation pair nominal international shop semantic evaluation hendrix thompson slocum language ing canonical verb semantic model proceeding herdan type token matic mouton hermann kocisky stette espeholt kay leyman blunsom teach machine read prehend neurips hernault prendinger verle ishizuka hilda discourse parser support vector machine classiﬁcation logue discourse hidey musi hwang san mckeown lyze semantic type claim premise online persuasive forum workshop argument mining hill reichart korhonen evaluate mantic model genuine ilarity estimation computational linguistic learn tribute representation concept cogsci hinton osindero teh fast learning algorithm deep belief net neural tation hinton srivastava krizhevsky sutskever salakhutdinov ing neural network prevent adaptation feature detector arxiv preprint hirschman light breck burger deep read reading comprehension system acl hirst anaphora ral language understanding vey number lecture note computer science springer verlag hirst semantic tion resolution ity cambridge university press hjelmslev prologomena theory language university wisconsin press translate francis whitﬁeld original danish edition hobb resolve pronoun reference lingua hobb coherence coreference cognitive science hobb appelt bear israel kameyama stickel tyson tus cascade ﬁnite state ducer extract information natural language text roche schabe ed finite state language processing mit press hochreiter schmidhuber long short term memory neural computation hofmann probabilistic latent semantic indexing hofmann kalluri sky king generate covertly racist decision ple base dialect nature holtzman buys forbe choi curious case neural text ation iclr hopcroft ullman introduction automata ory language computation addison wesley hou markert strube unrestricted bridge lution computational linguistic dionysius thrax technai sextus piricus koerner asher ed concise history language science sevier science hovy parsimonious proﬂigate approach question discourse structure tion proceeding national workshop natural guage generation hovy marcus palmer ramshaw weischedel ontonote tion hlt naacl hsu bolte tsai lakhotia salakhutdinov mohamed hubert self supervise speech tion learning mask prediction hidden unit ieee acm taslp liu mining summarize customer review huang socher ning improve word representation global text multiple word prototype acl huang bidirectional lstm crf model sequence tagging arxiv preprint huffman learn tion extraction pattern ple wertmer riloff scheller ed connectionist tistical symbolic approach learn natural language cessing springer hunt black unit selection concatenative speech synthesis system large speech database icassp hutchin machine lation past present future elli horwood chichester england hutchin ﬁrst ception ﬁrst demonstration nascent year machine tion chronology chine translation hutchin somers introduction machine tion academic press hutchinson prabhakaran denton webster zhong denuyl social ase nlp model barrier person disability acl hyme way speak bauman sherzer ed exploration ethnography speaking cambridge versity iida inui takamura matsumoto incorporate contextual cue trainable model coreference resolution eacl workshop computational treatment anaphora irsoy cardie ion mining deep recurrent ral network emnlp ischen araujo oorveld van noort smit privacy concern chatbot tion international workshop chatbot research design datum element interchange format information interchange representation date time technical report international organization dard iso itakura minimum prediction residual principle apply speech recognition ieee transaction assp iter guu lansing jurafsky pretraine contrastive sentence objective improve discourse performance language model acl iter yoon jurafsky automatic detection incoherent speech diagnose nia fifth workshop tional linguistic clinical chology iyer konstas cheung namurthy zettlemoyer learn neural semantic parser user feedback acl iyer lin pasunuru haylov simig ter wang liu koura pereyra wang dewan maz zettlemoyer anov opt iml scale guage model instruction meta ing lens tion arxiv preprint izacard lewis lomeli hosseini petroni schick dwivedi joulin riedel grave shot ing retrieval augment guage model arxiv preprint jackendoff semantic cognition mit press jacobs rau scisor system ing information line news cacm jaech mulcaire hathi tendorf smith hierarchical character word model language identiﬁcation acl workshop nlp social medium nguyen senior vanhoucke application pretraine deep neural network large vocabulary speech recognition interspeech jauhiainen lui zampieri baldwin lind automatic language identiﬁcation text survey jair jefferson sequence sudnow study social teraction free press new york jefferson note systematic deployment knowledgement token yeah paper linguistic jeffrey theory ity edition clarendon press section jekat klein maier maleck mast quantz logue act verbmobil verbmobil jelinek fast sequential code algorithm stack ibm journal research ment jelinek self organize guage modeling speech tion waibel lee ed reading speech tion morgan kaufmann originally distribute ibm nical report jelinek mercer interpolate estimation markov source parameter sparse datum gelsema kanal ed proceeding workshop tern recognition practice north holland jelinek mercer bahl design tic statistical decoder nition continuous speech ieee transaction information ory grishman edge base population successful approach challenge acl grishman dang overview tac knowledge base population track eisenstein sentation learning text level course parse acl eisenstein tor entity augment distribute semantic discourse relation tacl jia liang datum bination neural semantic parsing meng zhao chang mitigate gender bias ampliﬁcation distribution terior regularization acl jiang hong cheng meng zhou zhou large guage model hallucination gard know fact naacl hlt johnson douze egou billion scale similarity search gpus arxiv preprint johnson acoustic tory phonetic edition johnson probability ductive inductive problem pendix mind johnson laird mental model harvard university press cambridge jones martin contextual spelling correction latent semantic analysis anlp jones mccallum nigam riloff bootstrappe text learn task shop text mining foundation technique application jones tion african american ular english dialect region black twitter american speech joos description language design jasa jordan serial order lel distribute processing approach technical report ics report university california san diego joshi hopely parser antiquity nai extend finite state el language cambridge university press joshi kuhn tere logic role entity tere sentence representation ural language inference joshi weinstein control inference role aspect discourse structure tering joshi chen liu weld zettlemoyer levy spanbert improve training represent ing span tacl joshi levy weld zettlemoyer bert coreference resolution baseline analysis emnlp bibliography joty carenini codra novel native framework rhetorical ysis computational linguistic jurafsky language food norton new york jurafsky chahuneau ledge smith tive framing consumer sentiment online restaurant review monday jurafsky wooters tajchman segal stolcke fosler morgan berkeley restaurant project icslp jurgens mohammad turney holyoak task ing degree relational similarity sem jurgens tsvetkov sky incorporate dialectal variability socially equitable guage identiﬁcation acl justeson katz occurrence antonymous jective contexts tational linguistic kalchbrenner blunsom recurrent continuous tion model emnlp kameyama sharing constraint center acl kamp theory truth semantic representation nendijk janssen stokhof ed formal method study language cal centre amsterdam kamphuis vries boytsov lin mean large scale reproducibility study scoring variant european conference information retrieval kane morris paradiso campbell time avuncular cantankerous reﬂexe mongoose derstande self expression augmentative alternative munication device cscw kaplan mccandlish henighan brown chess child gray radford amodei scale law neural language model arxiv preprint kaplan general tic processor rustin ral language processing algorithmic press karamani poesio mellish oberlander ing centering base metric herence text structure reliably annotate corpus chen hayashi hori inaguma jiang someki soplin mamoto wang watanabe yoshimura zhang comparative study transformer rnn speech application ieee karlsson outilainen heikkil anttila eds constraint grammar language independent system parse unrestricted text mouton gruyter karpukhin min lewis edunov chen yih dense passage retrieval open domain question answer emnlp karttunen discourse ent cole preprint karttunen comment joshi kornai extend finite state model language cambridge university press kasami efﬁcient nition syntax analysis algorithm context free language nical report air force cambridge research tory bedford katz fodor structure semantic theory guage kay experiment erful parser cole kay mind system rustin natural language processing algorithmic press kay algorithm schemata data structure syntactic ing text ing text analysis generation text typology attribution almqvist wiksell holm kay text translation alignment cal report xerox palo alto research center palo alto kay text translation alignment tational linguistic kehler effect tablishe coherence ellipsis anaphora resolution acl kehler temporal relation reference discourse coherence acl kehler current theory center pronoun interpretation critical evaluation computational linguistic probabilistic erence information extraction emnlp kehler coherence reference theory grammar csli publication kehler appelt taylor simma predicate argument frequency pronoun interpretation naacl kehler rohde abilistic reconciliation drive center drive theory pronoun interpretation cal linguistic keller lapata web obtain frequency see bigram computational guistic kendall farrington corpus regional african american language version eugene line resource african ican language project http coraal kennedy boguraev anaphora nal anaphora resolution parser cole khandelwal levy sky zettlemoyer lewis generalization orization near neighbor guage model iclr khattab potts zaharia relevance guide supervision openqa colbert tacl khattab singhvi wari zhang santhanam haq sharma joshi moazam miller zaharia potts dspy ing declarative language model call self improve pipeline iclr khattab zaharia bert efﬁcient effective sage search contextualized late interaction bert sigir kiela bartolo nie kaushik geiger vidgen prasad singh ringshia thrush riedel waseem torp jia bansal potts williams dynabench rethinking benchmarking nlp naacl hlt kiela clark atic study semantic vector space model parameter eacl shop continuous vector space model compositionality cvsc kim optimize putational efﬁciency gram negative sampling gram kim hovy termine sentiment opinion cole king african ican vernacular english african american language rethink study race language african americans speech annual review linguistic kingma adam method stochastic optimization iclr kintsch van dijk model text sion production psychological review kiperwasser goldberg simple accurate dependency parsing bidirectional lstm feature representation tacl kipper dang palmer class base construction verb lexicon aaai kiritchenko mohammad well bad scale liable rating scale case study sentiment intensity tion acl kiritchenko mohammad examine gender race bias sentiment ysis system sem kiss strunk vise multilingual sentence ary detection computational guistic kitaev cao klein multilingual constituency parse self attention training acl kitaev klein stituency parse attentive encoder acl klatt oice onset time friction aspiration initial consonant cluster journal speech hearing research klatt review arpa speech understand project jasa klatt klattalk text speech conversion system icassp kleene representation event nerve net ﬁnite tomata technical report rand corporation rand search representation event nerve net ﬁnite tomata shannon carthy ed automata study princeton university press klein simmons computational approach matical coding english word journal acm knott dale linguistic phenomenon motivate set coherence relation discourse process kocijan cretu camburu yordanov lukasiewicz ingly robust trick winograd schema challenge acl kocmi federmann kiewicz junczys dowmunt matsushita menezes ship ship extensive evaluation automatic metric machine translation arxiv koehn europarl parallel corpus statistical machine lation summit vol koehn hoang birch callison burch federico bertoldi cowan shen moran zens dyer jar constantin herbst moses open source toolkit statistical machine translation acl koehn och marcu statistical phrase base lation hlt naacl koenig dunn lacy sound spectrograph jasa kolhatkar roussel dipper zinsmeister anaphora non nominal antecedent computational linguistic vey computational linguistic kreutzer caswell wang wahab van esch orshikh tapo mani sokolov sikasote setyawan sarin samb sagot rivera rios padimitriou osei suarez orife ogueji rubungo nguyen muhammad mad mnyakeni mirzakhalov matangira leong lawson kudugunta jernite jenny firat dossou dlamini silva abuk ballı derman battisti baruwa bapna baljekar ime awokoya ataman ahia ahia agrawal adeyemi quality aglance audit web crawl multilingual dataset tacl kruskal overview quence comparison sankoff kruskal ed time warps string edit molecule theory tice sequence comparison addison wesley kudo subword regularization improve neural network tion model multiple subword candidate acl kudo matsumoto japanese dependency analysis cascade chunk conll kudo richardson sentencepiece simple guage independent subword enizer detokenizer neural text processing emnlp kudo richardson sentencepiece simple guage independent subword enizer detokenizer neural text processing emnlp kullback leibler information sufﬁciency annal mathematical statistic kulmizev lhoneux gontrum fano nivre deep contextualized word embedding transition base graph base dependency parsing tale parser revisit emnlp kumar byrne imum baye risk decode tistical machine translation naacl kummerfeld klein error drive analysis challenge coreference resolution emnlp kuno predictive lyzer path elimination nique cacm kupiec robust speech tagging hide markov model computer speech guage kurebito koryak fortescue mithun evans ed oxford handbook polysynthesis oxford kurita vyas pareek black tsvetkov tifye social bias contextual word representation acl shop gender bias natural language processing kwiatkowski palomaki ﬁeld collins parikh berti epstein polosukhin devlin lee bibliography jones kelcey chang dai uszkoreit petrov natural question benchmark question ing research tacl ladefoge course netic harcourt brace jovanovich ladefoge element tic phonetic edition sity chicago lafferty mccallum pereira conditional random ﬁeld probabilistic el segment labeling quence datum icml lai tetreault course coherence wild dataset evaluation method sigdial lake murphy word meaning mind chine psychological review press lakoff nature tactic irregularity thesis diana university publish ularity syntax holt rinehart winston new york lakoff structural complexity fairy tale study man school social science university california irvine lakoff johnson metaphor live university chicago press chicago lambert tunstall rajani thrush huggingface stack exchange preference dataset lample ballesteros manian kawakami dyer neural architecture name entity recognition naacl hlt lample conneau cross lingual language model training neurip volume lan chen goodman gimpel sharma cut albert lite bert self supervise learning guage representation iclr landauer dumais solution plato problem latent semantic analysis theory acquisition induction tation knowledge psychological review landauer laham rehder schreiner passage meaning derive word order parison latent semantic analysis human cogsci lapata similarity drive semantic role duction graph partitioning putational linguistic lang waibel hinton time delay ral network architecture isolated word recognition neural network lapata probabilistic text structuring experiment tence ordering acl lapesa evert large scale evaluation distributional mantic model parameter tion model selection tacl lappin leass rithm pronominal anaphora olution computational linguistic lascaride asher temporal interpretation discourse relation common sense ment linguistic philosophy lawrence synthesis speech signal low information rate jackson communication theory butterworth ldc ldc catalog project university sylvania catalog lecun boser denker henderson howard hubbard jackel backpropagation apply write zip code recognition neural computation lee seung learn part object negative matrix factorization ture lee chang peirsman chambers surdeanu jurafsky istic coreference resolution base entity centric precision rank rule computational linguistic lee peirsman chang chambers surdeanu jurafsky stanford pass sieve coreference resolution system share task conll lee surdeanu sky scaffold approach coreference resolution ing statistical rule base language engineering chang toutanova latent trieval weakly supervise open domain question answer acl lee lewis moyer end end neural coreference resolution emnlp lee zettlemoyer high order coreference resolution coarse ence naacl hlt lehiste reading acoustic phonetic mit press lehnert cardie fisher riloff williams description circus system levenshtein binary code capable correct deletion sertion reversal cybernetic control theory original doklady akademii nauk sssr levesque winograd schema challenge logical ization commonsense ing paper aaai spring symposium levesque davis stern winograd schema challenge levin mapping sentence case frame technical report mit laboratory work levin english verb class alternation preliminary vestigation university chicago press levin rappaport hovav argument realization bridge university press levy goldberg dependency base word ding acl levy goldberg guistic regularity sparse plicit word representation conll levy goldberg ral word embed implicit trix factorization neurips levy goldberg gan improve distributional similarity lesson learn word embedding tacl zheng byrne fung kamm song ruhi venkataramani chen cass phonetically scribe corpus mandarin neous speech icslp min iyer mehdad yih efﬁcient pass end end entity link question emnlp jurafsky ral net model open domain course coherence emnlp hovy recursive deep model discourse parsing emnlp chang discourse parse base hierarchical neural network emnlp meng sun han yuan word segmentation necessary deep learning chinese tion acl liang bommasani lee tsipras soylu yasunaga zhang narayanan kumar newman yuan yan zhang cosgrove manning navas hudson zelikman durmus ladhak rong ren yao wang thanam orr zheng sekgonul suzgun kim guha chatterji khattab henderson huang chi xie santurkar ganguli hashimoto icard zhang chaudhary wang mai zhang koreeda holistic evaluation guage model transaction chine learning research liberman delattre cooper role lecte stimulus variable ception unvoiced stop nant american journal ogy lin dependency base uation minipar workshop evaluation parse system lin michel aiden man orwant brockman petrov syntactic tion google books ngram corpus acl lin michel man aiden orwant man petrov tic annotation google books ngram corpus acl lin kan recognize implicit discourse tion penn discourse bank emnlp lin kan automatically evaluate text ence discourse relation acl lin kan pdtb style end end discourse parser natural language ing ling dyer black trancoso fermandez amir marujo ing function form compositional character model open lary word representation emnlp linzen issue evaluate mantic space word analogy workshop evaluate space representation nlp lison tiedemann extract large parallel corpora movie subtitle lrec litman plan recognition discourse analysis grated approach understand dialogue thesis university rochester rochester litman allen plan recognition model subdialogue conversation cognitive science liu hayase hofmann smith choi perbpe space travel language model arxiv preprint liu zhang vey opinion mining sentiment analysis aggarwal zhai eds mining text datum springer liu dacon fan liu liu tang der matter fairness logue system cole liu min zettlemoyer choi hajishirzi inﬁni gram scale unbounded gram language model trillion token arxiv preprint liu sun lin wang learn natural language inference bidirectional lstm model inner attention arxiv liu fung yang cieri huang graff hkust mts large scale mandarin telephone speech corpus international conference nese speak language processing liu ott goyal joshi chen levy lewis zettlemoyer stoyanov roberta robustly optimize bert training approach arxiv preprint llama team llama herd model logeswaran lee radev sentence ordering ence modeling recurrent ral network aaai longpre hou webson chung tay zhou zoph wei roberts flan collection ing datum method effective instruction tuning icml mahari lee lund oderinwale non saxena obeng marnu south hunter consent crisis rapid decline data commons arxiv preprint longpre yauney reif lee roberts zoph zhou wei robinson mimno ippolito pretrainer guide training datum measure effect datum age domain erage quality toxicity naacl hlt louis nenkova coherence model base syntactic pattern emnlp loureiro jorge language modelling make sense propagate representation wordnet coverage word sense disambiguation acl louviere flynn marley well bad scaling theory method application cambridge university press lovin development stem algorithm mechanical translation computational guistic lowerre harpy speech recognition system thesis carnegie mellon university burgh lukasik dadachev papineni sim text mentation cross segment tion emnlp luo coreference tion performance metric emnlp luo pradhan uation metric poesio stuckardt versley ed anaphora resolution algorithm resource application springer luo pradhan recasen hovy extension blanc system mention acl hovy end sequence label directional lstm cnns crf acl maas xie jurafsky lexicon free tional speech recognition ral network naacl hlt maas hannun rectiﬁer tie improve neural network acoustic model icml maas xie nun lengerich jurafsky build dnn acoustic model large lary speech recognition computer speech language bibliography magerman cal decision tree model parse acl mairesse walker trainable generation sonality style data drive parameter estimation acl mann thompson rhetorical structure theory theory text organization cal report information sciences institute manning speech tagging time linguistic cicling manning raghavan sch introduction formation retrieval cambridge manning surdeanu bauer finkel bethard closky stanford corenlp natural language ing toolkit acl marcu rhetorical parsing natural language text acl marcu decision base proach rhetorical parsing acl marcu rhetorical ing unrestricted text base approach computational guistic marcu theory practice discourse parsing summarization mit press marcu echihabi unsupervised approach ing discourse relation acl marcu wong phrase base joint probability model statistical machine lation emnlp marcus theory tactic recognition natural guage mit press marcus santorini marcinkiewicz build large annotated corpus english penn treebank computational linguistic marie fujita rubino scientiﬁc credibility chine translation research evaluation paper acl markov essai recherche statistique sur texte roman eugene onegin illustrant liaison des epreuve chain ample statistical investigation text eugene onegin lustrate dependence sample chain izvistia torskoi akademii nauk bulletin emie imp eriale des sciences etersbourg marneffe dozat veira haverinen ginter nivre manning universal stanford dependencie cross linguistic typology lrec marneffe maccartney manning ate type dependency parse phrase structure parse lrec marneffe ning stanford type pendencie representation cole workshop cross framework cross domain parser evaluation marneffe manning nivre zeman versal dependency tional linguistic marneffe recasen potts model pan discourse entity plication coreference resolution jair carreras litkowski stevenson semantic role labeling tion special issue tional linguistic marshall choice ical word class global tactic analysis tag word lob corpus computer manitie marshall tag selection probabilistic method garside leech sampson ed computational analysis english longman martschat strube error analysis coreference resolution emnlp martschat strube tent structure coreference lution tacl mathis mozer computational utility sciousness neurip mit press mccallum freitag pereira maximum entropy markov model information traction segmentation icml mccallum early result name entity tion conditional random ﬁeld feature induction web enhance lexicon conll mccarthy lehnert decision tree erence resolution mcclelland elman trace model speech perception cognitive psychology mcclelland hart eds parallel tribute processing explorationsin microstructure cognition volume psychological logical model mit press mcculloch pitts logical calculus idea immanent nervous activity bulletin ematical biophysic mcdonald crammer pereira online large margin training dependency parser acl mcdonald nivre alyze integrate dependency parser computational linguistic mcdonald pereira arov haji projective dependency parse ing span tree algorithm emnlp mcgufﬁe newhouse radicalization risk advanced neural guage model arxiv preprint mcluhan understand medium extension man new american library melamud goldberger gan ing generic context embed bidirectional lstm conll meng bau andonian belinkov locate edit factual association gpt neurip volume merialdo tag glish text probabilistic model computational linguistic mesgar strube cal coherence graph modeling word embedding acl meyer reeves macleod szekely zielinska young grishman bank project interim report naacl hlt workshop frontier corpus annotation mihalcea csomai wikify link document cyclopedic knowledge cikm mikheev moens grover name entity recognition gazetteer eacl mikolov statistical guage model base neural work thesis brno university technology mikolov chen corrado dean efﬁcient tion word representation tor space iclr mikolov karaﬁ ˇcernock pur recurrent neural work base language model terspeech mikolov kombrink burget ˇcernock khudanpur extension recurrent neural network language model icassp mikolov sutskever chen corrado dean distribute representation word phrase mikolov yih zweig linguistic regularity continuous space word tion naacl hlt miller nicely analysis perceptual sion english nant jasa miller beebe center psychological method evaluate quality lation mechanical translation miller charles contextual correlate semantic similarity language cognitive process miller chomsky finitary model language user luce bush galanter ed handbook ematical psychology volume john wiley miller selfridge verbal context recall meaningful material american journal psychology milne witten learn link wikipedia cikm miltsakaki prasad joshi webber penn discourse treebank lrec min lyu holtzman artetxe lewis hajishirzi zettlemoyer e role demonstration make context learning work emnlp minsky framework resent knowledge technical port mit laboratory memo minsky papert ceptron mit press mintz bills snow rafsky distant supervision relation extraction label datum acl ijcnlp mirza tonelli catena causal temporal relation extraction natural language text cole khashabi baral hajishirzi cross task generalization natural language crowdsourcing instruction acl mitchell divar barnes vasserman hutchinson spitzer raji gebru model card model reporting acm facct mitkov anaphora resolution longman mohame dahl hinton deep belief network phone recognition nip shop deep learning speech recognition relate tion mohammad obtain reliable human rating valence arousal dominance english word acl mohammad word affect intensity lrec mohammad ney crowdsource emotion association lexicon putational intelligence monroe colaresi quinn lexical feature selection ation identify content political conﬂict political analysis moor ellsworth scherer frijda praisal theory emotion state art future development emotion review moosavi strube coreference evaluation ric trust proposal link base entity aware metric acl morey muller asher progress rst discourse parsing replication study recent result rst emnlp morgan hirschman colosimo yeh colombe gene tiﬁcation normalization model organism database journal biomedical informatic morgan bourlard continuous speech recognition ing multilayer perceptron den markov model icassp morgan bourlard neural network tistical recognition continuous speech proceeding ieee hirst lexical cohesion compute thesaural lation indicator ture text computational tic mousavi maimon moumen petermann shi yang kuznetsova jnikov marxer run elizalde lugosch subakan woodland kim lee watanabe adi ravanelli discrete audio token survey arxiv preprint muller braud morey tony contextual ding accurate multilingual course segmentation ment workshop discourse lation parse treebanking murphy machine learning probabilistic perspective mit press musi stede kriese san rocci layer annotate corpus tative text argument scheme discourse relation lrec myers paper report speech act tiﬁc fact journal pragmatics estimation ability language model ibm speech recognition tem ieee transaction assp nadeem bethke reddy stereoset measure typical bias pretraine language model acl nash webber role semantic automatic speech derstande bobrow collins ed representation understanding academic press naur backus bauer green katz mccarthy perlis rutishauser samelson vauquois wegstein van wijnagaarden woodger report mic language algol cacm revise cacm neff nagy talk bot symbiotic agency case tay international journal communication teo kwan machine learning approach answer question read comprehension test emnlp learn noun phrase anaphoricity improve bibliography resolution issue representation optimization acl machine learning coreference resolution cal classiﬁcation global ranking acl supervise ranking pronoun resolution recent improvement aaai supervise noun phrase coreference research ﬁrst teen year acl machine learning tity coreference resolution spective look decade search aaai cardie fye anaphoric non anaphoric noun phrase improve coreference resolution cole cardie ing machine learn approach coreference resolution acl nguyen joty ral local coherence model acl nickerson versational interaction er proceeding acm graph workshop user orient design interactive graphic tem nie bennett man dissent learn tence representation explicit discourse relation acl nielsen neural network deep learning determination press usa nigam lafferty callum maximum tropy text classiﬁcation workshop machine learning information ﬁltere nirenburg somers wilk ed reading machine translation mit press nissim dingare carletta steedman annotation scheme information status alogue lrec nist timit acoustic phonetic continuous speech corpus tional institute standard technology speech disc nist order nist speech recognition scoring toolkit sctk version nist match pair segment word error mapsswe test nivre incremental projective dependency parsing naacl hlt efﬁcient algorithm projective dependency parsing proceeding international workshop parse technology iwpt nivre inductive dependency parsing springer nivre non projective pendency parsing expected linear time acl ijcnlp nivre hall donald nilsson riedel yuret conll share task dependency parsing emnlp conll nivre hall nilsson chanev eryigit nov marsi parser language independent system data drive dependency parsing natural language neere nivre nilsson projective dependency parsing acl nivre scholz ministic dependency parsing glish text cole noreen computer sive method test hypothesis wiley norman design eryday thing basic book norvig technique tomatic memoization tion context free parsing putational linguistic nosek banaji greenwald e implicit group attitude lief demonstration web site group dynamic theory research practice nosek banaji greenwald math male female math journal personality social psychology nostalgebraist interpret gpt logit lens white paper ocal perez radas finlayson holistic uation automatic timeml tator lrec och ein basierter und statistischer ansatz zum maschinellen lernen von thesis universit germany diplomarbeit diploma thesis och minimum error rate training statistical machine lation acl och ney inative training maximum tropy model statistical machine translation ney atic comparison statistical alignment model computational linguistic och ney ment template approach statistical machine translation computational linguistic olive rule sis speech dyadic unit olsson elhage nanda joseph dassarma henighan mann askell bai chen context learning induction head arxiv preprint oppenheim schafer stockham nonlinear ﬁltering multiplied convolve signal proceeding ieee oravecz diene ﬁcient stochastic speech ging hungarian lrec osgood suci nenbaum measurement meaning university illinois press ostendorf price shattuck hufnagel boston university radio news pus technical report boston university ouyang jiang almeida wainwright mishkin zhang wal slama ray man hilton kelton miller simen askell welinder christiano leike lowe train language model follow instruction human feedback neurip volume packard assist morphological analysis ancient greek cole palmer gildea xue semantic role labeling thesis lecture human language technology palmer kingsbury gildea tion bank annotated corpus semantic role computational linguistic panayotov chen povey khudanpur librispeech asr corpus base public domain audio book icassp pang lee ion mining sentiment analysis foundation trend tion retrieval pang lee vaithyanathan thumb sentiment classiﬁcation machine ing technique emnlp papadimitriou lopez rafsky multilingual bert accent evaluate english ﬂuence ﬂuency multilingual model eacl finding papineni roukos ward zhu bleu method automatic evaluation machine translation acl park shin fung reduce gender bias abusive guage detection emnlp park cardie e appropriate support tion online user comment workshop argumentation mining parrish chen nangia makumar phang thompson htut bowman bbq hand build bias benchmark question answer finding acl paszke gross chintala chanan yang devito lin desmaison antiga lerer automatic ferentiation pytorch nip peldszus stede argument diagram argumentation mining text survey ternational journal cognitive formatic natural intelligence ijcini peldszus stede annotate corpus argumentative microtext european ence argumentation peng tian yan berrebbi chang shi arora chen sharma zhang sudo shakee weon jung maiti watanabe produce whisper style training e open source toolkit licly available datum asru penn kiparsky generative capacity contextualize replacement system cole pennebaker booth francis linguistic quiry word count liwc austin pennington socher manning glove global vector word representation emnlp percival torical source immediate stituent analysis mccawley syntax semantic volume note linguistic ground academic press peters neumann iyyer gardner clark lee zettlemoyer deep contextualize word representation naacl hlt barney control method study vowel jasa peterson wang sivertsen tion technique speech synthesis jasa peterson chen ﬁth parallelogram revisit explore limitation vector space model simple analogy cognition petroni rockt riedel lewis bakhtin miller language model knowledge basis emnlp petrov das mcdonald universal speech tagset lrec petrov mcdonald overview share task parse web note workshop syntactic analysis non canonical language sancl volume picard affective ing technical report mit dia lab perceputal computing nical report revise november pierce carroll hamp hays hockett oettinger perlis language machine computer translation guistic alpac report national academy sciences national search council washington pilehvar collado wic context dataset evaluate context sensitive meaning tation naacl hlt pitler louis nenkova automatic sense prediction implicit discourse relation text acl ijcnlp pitler nenkova ing syntax disambiguate explicit discourse connective text acl ijcnlp pitt dilley son kiesling raymond hume fosler lussier buckeye corpus tional speech release ment psychology ohio state versity distributor pitt johnson hume kiesling raymond buckeye corpus versational speech label vention test transcriber liability speech communication plutchik emotion fact theory new model random general evolutionary theory emotion plutchik kellerman ed emotion theory research perience volume academic press poesio stevenson nio hitzeman ing parametric theory stantiation computational tic poesio stuckardt sley anaphora resolution algorithm resource tion springer poesio sturt artstein filik underspeciﬁcation anaphora theoretical issue preliminary evidence discourse process poesio vieira corpus base investigation nite description use computational linguistic polanyi formal model structure discourse journal pragmatic polanyi culy van den berg thione ahn rule base approach discourse parsing proceeding sigdial pollard sag drive phrase structure grammar university chicago press ponzetto strube exploit semantic role labeling wordnet wikipedia ence resolution hlt naacl ponzetto strube knowledge derive wikipedia compute semantic relatedness jair popovi chrf ter gram score automatic evaluation proceeding tenth workshop statistical chine translation popp donovan ford marsh peele gender race speech style stereotype sex role post clarity port bleu score wmt potts negativity negation lutz ed proceeding semantic linguistic theory clc publication ithaca povey ghoshal boulianne burget glembek goel hannemann motlicek qian schwarz silovsk stemmer vesel kaldi speech recognition toolkit asru bibliography pradhan hovy cus palmer ramshaw weischedel ontonote uniﬁed relational semantic sentation proceeding icsc pradhan hovy cus palmer ramshaw weischedel ontonote uniﬁed relational mantic representation int tic computing pradhan luo recasen hovy strube scoring coreference partition predict mention reference implementation acl pradhan moschitti xue uryupina zhang zhong ward robust linguistic analysis ing ontonote conll pradhan moschitti xue uryupina zhang share task ing multilingual unrestricted erence ontonotes conll pradhan moschitti xue uryupina zhang share task ing multilingual unrestricted erence ontonotes conll pradhan ramshaw cus palmer weischedel xue share task model unrestricted ence ontonote conll pradhan ramshaw schedel macbride ciulla unrestricted ence identify entity event ontonote proceeding icsc pradhan ward hacioglu martin jurafsky semantic role label ent syntactic view acl prasad dinesh lee sakaki robaldo joshi webber penn course treebank lrec prasad webber joshi reﬂection penn course treebank comparable pora complementary tion computational linguistic prate avelar lamb assess gender bias machine translation case study google translate neural puting application price fisher stein pallet darpa word resource agement database continuous speech recognition icassp ostendorf hufnagel fong use prosody syntactic biguation jasa prince taxonomy give new information cole radical pragmatic academic press propp morphology folktale edition university texas press original russian translate laurence scott pundak sainath low frame rate neural network acoustic model interspeech pustejovsky generative lexicon computational linguistic pustejovsky hanks saur gaizauskas setzer radev sundheim day ferro lazo timebank corpus proceeding corpus linguistics ence ucrel technical paper ber pustejovsky ingria saur casta littman gaizauskas setzer katz mani tion language timeml chapter oxford qin zhang zhao stacking gate neural architecture implicit discourse relation ﬁcation emnlp qin zhang zhao xing adversarial connective exploit network implicit discourse relation tion acl radford kim brockman mcleavey sutskever robust speech recognition large scale weak pervision icml radford child luan amodei sutskever language model unsupervised multitask learner openai tech port rafailov sharma mitchell ermon manning finn direct preference timization language model secretly reward model neurips raffel shazeer roberts lee narang matena zhou liu explore limit fer learn uniﬁed text text transformer jmlr raghunathan lee jan chambers surdeanu jurafsky manning multi pass sieve erence resolution emnlp vise model coreference tion emnlp rahman ing complex case deﬁnite noun winograd schema lenge emnlp rajpurkar jia liang know know unanswerable question squad acl rajpurkar zhang lopyrev liang squad question machine sion text emnlp ram levine dalmedigos muhlgay shashua brown shoham context retrieval augment guage model arxiv preprint ramshaw cus text chunk transformation base learning ceeding annual shop large corpora rashkin bell choi olkova multilingual notation frame case study social medium target sentiment analysis forecast acl rashkin singh choi connotation frame drive investigation acl ratinov roth learning base multi sieve reference resolution edge emnlp ratnaparkhi mum entropy speech tagger emnlp ratnaparkhi linear serve time statistical parser base maximum entropy model emnlp rawl justice fairness restatement harvard university press recasen hovy blanc implement rand index coreference evaluation natural language engineering recasen hovy mart identity non identity near identity address complexity coreference lingua recasen mart ancora coreferentially tat corpora spanish lan language resource uation reed mochales palau rowe moen guage resource study ment lrec reeve nass medium equation people treat computer television new dia like real people place cambridge university press rehder schreiner wolfe laham landauer kintsch latent semantic analysis assess edge technical tion discourse process rei stewart farinha lavie comet ral framework evaluation emnlp reichenbach element symbolic logic macmillan new york reichman get computer talk like mit press renal hain bourlard recognition ing meeting ami amida project asru resnik semantic class syntactic ambiguity hlt resnik selectional straint information theoretic model computational tion cognition riedel yao mccallum modeling relation mention label text machine learning knowledge discovery database springer riedel yao mccallum marlin relation tion matrix factorization universal schemas naacl hlt riloff automatically structe dictionary tion extraction task aaai riloff automatically erate extraction pattern tag text aaai riloff jones learn dictionary information tion multi level bootstrapping aaai riloff schmelzenbach empirical approach conceptual case frame acquisition proceeding sixth workshop large corpora riloff shepherd corpus base approach build semantic lexicon emnlp riloff thelen base question answer system read comprehension test anlp naacl workshop read comprehension test riloff wiebe ing extraction pattern subjective expression emnlp etzioni mausam latent dirichlet allocation method selectional preference acl ritter zettlemoyer mausam etzioni model ing datum distant supervision formation extraction tacl roberts raffel shazeer knowledge pack parameter language model emnlp robertson walker jones hancock beaulieu gatford okapi overview text retrieval conference robin short history linguistic indiana university press bloomington robinson fallside recurrent error propagation work speech recognition system computer speech language robinson hochberg nal use recurrent ral network continuous speech recognition lee soong paliwal ed tomatic speech speaker tion springer roger gardner genstein dataset sion taxonomy nlp resource question answer read comprehension acm computing survey rohde gonnerman plaut improved model semantic similarity base lexical occurrence cacm rooth riezler prescher carroll beil ing semantically annotate lexicon base clustering acl rosenblatt tron probabilistic model formation storage organization brain psychological review rosenfeld adaptive tical language modeling mum entropy approach sis carnegie mellon university rosenfeld maximum tropy approach adaptive cal language modeling computer speech language rosenthal mckeown detect inﬂuencer multiple line genre acm transaction internet technology toit ebert sch ultradense word ding orthogonal tion naacl hlt rudinger naradowsky leonard van durme gender bias coreference resolution naacl hlt rumelhart hinton williams learn ternal representation error agation rumelhart mcclelland ed parallel tribute processing volume mit press rumelhart mcclelland learn past tense english verbs rumelhart mcclelland ed parallel distribute processing volume mit press rumelhart mcclelland ed parallel distribute processing mit press rumelhart son model cal reasoning cognitive ogy rumelhart mcclelland ed parallel distribute processing exploration crostructure cognition volume foundation mit press ruppenhofer ellsworth petruck johnson baker scheffczyk framenet extend theory practice ruppenhofer sporleder morante baker palmer task link event participant discourse ternational workshop semantic evaluation russell plex model affect journal personality social psychology russell norvig tiﬁcial intelligence modern proach edition prentice hall rust pfeiffer vuli ruder gurevych good tokenizer lingual performance multilingual language model acl rutherford xue prove inference implicit course relation classify plicit discourse connective naacl hlt sachan lewis gatama zettlemoyer pineau zaheer question need train dense passage retriever tacl bibliography sack schegloff ferson simple atic organization take conversation language sagae analysis course structure syntactic pendencie data drive reduce parsing sagawa koh hashimoto liang tributionally robust neural network group shift importance regularization bad case eralization iclr sagisaka speech sis rule optimal tion non uniform synthesis unit icassp sagisaka kaiki iwahashi mimura atr talk speech synthesis system icslp sakoe chiba dynamic programming approach continuous speech recognition ceeding seventh tional congress acoustic ume akad emiai kiad sakoe chiba namic programming algorithm mization speak word tion ieee transaction assp salomaa probabilistic weighted grammar information control salton smart trieval system experiment tomatic document processing tice hall sampson alternative matical code system leech sampson ed computational analysis english longman sankoff labov use variable rule language society sap card gabriel choi smith risk racial bias hate speech detection acl sap prasettio holtzman rashkin choi notation frame power agency modern ﬁlm emnlp littman knippen gaizauskas setzer pustejovsky timeml notation guideline version manuscript scha polanyi augment context free grammar discourse cole schank abelson script plan knowledge ceeding abelson script plan goal standing lawrence erlbaum schegloff sequence conversational opening american anthropologist schegloff discourse interactional achievement use huh thing come sentence nen analyze discourse text talk georgetown versity press washington scherer psychological model emotion borod neuropsychology tion oxford schiebinger machine translation analyze gender schiebinger scientiﬁc search gender count nature schluter word analogy testing caveat naacl hlt schone jurafsky knowlege free induction phology latent semantic ysis conll schone jurafsky knowledge free induction word unit dictionary headword solve problem emnlp schone jurafsky knowledge free induction tional morphology naacl schuster nakajima japanese korean voice search icassp schuster paliwal bidirectional recurrent neural work ieee transaction signal processing context space aaai fall symposium bilistic approach natural guage dimension meaning proceeding put ieee press ambiguity tion language learn putational cognitive model csli stanford hull sen comparison siﬁer document representation routing problem pedersen vector model syntagmatic paradigmatic relatedness nual conference centre new oed text research singer speech tagging variable memory markov model acl schwartz eichstaedt kern dziurzynski ramone agrawal shah kosinski stillwell seligman ungar personality gender age language social medium vocabulary approach plos schwenk continuous space language model computer speech language schwenk filter ing parallel datum joint gual space acl schwenk dechelotte gauvain continuous space language model statistical chine translation cole acl schwenk wenzek edunov grave joulin fan ccmatrix mining billion high quality parallel sentence web acl latent able model selectional ence acl seddah tsarfaty candito choi farkas foster goenaga gojenola goldberg green habash kuhlmann maier nivre przepi orkowski roth seeker versley vincze woli nski oblewska villemonte ergerie overview spmrl share task cross framework evaluation parse ically rich language shop statistical parsing morphologically rich language sekine collins evalb software http proteus evalb sellam das parikh bleurt learn robust metric text generation acl seneff zue scription alignment timit database proceeding second symposium advanced man machine interface speak language sennrich haddow birch neural machine translation rare word subword unit acl seo kembhavi farhadi hajishirzi bidirectional attention ﬂow machine hension iclr shannon mathematical theory communication bell tem technical journal continue following ume shannon prediction tropy print english bell system technical journal sheil observation text free parsing smil statistical method linguistic sheng chang natarajan peng woman work babysitter bias language generation emnlp shi lin simple bert model relation extraction semantic role labeling arxiv shi min yasunaga seo james lewis zettlemoyer yih replug retrieval augment black box guage model arxiv preprint shoup phonological aspect speech recognition lea trend speech recognition prentice hall sidner tational theory deﬁnite anaphora comprehension english discourse technical report mit cial intelligence laboratory bridge sidner focus comprehension deﬁnite anaphora brady berwick ed computational model course mit press silverman beckman pitrelli ostendorf man price bert hirschberg tobi standard label english prosody icslp simmon answer glish question computer vey cacm simmon semantic work computation use understand english sentence schank colby ed computer model thought language man simmon klein conlogue indexing pendency logic answer glish question american mentation simon fennig ethnologue language world edition sil national singh vargus karlsson mahendiran shandilya tel mataciunas zhang hettiarachchi son machado moura krzemi nski fadaei erg okoh alaagib nayake alyafeai chien ruder guthikonda alghamdi gehrmann nighoff bartolo kreutzer fadaee hooker aya dataset open access collection lingual instruction tuning arxiv preprint sleator temperley parse english link mar sloan aristotle machean ethic original cus septem circumstantiae classical philology slobin way travel shibatani thompson ed grammatical struction form ing clarendon press smolensky proper treatment connectionism ioral brain science smolensky tensor product variable binding tion symbolic structure nectionist system artiﬁcial ligence snover dorr schwartz micciulla makhoul study translation edit rate target human annotation snow jurafsky learn syntactic pattern automatic hypernym discovery neurip socher bauer ning ing compositional vector mar acl socher lin manning parse ral scene natural language recursive neural network icml soderland fisher aseltine lehnert tal induce conceptual nary søgaard simple supervised training speech tagger acl søgaard goldberg deep multi task learning low level task supervise low søgaard johannsen plank hovy alonso value nlp conll kinney gia schwenk atkinson authur bogin chandu dumas elazar hofmann jha kumar lucy lyu lambert son morrison muennighoff naik nam ter ravichander richardson shen strubell subramani tafjord walsh zettlemoyer smith hajishirzi agy groeneveld dodge dolma open pus trillion token guage model pretraine research arxiv preprint solorio blair maharjan bethard diab ghoneim hawwari alghamdi hirschberg chang fung overview ﬁrst share task language tiﬁcation code switch datum workshop computational proache code switching somasundaran burstein chodorow lexical ing measure discourse ence quality test taker essay cole soon lim machine learn proach coreference resolution noun phrase computational guistic soricut marcu tence level discourse parse syntactic lexical information hlt naacl soricut marcu discourse generation train coherence model ing acl sorokin gurevych mix context granularity prove entity link question answer datum entity gorie sem sparck jones statistical terpretation term speciﬁcity application retrieval journal documentation sparck jones synonymy semantic classiﬁcation edinburgh university press edinburgh lication phd thesis sporleder lascarides exploit linguistic cue classify rhetorical relation sporleder lapata course chunk application sentence compression emnlp srivastava hinton krizhevsky sutskever salakhutdinov bibliography simple way prevent neural work overﬁtte jmlr stab gurevych tat argument component lation persuasive essay ing stab gurevych fye argumentative discourse ture persuasive essay emnlp stab gurevych parse argumentation structure sive essay computational tic stalnaker assertion cole pragmatic syntax semantic volume demic press stamatato survey ern authorship attribution method jasist stanovsky smith zettlemoyer evaluate gender bias machine translation acl stede discourse processing morgan claypool stede schneider mentation mining morgan pool stern andreas klein minimal span base neural constituency parser acl steven acoustic ic mit press steven house development tive description vowel tion jasa steven house acoustical theory vowel duction implication journal speech hear search steven kasowski fant electrical analog vocal tract jasa steven olkmann relation pitch frequency revise scale american journal psychology steven olkmann newman scale surement psychological nitude pitch jasa stiennon ouyang ziegler lowe oss ford amodei christiano learn summarize human feedback proceeding international conference neural information processing system entropy base ing backoff language model proc darpa broadcast news scription understand shop stolcke srilm sible language modeling toolkit slp stolcke konig traub explicit word error imization good list rescoring eurospeech volume stolz tannenbaum carstensen stochastic approach grammatical coding english cacm stone dunphry smith ogilvie general quirer computer approach content analysis mit press gertz tilingual cross domain temporal tagging language resource evaluation strube hahn tional centering acl strubell ganesh lum energy policy sideration deep learning nlp acl sun sadler srivatsa yan yan generate characteristic rich tion set evaluation emnlp subba eugenio effective discourse parser use rich linguistic information naacl hlt sukhbaatar szlam weston fergus end end memory network neurip sundheim proceeding sundheim proceeding sundheim proceeding baltimore sundheim proceeding surdeanu overview knowledge base tion evaluation english slot ﬁlling temporal slot ﬁlling surdeanu harabagiu williams aarseth predicate argument structure information extraction acl surdeanu hicks valenzuela escarcega practical rhetorical structure theory parser naacl hlt surdeanu johansson er arquez nivre conll share task joint parsing syntactic tic dependency conll vinyal sequence sequence ing neural network neurip sutton barto inforcement learning tion mit press suzgun melas kyriazi jurafsky follow dom crowd effective text generation minimum baye risk decode finding acl suzgun scales sch gehrmann tay chung chowdhery chi zhou wei challenge big bench task chain thought solve acl finding sweet handbook netic clarendon press swier stevenson supervise semantic role labelling emnlp switzer vector image ument retrieval statistical tion method mechanized mentation symposium proceeding washington usa march gov nistpub legacy syrdal wightman conkie stylianou nagel schroeter strom lee corpus base technique nextgen synthesis system icslp talmy lexicalization pattern semantic structure lexical form shopen language ogy syntactic description ume cambridge university press originally appear berkeley cognitive science program report talmy path realization typology event conﬂation tan niculae niculescu mizil lee win argument interaction namic persuasion strategy good faith online discussion tannen frame surface evidence underlie pectation freedle new direction discourse processing ablex taylor text speech sis cambridge university press taylor cloze procedure new tool measure readability journalism quarterly teranishi umeda use pronounce dictionary speech synthesis experiment tional congress acoustic tesni ere syntaxe structurale librairie sieck paris tetreault corpus base evaluation center pronoun resolution computational tic teufel carletta moens annotation scheme discourse level argumentation search article eacl teufel siddharthan batchelor domain independent tive zoning evidence istry computational linguistic emnlp thede harper second order hide markov model speech tagging acl thompson koehn calign improve sentence ment linear time space emnlp thompson regular pression search algorithm cacm tian kulkarni perozzi skiena convergent property word bed method arxiv preprint tibshirani regression shrinkage selection lasso journal royal statistical ciety series methodological timkey van schijndel bark bite rogue sion transformer language el obscure representational quality emnlp titov khoddam pervise induction semantic role reconstruction error mization framework naacl hlt titov klementiev bayesian approach unsupervised semantic role induction eacl tomkin affect imagery consciousness vol positive affect springer toutanova klein ning singer rich speech tagging cyclic dependency network naacl tria loreto dio zipf heap taylor law determine sion adjacent possible tropy trichelair emami cheung trischler suleman diaz uation common sense reasoningin natural language understanding neurip workshop tiquing correct trend machine learning trnka yarrington mccaw mccoy pennington effect word diction communication rate aac naacl hlt turian shen me evaluation machine translation evaluation ceeding summit turian ratinov bengio word representation ple general method supervised learning acl turney thumb thumb semantic tion apply unsupervised ﬁcation review acl turney littman measure praise criticism ference semantic orientation association acm transaction information system tois turney littman corpus base learning analogy semantic relation machine learning umeda linguistic rule text speech synthesis ing ieee umeda matui suzuki omura synthesis fairy tale analog vocal tract international congress tic uryupina artstein bristot cavicchio delogu driguez poesio notate broad range anaphoric phenomena variety genre arrau corpus natural guage engineering uszkoreit transformer novel neural network architecture language understanding google research blog post thursday gust van deemter kibble coreferre ence muc related annotation scheme computational tic van den oord vinyal kavukcuoglu ral discrete representation learning neurip van der maaten hinton visualize high dimensional datum sne jmlr van rijsbergen information retrieval shazeer parmar uszkoreit jones gomez kaiser polosukhin attention need neurip vauquois survey mal grammar algorithm recognition transformation machine translation ifip congress velichko zagoruyko automatic recognition word international journal man machine study velikovich blair goldensohn hannan mcdonald viability web derive polarity lexicon naacl hlt vendler linguistic ophy cornell university press verhagen gaizauskas schilder hepple icz pustejovsky tempeval challenge identify temporal relation text guage resource evaluation verhagen mani sauri knippen jang littman rumshisky phillips pustejovsky automate temporal annotation tarsqi acl versley vagueness erential ambiguity large scale annotate corpus research language computation vieira poesio pirically base system ing deﬁnite description tional linguistic vilain burger aberdeen connolly hirschman model theoretic coreference scoring scheme vintsyuk speech ination dynamic programming cybernetic nal russian kibernetika vinyal kaiser koo petrov sutskever ton grammar foreign guage neurip oorhee tion answer track report ceeding text retrieval conference oorhee harman trec experiment evaluation information retrieval mit press outilainen handcraft rule van halteren tactic wordclass tagging bibliography vrande wikidata free collaborative knowledge base cacm wagner fischer string string correction lem journal acm waibel hanazawa ton shikano lang phoneme recognition time delay neural network ieee transaction assp walker iida cote japanese discourse process center computational linguistic walker joshi prince eds center discourse oxford university press walker maier allen carletta condon flammia hirschberg isard ishizaki levin luperfoy traum whittaker penn multiparty standard coding scheme draft annotation manual course tag wang singh michael hill levy bowman glue multi task benchmark analysis platform natural guage understanding iclr wang manning baseline bigram simple good sentiment topic tion acl wang chang base dependency parse rectional lstm acl wang yang fast accurate neural course segmentation emnlp wang mishra labashi kordi mirzaei naik ashok dhanasekaran arunkumar stap pathak manolakis lai purohit mondal anderson nia doshi pal tel moradshahi mar purohit varshney kaza verma puri karia doshi sampat mishra reddy patro dixit shen naturalinstruction tion declarative instruction nlp task emnlp wang dong zeng adams sreedhar egert lalleau scowcroft kant swope kuchaiev multi attribute ness dataset steerlm naacl hlt ward tsukahara prosodic feature cue channel feedback english japanese journal pragmatics watanabe hori karita hayashi nishitoba unno soplin heymann wiesner chen intala ochiai net end end speech processing toolkit interspeech weaver translation locke boothe ed machine translation language mit press reprint memorandum write weaver webber formal approach discourse anaphora thesis harvard university webber talk brady berwick ed computational model discourse mit press webber structure tension interpretation course deixis language tive process webber baldwin accommodate context change acl webber egg doni discourse structure language technology natural guage engineering webber discourse deixis reference discourse segment acl webson pavlick prompt base model stand meaning prompt naacl hlt webster recasen rod baldridge mind gap balanced corpus dere ambiguous pronoun tacl wei wang schuurmans bosma xia chi zhou chain think prompt elicit reason large language model neurip volume weischedel meteer schwartz ramshaw palmucci cope biguity unknown word probabilistic model computational linguistic weizenbaum eliza computer program study ofnatural language communication tween man machine cacm weizenbaum computer power human reason ment calculation freeman wells accent english cambridge university press werbo regression new tool prediction sis behavioral sciences thesis harvard university werbo backpropagation time proceeding ieee weston chopra borde memory network iclr widrow hoff adaptive switch circuit ire wescon convention record ume wiebe tracking point view narrative computational tic wiebe learn subjective jective corpora aaai wiebe bruce development use gold standard datum set ity classiﬁcation acl wierzbicka semantics ture cognition university man concept culture speciﬁc conﬁguration oxford university press wierzbicka semantic prime universal oxford versity press wilk artiﬁcial gence approach machine tion schank colby ed computer model thought language freeman wilk preference semantic keenan formal mantic natural language cambridge univ press wilk preferential pattern seeking semantic ral language inference artiﬁcial telligence williams nangia man broad coverage lenge corpus sentence stand inference naacl hlt wilson wiebe hoffmann recognize contextual ity phrase level sentiment winograd understand ural language academic winston artiﬁcial gence addison wesley wiseman rush shieber learn global feature coreference resolution naacl hlt wiseman rush shieber weston ing anaphoricity antecedent ranking feature coreference olution acl witten bell zero frequency problem timate probability novel event adaptive text compression ieee transaction information theory witten frank datum mining practical machine ing tool technique tion morgan kaufmann wittgenstein ical investigation translate anscombe blackwell wolf gibson resent discourse coherence corpus base analysis tional linguistic wolf miller grodzinsky see come comment microsoft tay experiment wide implication orbit journal wood semantic quantiﬁcation natural language question answer yovit computer academic wood kaplan nash webber lunar ence natural language information system final report technical port bbn woodsend lapata distribute representation supervise semantic role labeling emnlp polynomial time rithm statistical machine tion acl weld tonomously semantifye pedia weld open information extraction pedia acl petroni josifoski riedel zettlemoyer scalable zero shot entity link dense entity retrieval emnlp dredze beto bentz beca surprising lingual effectiveness bert emnlp schuster chen norouzi macherey krikun cao gao macherey klingner shah johnson liu kaiser gouws kato kudo kazawa stevens kurian patil wang young smith riesa nick vinyal corrado hughe dean google neural machine translation system bridge gap human machine translation arxiv preprint wundt eine untersuchung der lungsgesetze von sprache mythus und sitte engelmann leipzig band die sprache zweiter teil pathak wallace rurangan sap klein detoxify language model risk marginalize minority voice naacl hlt boureau weston dinan recipe safety domain chatbot arxiv preprint saghir kang long bose cao ung cross domain able neural coherence model acl speech melody ulatorily implement tive function speech tion xue pradhan rutherford webber wang wang conll share task tilingual shallow discourse parsing share task xue palmer ing feature semantic role ing emnlp yamada matsumoto statistical dependency analysis support vector machine yang chen yang jurafsky hovy let request persuasive ele persuasive strategy supervise neural net funding platform naacl hlt yang zhou tan coreference resolution ing competition learn approach acl yang pedersen parative study feature selection text categorization icml yih richardson meek chang suh value semantic parse label knowledge base question answer get word edgewise university chicago young keizer mairesse schatzmann thomson hide information state model practical framework base speak dialogue management computer speech language young recognition parsing context free language time control zhang transition base neural rst parsing implicit syntax feature ing zhu liu liu peng gong zelde gumdrop share task model stack proach discourse unit tion connective detection shop discourse relation parsing treebanke yuan liberman cieri integrate standing speaking rate sation interspeech zao sanders people gen people gen access zapirain agirre arquez surdeanu selectional preference semantic role ﬁcation computational linguistic zelle mooney learn parse database query inductive logic programming aaai zeman reusable tagset version tagset driver lrec zen ney efﬁcient phrase table representation chine translation application online speech translation naacl hlt zettlemoyer collins learn map sentence ical form structure classiﬁcation probabilistic categorial mar uncertainty artiﬁcial ligence zettlemoyer collins online learning relaxed ccg grammar parse logical form emnlp conll zhang klyman mai levine zhang bommasani liang language model developer report train test overlap icml bibliography zhang dos santo sunaga xiang radev neural coreference resolution deep biafﬁne attention joint mention detection mention tere acl zhang kishore weinberger artzi bertscore evaluate text ation bert iclr zhang zhong chen geli manning position aware attention pervise datum improve slot ﬁlling emnlp zhao chen kit zhou multilingual dependency learning huge feature ing method semantic dependency parsing conll zhao wang yatskar terell ordonez chang gender bias contextualized word embedding naacl hlt zhao wang yatskar donez chang man like shop reduce gender bias ampliﬁcation corpus level constraint emnlp zhao wang yatskar donez chang gender bias coreference lution evaluation debiase method naacl hlt zhao zhou wang chang e gender neutral word ding emnlp zheng vilnis singh choi mccallum dynamic knowledge base alignment coreference resolution conll zhou bousquet lal weston sch learn local global sistency neurips zhou zhang zhang explore ious knowledge relation tion acl zhou end end learning semantic role ing recurrent neural network acl zhou end end learning semantic role ing recurrent neural network acl zhou ethayarajh card jurafsky problem cosine measure embed similarity high frequency word acl zhou hwang ren sap rely reliable impact languagemodel reluctance express tainty acl zhou ticrea hovy multi document biography summarization emnlp zhou xue nese discourse treebank chinese corpus annotate discourse lation language resource evaluation zhu ghahramani learn label unlabeled datum label propagation cal report cmu cmu zhu ghahramani ferty semi supervised ing gaussian ﬁeld monic function icml zhu kiros zemel salakhutdinov urtasun torralba fidler align book movie ward story like visual explanation watch movie read book ieee international ence computer vision ziegler stiennon brown radford amodei christiano irving fine tune language model human preference arxiv ziemski junczys dowmunt pouliquen united nation parallel corpus lrec index format fold cross validation derive kleene kleene character end line precedence symbol character disjunction non word boundary word boundary character disjunction start line char negation gram tuple gram conversion aac aae ablating absolute position absolute temporal expression abstract word acceleration feature asr accented syllable accessible access referent accomplishment expression accuracy achievement expression acknowledgment speech act activation activity expression add gate add add smoothing adequacy adjacency pair adjective adverb degree directional locative manner temporal adverb adversarial loss hoc retrieval ae afﬁx affricate sound agent thematic role agglutinative language aiff ﬁle aktionsart algol algorithm byte pair encoding cky minimum edit distance semantic role labeling texttiling viterbi align alignment asr minimum cost transcript string minimum edit distance allen relation allocational harm alveolar sound ambiguity speech brown corpus attachment coordination refer expression speech resolution tag american structuralism ami amplitude signal rm anaphor anaphora anaphoricity detector anchor text anchor regular expression anisotropy antecedent apple aiff approximant sound approximate randomization arabic egyptian aramaic arc eager arc standard argumentation mining scheme argumentative relation argumentative zoning aristotle arpa arpabet article speech articulatory phonetic articulatory synthesis ascii aspect asr association atis corpus atn atran attachment ambiguity attention cross attention encoder decoder history transformer attention head attention mechanism attribution coherence relation augmentative communication authorship attribution autoregressive generation auxiliary babbage backchannel backoff backprop backpropagation time backtrace minimum edit distance backtranslation backus naur form backward look center bag word bakeoff speech recognition competition base model basic emotion batch training baye rule dropping denominator beam search beam width pitch accent berkeley restaurant project bert affect well bad scaling bias ampliﬁcation bias term bidirectional rnn bigram bilabial binary branching binary tree bio bio tagging ner bioe bitext bit measure entropy blank ctc bnf backus naur form bootstrap bootstrap algorithm bootstrap test bootstrappe bind pronoun boundary tone bpe bpe bracketed notation bradley terry model bridging inference broadcast news speech recognition brown corpus original tagging byte pair encoding calibrate callhome candide cantonese capture group cascade regular expression eliza case sensitivity regular expression search case frame cat cataphora conceptual dependency celex center theory centroid cepstral coefﬁcient subject index cepstrum basis cepstral coefﬁcient delta feature formal deﬁnition history cfg seecontext free grammar chain rule chain thought channel channel store waveform character disjunction chart parse chime chinese verb frame language character word brother chomsky normal form chomsky adjunction chrf circus citation form citizen kane cky algorithm claim class base gram classiﬁer head cleft clitic origin term clitic close book closed class closure stop cloze task cluster cmo comparative mean opinion score cnf seechomsky normal form cnn cochlea cocke kasami young algorithm seecky coda syllable code code point code switching codebook codec codeword coherence entity base relation cohesion lexical colbert cold language collaborative completion collection commissive speech act common crawl ground common noun complementizer componential analysis compression computational grammar coder cgc conceptual dependency concrete word conditional generation conditional random ﬁeld conﬁdence relation extraction conﬁdence value conﬁguration confusion matrix conjunction connectionist connotation frame connotation frame connotation consonant constative speech act constituency constituent title constraint grammar content word context embed context free grammar chomsky normal form invention non terminal symbol production rule terminal symbol weak strong equivalence contextual embedding continuation rise continue pretraining continuer contribution conversation analysis conversational implicature conversational speech convex convolving coordination ambiguity copula coraal corefer coreference chain coreference resolution gender agreement hobbs tree search algorithm number agreement person agreement recency preference selectional restriction bind constraint verb semantic coronal sound corpus atis broadcast news brown cass phonetic mandarin ﬁsher kiel german lob switchboard timebank timit wall street journal cosine similarity metric cost function count noun counter count treat low zero crf compare hmm inference viterbi inference crf learning cross attention cross brackets cross correlation cross entropy cross entropy loss cross validation fold crowdsourcing ctc cycle wave cycle second damsl datum contamination datasheet dative alternation debiase decision boundary decoder decoder model decode viterbi deep neural network deep learning deﬁnite reference degree adverb delta feature demonstration denoising dental sound dependency grammar dependency tree dependent depth direct formal language syntactic derivational morpheme det determiner determiner devanagari development set development test set development test set dev test devset seedevelopment test set dev test dft dialogue act acknowledgment backchannel continuer dialogue act accept check hold offer open option statement diathesis alternation diff program digit recognition digital divide digitization dimension diphthong origin term direct derivation formal language directional adverb directive speech act disambiguation parsing syntactic discount discount discourse segment discourse connective discourse deixis discourse model discourse parsing discourse new discourse old discovery procedure discrete fouri transform disﬂuency disjunction pipe regular expression square brace regular expression distant supervision distributional hypothesis distributional similarity index divergence language document domination syntax dot product dot product attention double delta feature dragon system dropout duration temporal expression dynamic programming parse viterbi dynamic time warping edge factor edit distance minimum algorithm edu effect size elaboration coherence relation eliza implementation sample conversation elman network elmo affect deleted interpolation embed matrix embedding cosine similarity skip gram learning sparse emission probability emolex emotion encoder encoder decoder encoder decoder encoder decoder attention encoding end end training endpointing energy frame english lexical difference french simpliﬁed grammar rule verb frame entity dictionary entity grid entity link entity link entity base coherence entropy perplexity cross entropy word rate error backpropagation espnet ethos euclidean distance regularization eugene onegin euler formula europarl evalb evaluate parser evaluation fold cross validation compare model cross validation development test set devset devset development test set extrinsic ﬂuency match pair sentence segment word error mapsswe mean opinion score frequent class baseline name entity recognition gram gram perplexity pseudoword relation extraction test set training test set training set tts event coreference event extraction event evidence coherence relation evoke referent expansion expletive extraposition extrinsic evaluation measure measure measure ner factoid question faiss false negative false positive farsi verb frame fast fouri transform fasttext fastus feature cutoff feature interaction selection information gain feature template feature template speech tagging feature vector feedforward network fencepost shot fft ﬁle format ﬁlle pause ﬁller ﬁnal fall ﬁnetuning ﬁrst order occurrence ﬂap phonetic ﬂuency fold cross validation forget gate formal language formant formant synthesis forward look center fosler fosler lussier foundation model fragment word frame semantic frame element framenet free word order freebase french frequency signal fricative sound frump fully connect function word function word fundamental frequency fusion language gaussian prior weight gazetteer general inquirer generalize generalize semantic role generation sentence test cfg grammar generative generative grammar generator generic german give new glottal stop glottis glyph godzilla speaker gold label gradient grammar constraint head drive phrase structure hpsg link grammar binary branching checking equivalence generative inversion transduction grammatical function grammatical relation grammatical sentence greedy decoding greedy regular expression pattern greek grep gricean maxim grounding ﬁve kind pitch accent hallucinate hallucination hamming hansard hanzi harmonic harmonic mean head ﬁnding head drive phrase structure grammar hpsg heap law hearst pattern hebrew hold herdan law hertz unit measure hide hide layer representation input hide unit hindi hindi verb frame hkust hmm formal deﬁnition history speech recognition initial distribution observation likelihood observation subject index simplify assumption pos tagging state transition probability hobbs algorithm hobbs tree search algorithm pronoun resolution hold dialogue act homonymy hot language hubert hungarian speech tagging hybrid hypernym lexico syntactic pattern hyperparameter hyperparameter unit measure ibm model ibm thomas watson research center idf term weighting immediately dominate implicature implicit argument context learning indeﬁnite reference induction head inference base learning inﬂectional morpheme infoboxe information structure status information extraction bootstrappe information gain feature selection information retrieval information retrieval initiative inner ear inner product instance word institutional review board instruction tuning intensity sound intercept interjection intermediate phrase international phonetic alphabet interpolate precision interpolation smoothing interpretability interval algebra intonation phrase intrinsic evaluation inversion transduction grammar itg invert index iob tagging temporal expression ipa idf term weighting term weighting irb iso isolate language isrl itg inversion transduction grammar japanese mean kaldi kbp kenlm kernel divergence klatt formant synthesizer kleene sneakiness match zero thing kleene knowledge claim knowledge graph korean koryak kullback leibler divergence cache pitch accent pitch accent regularization regularization label precision label recall labial place articulation labiodental consonant language identiﬁcation universal language model language model coin language modeling head laplace smoothing larynx lasso regression semantic analysis lateral sound layer norm ldc learning rate lemma levenshtein distance lexical category cohesion gap semantic stress trigger lexico syntactic pattern lexicon librispeech light verbs linear chain crf linear interpolation gram linearly separable linguistic datum consortium linguistic discourse model link grammar list coherence relation listen attend spell liwc lob corpus localization locative locative adverb log probability compress speech log likelihood ratio log odd ratio log probability logistic function logistic regression conditional maximum likelihood estimation gaussian prior learn regularization relation neural network logit logit lens logo long short term memory lookahead regex lora loss loudness low frame rate lpc linear predictive coding seelatent semantic analysis lstm lunar machine learning ner textbook machine translation macroaveraging mae mandarin manhattan distance regularization manner adverb manner articulation markov assumption markov assumption markov chain formal deﬁnition initial distribution gram state transition probability markov model formal deﬁnition history marx mask language modeling mass noun max pooling maxent maxim gricean maximum entropy maximum span tree mayan mbr mcnemar test mean element wise mean average precision mean opinion score mean reciprocal rank mean pooling mechanical indexing mechanical turk mel frequency cepstral coefﬁcient scale memory network mention detection mention pair mention mert training message understanding conference meteor metonymy mfcc microaveraging microsoft format index mini batch minimum baye risk minimum edit distance example speech recognition evaluation minimum edit distance minimum edit distance algorithm minimum error rate training mle gram gram intuition mlm mlp mmlu modal verb model alignment model card morpheme morphological typology morphology mos mean opinion score moses michelangelo statue moses toolkit mrr marco divergence post editing law muc muc measure multi head attention multi hop multi layer perceptron multinomial logistic regression multiword expression mwe good list gram add smoothing approximation generator markov chain equation example shakespeare history interpolation kenlm logprob normalizing parameter estimation sensitivity corpus smoothing srilm test set training set name entity type name entity recognition nasal sound nasal tract natural language inference natural question negative log likelihood loss ner neural network relation logistic regression newline character sentence prediction nist evaluation noisy nombank nominal non capturing group non greedy non stationary process non terminal symbol normal form normalization temporal normalization probability normalize normalizing noun abstract common count mass proper noun phrase constituent noun nucleus nucleus syllable null hypothesis nyquist frequency observation observation likelihood role viterbi hot vector onset syllable open book open class open information extraction openai operation list operator precedence optionality use regular expression oral tract orthography opaque output gate overﬁtte value pad pair palatal sound palate palato alveolar sound parallel corpus parallel distribute processing parallelogram model parameter efﬁcient ﬁne tuning parse tree parsev parse ambiguity cky cyk seecky evaluation relation grammar syntactic form substring table speech cfg speech adjective adverb closed class interjection noun open class particle subtle distinction verb noun verb speech tagger part taggit speech tagging speech tagging ambiguity ambiguity brown corpus morphological analysis feature template history hungarian turkish unknown word particle part tagger part speech pathos pattern regular expression pcm pulse code modulation pdp pdtb penn discourse treebank penn treebank tagset penn treebank tokenization word entropy perceptron period period disambiguation period wave perplexity weight average branching factor deﬁne cross entropy perplexity coin personal pronoun persuasion phone phonetic articulatory phonotactic phrasal verb phrase base translation phrase structure grammar pipe pitch pitch accent tobi pitch extraction pitch track place articulation pleonastic plosive sound plural polysynthetic language pool pooling max mean pos position embedding relative positional embedding possessive pronoun post editing post training posting postposition pott diagram power signal attachment ambiguity praat praat precedence precedence operator precision precision evaluation ner subject index precision recall curve preference base learning premise prepositional phrase constituency preposition presequence pretokenization pretraining primitive decomposition principle contrast pro drop language probabilistic context free grammar production projective prominence phonetic prominent word prompt prompt engineering pronoun bind demonstrative non binary personal possessive pronunciation dictionary celex cmu propbank proper noun prosodic phrasing prosody prosody accent syllable reduce vowel proto proto prototype clustering pseudoword ptran punctuation number cross linguistically sentence segmentation tokenization treat word treat word quantization query question rise question factoid radio rex rag sampling range regular expression rank rarefaction rdf rdf triple read speech reading comprehension reason coherence relation recall recall evaluation ner receptive ﬁeld reconstruction loss rectangular reduce vowel reduction phonetic reference bind pronoun cataphora deﬁnite generic indeﬁnite reference point referent access evoking referential density reﬂexive reformulation regex regular expression regression lasso ridge regular expression substitution regularization relatedness relation extraction relative temporal expression relative entropy relative frequency release stop relevance relu reporting event representation learning representational harm representational harm rescore residual rvq residual stream residual vector quantization resolve resource management retrieval augment generation reverb reward rewrite structure theory seerst rhyme syllable riau indonesian ridge regression rime syllable rm amplitude rnn extraction root rosebud sle name rounded vowel rst treebank rule context free context free expansion context free sample russian fusion language verb frame rvq sas start symbol cfg salience discourse model sampling sampling analog waveform rate satellite satellite frame language saturate scale law schwa scisor sclite sclite package script schankian script sdrt segment discourse representation theory search engine search tree second order occurrence seed pattern seed tuple segmentation sentence selectional association selectional preference strength selectional preference pseudoword evaluation selectional restriction represent event violation wsd supervise self supervision self training semantic drift semantic feature semantic ﬁeld semantic relation table semantic role semantic role labeling semantic lexical semivowel sense word sentence error rate segmentation sentence separation sentencepiece sentiment origin term sentiment analysis sentiwordnet sequence labeling sft sgn shakespeare gram approximation shallow discourse parsing sibilant sound sequence sigmoid signiﬁcance test mapsswe asr mcnemar similarity cosine singleton singular skip gram slot ﬁlling smoothing add interpolation laplace linear interpolation softmax model sov language spam detection span spanish speaker diarization speaker identiﬁcation speaker recognition speaker veriﬁcation spectrogram spectrum speech telephone bandwidth speech act index speech recognition architecture history speech synthesis split half reliability srilm srl stack rnns standardize start symbol state static embedding stationary process stationary stochastic process statistical statistical signiﬁcance mapsswe asr mcnemar test statistically signiﬁcant stative expression stop consonant stop list streaming stress lexical stride structural ambiguity stupid backoff subdialogue subjectivity substitutability substitution operator regular expression subword superbpe supervise ﬁnetuning supervise machine learning svd svo language swedish verb frame switchboard switchboard corpus syllabiﬁcation syllable accent coda nucleus onset prominent rhyme rime synchronous grammar synonym syntactic disambiguation syntax origin term synthetic language system prompt tac kbp tacre dataset taggit penn treebank table penn treebank tag tamil tanh tap phonetic target embed tay teacher force technai telephone bandwidth speech telic temperature sampling template clustering template ﬁlling template recognition template temporal adverb temporal anchor temporal expression absolute metaphor relative temporal logic temporal normalization term weight term weight terminal symbol test set development choose test time compute text categorization text speech texttiling pile thematic grid thematic role diathesis alternation example problem theme theme thematic role time align transcription timebank timit tobi boundary tone tokenization tokenization sentence word token sampling sampling topic model toxicity detection trachea training oracle set cros validation choose transcription speech reference time align transduction grammar transfer learning transformation discourse analysis project tdap transition probability role viterbi transition base translation divergence trec treebank trigram tts tune continuation rise turk mechanical turkish agglutinative speech tagging turn tydi type dependency structure type word typology linguistic unembedde ungrammatical sentence unicode unigram tokenization algorithm unit production unit vector universal dependency universal linguistic unix unknown word speech tagging unvoiced sound utterance utterance value vanish gradient vanish gradient variable length encoding vauquois triangle vector vector length quantization vector semantic vector semantic vector space velar sound velocity feature velum verb copula modal phrasal verb alternation verb phrase verb frame language verb vietnamese viterbi beam search viterbi algorithm inference crf viterbi algorithm vocal cord fold tract voice sound voiceless sound vowel height high low mid reduce rounded vso language wake word wall street journal wall street journal speech recognition warping waveﬁle format weight tying form substring table wfst pronoun wikiﬁcation wildcard regular expression winograd schema word boundary regular expression notation closed class deﬁnition error rate fragment function open class punctuation token subject index type word sense word sense disambiguation word shape word tokenization context matrix wordform lemma wordnet wordpiece yonker racetrack yupik score zero anaphor zero shot zero shot tts zero width zero